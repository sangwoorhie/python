#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
=== AI ë‹µë³€ ìƒì„± Flask API ì„œë²„ (ë‹¤êµ­ì–´ ì§€ì›) ===
íŒŒì¼ëª…: free_4_ai_answer_generator.py
ëª©ì : ASP Classicì—ì„œ í˜¸ì¶œí•˜ëŠ” AI ë‹µë³€ ìƒì„± API + Pinecone ë²¡í„°DB ë™ê¸°í™”
ì£¼ìš” ê¸°ëŠ¥:
1. OpenAI GPT-3.5-turboë¥¼ ì´ìš©í•œ ìì—°ì–´ ë‹µë³€ ìƒì„± (í•œêµ­ì–´/ì˜ì–´)
2. Pinecone ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ìœ ì‚¬ ë‹µë³€ ê²€ìƒ‰
3. MSSQL ë°ì´í„°ë² ì´ìŠ¤ì™€ Pinecone ë™ê¸°í™”
4. ë©”ëª¨ë¦¬ ìµœì í™” ë° ëª¨ë‹ˆí„°ë§
5. ë‹¤êµ­ì–´ ì§€ì› (í•œêµ­ì–´, ì˜ì–´)
"""

# ==================================================
# 1. í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸ êµ¬ê°„
# ==================================================
# ê¸°ë³¸ Python ëª¨ë“ˆë“¤
import os                   # í™˜ê²½ë³€ìˆ˜ ë° íŒŒì¼ ì‹œìŠ¤í…œ ì‘ì—…
import sys                  # ì‹œìŠ¤í…œ ê´€ë ¨ ê¸°ëŠ¥
import json                 # JSON ë°ì´í„° ì²˜ë¦¬
import json as json_module  # JSON ëª¨ë“ˆì˜ ë³„ì¹­ (ì¼ë¦¬ì•„ìŠ¤, ì½”ë“œ ë‚´ ì¤‘ë³µ ë°©ì§€)
import re                   # ì •ê·œí‘œí˜„ì‹ íŒ¨í„´ ë§¤ì¹­
import html                 # HTML ì—”í‹°í‹° ì²˜ë¦¬
import unicodedata          # ìœ ë‹ˆì½”ë“œ ë¬¸ì ì •ê·œí™”
import logging              # ë¡œê·¸ ê¸°ë¡ ì‹œìŠ¤í…œ
import gc                   # ê°€ë¹„ì§€ ì»¬ë ‰ì…˜ (ë©”ëª¨ë¦¬ ê´€ë¦¬)

# ì›¹ í”„ë ˆì„ì›Œí¬ ê´€ë ¨
from flask import Flask, request, jsonify  # Flask ì›¹ í”„ë ˆì„ì›Œí¬
from flask_cors import CORS                 # CORS(Cross-Origin Resource Sharing) ì²˜ë¦¬

# AI ë° ë°ì´í„°ë² ì´ìŠ¤ ê´€ë ¨
from pinecone import Pinecone      # Pinecone ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤
import openai                      # OpenAI API í´ë¼ì´ì–¸íŠ¸
import pyodbc                      # MSSQL ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²°

# í™˜ê²½ì„¤ì • ë° ìœ í‹¸ë¦¬í‹°
from dotenv import load_dotenv     # .env íŒŒì¼ì—ì„œ í™˜ê²½ë³€ìˆ˜ ë¡œë“œ
from datetime import datetime      # ë‚ ì§œ/ì‹œê°„ ì²˜ë¦¬
from typing import Optional, Dict, Any, List  # íƒ€ì… íŒíŒ…
import re                                     # ì •ê·œí‘œí˜„ì‹

# ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ê´€ë ¨
from memory_profiler import profile                  # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ í”„ë¡œíŒŒì¼ë§
import tracemalloc                                   # ë©”ëª¨ë¦¬ ì¶”ì 
import threading                                     # ë©€í‹°ìŠ¤ë ˆë”©
from contextlib import contextmanager                # ì»¨í…ìŠ¤íŠ¸ ë§¤ë‹ˆì € (withë¬¸ ì‚¬ìš©)
from langdetect import detect, LangDetectException   # ì–¸ì–´ ê°ì§€

# ==================================================
# 2. ì‹œìŠ¤í…œ ì´ˆê¸°í™” ë° ì„¤ì •
# ==================================================
# ë©”ëª¨ë¦¬ ì¶”ì  ì‹œì‘ - ë©”ëª¨ë¦¬ ëˆ„ìˆ˜ ë° ì‚¬ìš©ëŸ‰ì„ ëª¨ë‹ˆí„°ë§í•˜ê¸° ìœ„í•¨ (ì´ ì‹œì ë¶€í„° ëª¨ë“  ë©”ëª¨ë¦¬ í• ë‹¹ì´ ê¸°ë¡ë˜ì–´ ë‚˜ì¤‘ì— ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ë¶„ì„ì´ ê°€ëŠ¥í•´ì§)
tracemalloc.start() 

# Flask ì›¹ ì• í”Œë¦¬ì¼€ì´ì…˜ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
# __name__: í˜„ì¬ ëª¨ë“ˆëª…ì„ ì „ë‹¬í•˜ì—¬ Flaskê°€ ë¦¬ì†ŒìŠ¤ ìœ„ì¹˜ë¥¼ ì°¾ì„ ìˆ˜ ìˆê²Œ í•¨
app = Flask(__name__)

# CORS ì„¤ì • - ëª¨ë“  ì—”ë“œí¬ì¸íŠ¸ì—ì„œ cross-origin ìš”ì²­ì„ í—ˆìš© (ASP Classicì—ì„œ í˜¸ì¶œí•˜ê¸° ìœ„í•¨)
CORS(app)

# ==================================================
# 3. ë¡œê¹… ì‹œìŠ¤í…œ ì„¤ì • (ì½˜ì†” + íŒŒì¼)
# ==================================================
# ë¡œê±° ìƒì„±
logger = logging.getLogger()
logger.setLevel(logging.INFO)

# í¬ë§·í„° ìƒì„±
formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')

# íŒŒì¼ í•¸ë“¤ëŸ¬ (ê¸°ì¡´)
try:
    # ë¡œê·¸ ë””ë ‰í† ë¦¬ ìƒì„±
    os.makedirs('/home/ec2-user/python/logs', exist_ok=True)
    file_handler = logging.FileHandler('/home/ec2-user/python/logs/ai_generator.log', encoding='utf-8')
    file_handler.setLevel(logging.INFO)
    file_handler.setFormatter(formatter)
    logger.addHandler(file_handler)
except Exception as e:
    print(f"ë¡œê·¸ íŒŒì¼ í•¸ë“¤ëŸ¬ ìƒì„± ì‹¤íŒ¨: {e}")

# ì½˜ì†” í•¸ë“¤ëŸ¬ ì¶”ê°€ (ì‹¤ì‹œê°„ ë””ë²„ê¹…ìš©)
console_handler = logging.StreamHandler()
console_handler.setLevel(logging.INFO)
console_handler.setFormatter(formatter)
logger.addHandler(console_handler)

# ==================================================
# 4. í™˜ê²½ë³€ìˆ˜ ë¡œë“œ ë° ì‹œìŠ¤í…œ ìƒìˆ˜ ì •ì˜
# ==================================================
# .env íŒŒì¼ì—ì„œ API í‚¤ ë° ë°ì´í„°ë² ì´ìŠ¤ ì„¤ì • ë¡œë“œ
load_dotenv()

# AI ì„ë² ë”© ëª¨ë¸ ë° ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ì„¤ì • ìƒìˆ˜ë“¤
MODEL_NAME = 'text-embedding-3-small'          # OpenAI ì„ë² ë”© ëª¨ë¸ëª…
INDEX_NAME = "bible-app-support-1536-openai"   # Pinecone ì¸ë±ìŠ¤ëª…
EMBEDDING_DIMENSION = 1536                      # ì„ë² ë”© ë²¡í„° ì°¨ì›ìˆ˜
MAX_TEXT_LENGTH = 8000                          # í…ìŠ¤íŠ¸ ìµœëŒ€ ê¸¸ì´ ì œí•œ

# GPT ìì—°ì–´ ëª¨ë¸ ì„¤ì • - ë³´ìˆ˜ì  ì„¤ì •ìœ¼ë¡œ ì¼ê´€ì„± ìˆëŠ” ë‹µë³€ ìƒì„±
GPT_MODEL = 'gpt-3.5-turbo'     # ì‚¬ìš©í•  GPT ëª¨ë¸
MAX_TOKENS = 600                 # ìƒì„±í•  ìµœëŒ€ í† í° ìˆ˜ (ë‹µë³€ ê¸¸ì´ ì œí•œ)
TEMPERATURE = 0.5                # ì°½ì˜ì„± ìˆ˜ì¤€ (ë‚®ì„ìˆ˜ë¡ ì¼ê´€ëœ ë‹µë³€)

# ê³ ê° ë¬¸ì˜ ì¹´í…Œê³ ë¦¬ ë§¤í•‘ í…Œì´ë¸”
# ì¹´í…Œê³ ë¦¬ ë§¤í•‘ ë”•ì…”ë„ˆë¦¬ëŠ” MSSQLì˜ ìˆ«ì ì¸ë±ìŠ¤ë¥¼ ì‚¬ëŒì´ ì½ì„ ìˆ˜ ìˆëŠ” í•œê¸€ ì¹´í…Œê³ ë¦¬ëª…ìœ¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤. 
# ì´ëŠ” Pinecone ë©”íƒ€ë°ì´í„°ì— ì €ì¥ë˜ì–´ ê²€ìƒ‰ ê²°ê³¼ì˜ ê°€ë…ì„±ì„ ë†’ì…ë‹ˆë‹¤.
CATEGORY_MAPPING = {
    '1': 'í›„ì›/í•´ì§€',                   
    '2': 'ì„±ê²½ í†µë…(ì½ê¸°,ë“£ê¸°,ë…¹ìŒ)',   
    '3': 'ì„±ê²½ë‚­ë… ë ˆì´ìŠ¤',             
    '4': 'ê°œì„ /ì œì•ˆ',                   
    '5': 'ì˜¤ë¥˜/ì¥ì• ',               
    '6': 'ë¶ˆë§Œ',                        
    '7': 'ì˜¤íƒˆìì œë³´',                   
    '0': 'ì‚¬ìš© ë¬¸ì˜(ê¸°íƒ€)'               
}

# ì˜ì–´ ì¹´í…Œê³ ë¦¬ ë§¤í•‘ ì¶”ê°€
CATEGORY_MAPPING_EN = {
    '1': 'Sponsorship/Cancellation',
    '2': 'Bible Reading(Read,Listen,Record)',
    '3': 'Bible Reading Race',
    '4': 'Improvement/Suggestion',
    '5': 'Error/Failure',
    '6': 'Complaint',
    '7': 'Typo Report',
    '0': 'Usage Inquiry(Other)'
}

# ==================================================
# 5. ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ ì •ì˜
# ==================================================
# ë©”ëª¨ë¦¬ ì •ë¦¬ë¥¼ ìœ„í•œ ì»¨í…ìŠ¤íŠ¸ ë§¤ë‹ˆì €
# withë¬¸ ì§„ì…ì‹œ try ë¸”ë¡ ì‹¤í–‰
# yieldì—ì„œ ì¼ì‹œì •ì§€í•˜ê³  with ë¸”ë¡ ë‚´ë¶€ ì½”ë“œ ì‹¤í–‰
# yield í‚¤ì›Œë“œê°€ ë“¤ì–´ê°„ ì´ í•¨ìˆ˜ëŠ” ì œë„ˆë ˆì´í„°ì´ë©´ì„œ ë™ì‹œì— ì»¨í…ìŠ¤íŠ¸ ë§¤ë‹ˆì €ë¡œ ë™ì‘
# ì œë„ˆë ˆì´í„° í•¨ìˆ˜ëŠ” ê°’ì„ ë°˜í™˜í•˜ëŠ” ëŒ€ì‹  ê°’ì„ í•˜ë‚˜ì”© ìƒì„±í•˜ì—¬ ë°˜í™˜í•©ë‹ˆë‹¤. (íŒŒì´ì¬ì—ì„œ ëŒ€ìš©ëŸ‰ ë°ì´í„° ì²˜ë¦¬ë¥¼ ìœ„í•´ ì‚¬ìš©)
# with ë¸”ë¡ ì¢…ë£Œì‹œ finallyì—ì„œ gc.collect() ì‹¤í–‰

# ì´ë ‡ê²Œ í•˜ë©´ ëŒ€ìš©ëŸ‰ ë°ì´í„° ì²˜ë¦¬ í›„ ìë™ìœ¼ë¡œ ê°€ë¹„ì§€ ì»¬ë ‰ì…˜ì´ ì‹¤í–‰ë©ë‹ˆë‹¤.
@contextmanager
def memory_cleanup():
    try:
        yield  # with ë¸”ë¡ ë‚´ë¶€ ì½”ë“œ ì‹¤í–‰
    finally:
        gc.collect()  # ê°€ë¹„ì§€ ì»¬ë ‰ì…˜ ê°•ì œ ì‹¤í–‰ìœ¼ë¡œ ë©”ëª¨ë¦¬ ì •ë¦¬

# ==================================================
# 6. ì™¸ë¶€ ì„œë¹„ìŠ¤ ì—°ê²° ë° ì´ˆê¸°í™”
# ==================================================
try:
    # Pinecone ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì„¤ì •
    # ìœ ì‚¬ ë‹µë³€ ê²€ìƒ‰ì„ ìœ„í•œ ë²¡í„°DB - ì„ë² ë”©ëœ ì§ˆë¬¸/ë‹µë³€ ì €ì¥ì†Œ
    pc = Pinecone(api_key=os.getenv('PINECONE_API_KEY'))
    index = pc.Index(INDEX_NAME)
    
    # OpenAI API í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
    # GPT ëª¨ë¸ ë° ì„ë² ë”© ìƒì„±ì„ ìœ„í•œ í´ë¼ì´ì–¸íŠ¸
    # openai.api_key = ... ë°©ì‹ë³´ë‹¤ ê°ì²´ì§€í–¥ì ìœ¼ë¡œ ì„¤ê³„
    openai_client = openai.OpenAI(api_key=os.getenv('OPENAI_API_KEY'))
    
    # MSSQL ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì„¤ì •
    # ê¸°ì¡´ ê³ ê° ë¬¸ì˜ ë°ì´í„°ê°€ ì €ì¥ëœ ìš´ì˜ DB ì—°ê²°ì„ ìœ„í•œ ì„¤ì •
    mssql_config = {
        'server': os.getenv('MSSQL_SERVER'),       # DB ì„œë²„ ì£¼ì†Œ
        'database': os.getenv('MSSQL_DATABASE'),   # ë°ì´í„°ë² ì´ìŠ¤ëª…
        'username': os.getenv('MSSQL_USERNAME'),   # DB ì‚¬ìš©ìëª…
        'password': os.getenv('MSSQL_PASSWORD')    # DB ë¹„ë°€ë²ˆí˜¸
    }
    
    # MSSQL Server ì—°ê²° ë¬¸ìì—´ êµ¬ì„±
    # MSSQL Server í‘œì¤€ ODBC ë“œë¼ì´ë²„ë¥¼ ì‚¬ìš©í•œ ì—°ê²° ë¬¸ìì—´
    connection_string = (
            f"DRIVER={{ODBC Driver 17 for SQL Server}};"  # ODBC ë“œë¼ì´ë²„ ë²„ì „
            f"SERVER={mssql_config['server']},1433;"      # ì„œë²„ ì£¼ì†Œì™€ í¬íŠ¸
            f"DATABASE={mssql_config['database']};"       # ë°ì´í„°ë² ì´ìŠ¤ëª…
            f"UID={mssql_config['username']};"            # ì‚¬ìš©ì ID
            f"PWD={mssql_config['password']};"            # ë¹„ë°€ë²ˆí˜¸
            f"TrustServerCertificate=yes;"                # SSL ì¸ì¦ì„œ ì‹ ë¢°
            f"Connection Timeout=30;"                     # ì—°ê²° íƒ€ì„ì•„ì›ƒ (30ì´ˆ)
    )

except Exception as e:
    # ì´ˆê¸°í™” ì‹¤íŒ¨ ì‹œ ë¡œê·¸ ê¸°ë¡ í›„ í”„ë¡œê·¸ë¨ ì¢…ë£Œ
    logging.error(f"ëª¨ë“ˆ ë¡œë“œ ì‹¤íŒ¨: {str(e)}")
    app.logger.error(f"ëª¨ë“ˆ ë¡œë“œ ì‹¤íŒ¨: {str(e)}")
    raise  # ì˜ˆì™¸ë¥¼ ë‹¤ì‹œ ë°œìƒì‹œì¼œ í”„ë¡œê·¸ë¨ ì¤‘ë‹¨

# ==================================================
# 7. AI ë‹µë³€ ìƒì„± ë©”ì¸ í´ë˜ìŠ¤ (ê°ì²´ ì§€í–¥ í”„ë¡œê·¸ë˜ë°, ë‹¤êµ­ì–´ ì§€ì› ì¶”ê°€)
# ==================================================
    # AI ë‹µë³€ ìƒì„±ì„ ë‹´ë‹¹í•˜ëŠ” ë©”ì¸ í´ë˜ìŠ¤
    # ê°ì²´ì§€í–¥ ì„¤ê³„ë¡œ ê´€ë ¨ ê¸°ëŠ¥ë“¤ì„ í•˜ë‚˜ì˜ í´ë˜ìŠ¤ì— ìº¡ìŠí™”
    
    # ì£¼ìš” ê¸°ëŠ¥:
    # 1. í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ ë° ì •ì œ
    # 2. OpenAIë¥¼ ì´ìš©í•œ ì„ë² ë”© ìƒì„±
    # 3. Pineconeì—ì„œ ìœ ì‚¬ ë‹µë³€ ê²€ìƒ‰
    # 4. GPTë¥¼ ì´ìš©í•œ ë§ì¶¤í˜• ë‹µë³€ ìƒì„±
    # 5. í•œêµ­ì–´ í…ìŠ¤íŠ¸ ê²€ì¦ ë° í¬ë§·íŒ…

class AIAnswerGenerator:
    
    # í´ë˜ìŠ¤ ì´ˆê¸°í™” ë©”ì„œë“œ
    # OpenAI í´ë¼ì´ì–¸íŠ¸ë¥¼ ì¸ìŠ¤í„´ìŠ¤ ë³€ìˆ˜ë¡œ ì„¤ì •. ì´ëŠ” ì˜ì¡´ì„± ì£¼ì… íŒ¨í„´ì˜ ê°„ì†Œí™”ëœ í˜•íƒœ
    def __init__(self):
        self.openai_client = openai_client

    # â˜† í•œêµ­ì–´ ì˜¤íƒ€ ìˆ˜ì • ë©”ì„œë“œ
    def fix_korean_typos_with_ai(self, text: str) -> str:
        if not text or len(text.strip()) < 3:
            return text
        
        # ë„ˆë¬´ ê¸´ í…ìŠ¤íŠ¸ëŠ” ì²˜ë¦¬í•˜ì§€ ì•ŠìŒ (ë¹„ìš© ì ˆì•½)
        if len(text) > 500:
            logging.warning(f"í…ìŠ¤íŠ¸ê°€ ë„ˆë¬´ ê¸¸ì–´ ì˜¤íƒ€ ìˆ˜ì • ê±´ë„ˆëœ€: {len(text)}ì")
            return text
        
        try:
            with memory_cleanup():
                system_prompt = """ë‹¹ì‹ ì€ í•œêµ­ì–´ ë§ì¶¤ë²• ë° ì˜¤íƒ€ êµì • ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

ì§€ì¹¨:
1. ì…ë ¥ëœ í•œêµ­ì–´ í…ìŠ¤íŠ¸ì˜ ë§ì¶¤ë²•ê³¼ ì˜¤íƒ€ë§Œ ìˆ˜ì •í•˜ì„¸ìš”
2. ì›ë¬¸ì˜ ì˜ë¯¸ì™€ ì–´ì¡°ëŠ” ì ˆëŒ€ ë³€ê²½í•˜ì§€ ë§ˆì„¸ìš”
3. ë„ì–´ì“°ê¸°, ë§ì¶¤ë²•, ì¡°ì‚¬ ì‚¬ìš©ë²•ì„ ì •í™•íˆ êµì •í•˜ì„¸ìš”
4. ì•±/ì–´í”Œë¦¬ì¼€ì´ì…˜ ê´€ë ¨ ê¸°ìˆ  ìš©ì–´ëŠ” í‘œì¤€ ìš©ì–´ë¡œ í†µì¼í•˜ì„¸ìš”
5. ìˆ˜ì •ì´ í•„ìš”ì—†ë‹¤ë©´ ì›ë¬¸ ê·¸ëŒ€ë¡œ ë°˜í™˜í•˜ì„¸ìš”
6. ìˆ˜ì •ëœ í…ìŠ¤íŠ¸ë§Œ ë°˜í™˜í•˜ê³  ì¶”ê°€ ì„¤ëª…ì€ í•˜ì§€ ë§ˆì„¸ìš”

ì˜ˆì‹œ:
- "ì–´í”Œì´ ì•ˆë€ë‹¤" â†’ "ì•±ì´ ì•ˆ ë¼ìš”"
- "ë‹¤ìš´ë°›ê¸°ê°€ ì•ˆë˜ìš”" â†’ "ë‹¤ìš´ë¡œë“œê°€ ì•ˆ ë¼ìš”"
- "ì‚­ì¬í•˜ê³ ì‹¶ì–´ìš”" â†’ "ì‚­ì œí•˜ê³  ì‹¶ì–´ìš”"
- "ì—…ë°ì´ë“œí•´ì£¼ì„¸ìš”" â†’ "ì—…ë°ì´íŠ¸í•´ì£¼ì„¸ìš”"
"""

                user_prompt = f"ë‹¤ìŒ í…ìŠ¤íŠ¸ì˜ ë§ì¶¤ë²•ê³¼ ì˜¤íƒ€ë¥¼ ìˆ˜ì •í•´ì£¼ì„¸ìš”:\n\n{text}"

                response = self.openai_client.chat.completions.create(
                    model='gpt-3.5-turbo',
                    messages=[
                        {"role": "system", "content": system_prompt},
                        {"role": "user", "content": user_prompt}
                    ],
                    max_tokens=600,
                    temperature=0.1,  # ë§¤ìš° ë³´ìˆ˜ì ìœ¼ë¡œ ì„¤ì •
                    top_p=0.8,
                    frequency_penalty=0.0,
                    presence_penalty=0.0
                )
                
                corrected_text = response.choices[0].message.content.strip()
                del response # ë©”ëª¨ë¦¬ í•´ì œ
                
                # ê²°ê³¼ ê²€ì¦
                if not corrected_text or len(corrected_text) == 0:
                    logging.warning("AI ì˜¤íƒ€ ìˆ˜ì • ê²°ê³¼ê°€ ë¹„ì–´ìˆìŒ, ì›ë¬¸ ë°˜í™˜")
                    return text
                
                # ë„ˆë¬´ ë§ì´ ë³€ê²½ëœ ê²½ìš° ì˜ì‹¬ìŠ¤ëŸ¬ìš°ë¯€ë¡œ ì›ë¬¸ ë°˜í™˜
                if len(corrected_text) > len(text) * 2:
                    logging.warning("AI ì˜¤íƒ€ ìˆ˜ì • ê²°ê³¼ê°€ ì›ë¬¸ë³´ë‹¤ ë„ˆë¬´ ê¸¸ì–´ì§, ì›ë¬¸ ë°˜í™˜")
                    return text
                
                # ìˆ˜ì • ë‚´ìš©ì´ ìˆìœ¼ë©´ ë¡œê·¸ ê¸°ë¡
                if corrected_text != text:
                    logging.info(f"AI ì˜¤íƒ€ ìˆ˜ì •: '{text[:50]}...' â†’ '{corrected_text[:50]}...'")
                
                return corrected_text
                
        except Exception as e:
            logging.error(f"AI ì˜¤íƒ€ ìˆ˜ì • ì‹¤íŒ¨: {e}")
            # AI ì‹¤íŒ¨ ì‹œ ì›ë¬¸ ê·¸ëŒ€ë¡œ ë°˜í™˜
            return text
    
    # â˜† í…ìŠ¤íŠ¸ì˜ ì–¸ì–´ë¥¼ ê°ì§€í•˜ëŠ” ë©”ì„œë“œ
    def detect_language(self, text: str) -> str:
        try:
            # langdetect ë¼ì´ë¸ŒëŸ¬ë¦¬ ì‚¬ìš©
            detected = detect(text)
            
            # ì˜ì–´ì™€ í•œêµ­ì–´ë§Œ ì§€ì›
            if detected == 'en':
                return 'en'
            elif detected == 'ko':
                return 'ko'
            else:
                # ê¸°ë³¸ê°’ì€ í•œêµ­ì–´
                return 'ko'
        except LangDetectException:
            # ê°ì§€ ì‹¤íŒ¨ì‹œ í…ìŠ¤íŠ¸ ë‚´ í•œê¸€ ë¹„ìœ¨ë¡œ íŒë‹¨
            korean_chars = len(re.findall(r'[ê°€-í£]', text))
            english_chars = len(re.findall(r'[a-zA-Z]', text))
            
            if korean_chars > english_chars:
                return 'ko'
            else:
                return 'en'

    # â˜† ì…ë ¥ í…ìŠ¤íŠ¸ë¥¼ AI ì²˜ë¦¬ì— ì í•©í•˜ê²Œ ì „ì²˜ë¦¬í•˜ëŠ” ë©”ì„œë“œ (ì›ë³¸ í…ìŠ¤íŠ¸ -> ì •ì œëœ í…ìŠ¤íŠ¸)
    def preprocess_text(self, text: str) -> str:
        logging.info(f"ì „ì²˜ë¦¬ ì‹œì‘: ì…ë ¥ ê¸¸ì´={len(text) if text else 0}")
        logging.info(f"ì „ì²˜ë¦¬ ì…ë ¥ ë¯¸ë¦¬ë³´ê¸°: {text[:100] if text else 'None'}...")

        # null ì²´í¬
        if not text:
            logging.info("ì „ì²˜ë¦¬: ë¹ˆ í…ìŠ¤íŠ¸ ì…ë ¥")
            return ""
        
        # ë¬¸ìì—´ë¡œ ë³€í™˜ ë° HTML ì—”í‹°í‹° ë””ì½”ë”©
        text = str(text)
        text = html.unescape(text)  # &amp; â†’ &, &lt; â†’ < ë“±
        logging.info(f"HTML ë””ì½”ë”© í›„ ê¸¸ì´: {len(text)}")
        
        # HTML íƒœê·¸ ì œê±° ë°ë° í…ìŠ¤íŠ¸ í˜•íƒœë¡œ ë³€í™˜ (êµ¬ì¡° ìœ ì§€)
        text = re.sub(r'<br\s*/?>', '\n', text, flags=re.IGNORECASE)      # <br> â†’ ì¤„ë°”ê¿ˆ
        text = re.sub(r'</p>', '\n\n', text, flags=re.IGNORECASE)         # </p> â†’ ë‹¨ë½ êµ¬ë¶„
        text = re.sub(r'<p[^>]*>', '\n', text, flags=re.IGNORECASE)       # <p> â†’ ì¤„ë°”ê¿ˆ
        text = re.sub(r'<li[^>]*>', '\nâ€¢ ', text, flags=re.IGNORECASE)    # <li> â†’ ë¶ˆë¦¿í¬ì¸íŠ¸
        text = re.sub(r'</li>', '', text, flags=re.IGNORECASE)            # </li> ì œê±°
        text = re.sub(r'<[^>]+>', '', text)                               # ë‚˜ë¨¸ì§€ HTML íƒœê·¸ ëª¨ë‘ ì œê±°
        logging.info(f"HTML íƒœê·¸ ì œê±° í›„ ê¸¸ì´: {len(text)}")
        
        # ğŸ”¥ êµ¬ ì•± ì´ë¦„ì„ ë°”ì´ë¸” ì• í”Œë¡œ êµì²´ (ì „ì²˜ë¦¬ ë‹¨ê³„ì—ì„œ)
        # ì¤‘ë³µ ë°©ì§€ë¥¼ ìœ„í•´ ìˆœì„œë¥¼ ì¡°ì •: ì „ì²´ íŒ¨í„´ë¶€í„° ì²˜ë¦¬
        text = re.sub(r'ë°”ì´ë¸”\s*ì• í”Œ\s*\(êµ¬\)\s*ë‹¤ë²ˆì—­\s*ì„±ê²½\s*ì°¬ì†¡', 'ë°”ì´ë¸” ì• í”Œ', text, flags=re.IGNORECASE)
        text = re.sub(r'ë°”ì´ë¸”\s*ì• í”Œ\s*\(êµ¬\)\s*ë‹¤ë²ˆì—­ì„±ê²½ì°¬ì†¡', 'ë°”ì´ë¸” ì• í”Œ', text, flags=re.IGNORECASE)
        text = re.sub(r'\(êµ¬\)\s*ë‹¤ë²ˆì—­\s*ì„±ê²½\s*ì°¬ì†¡', 'ë°”ì´ë¸” ì• í”Œ', text, flags=re.IGNORECASE)
        text = re.sub(r'\(êµ¬\)\s*ë‹¤ë²ˆì—­ì„±ê²½ì°¬ì†¡', 'ë°”ì´ë¸” ì• í”Œ', text, flags=re.IGNORECASE)
        text = re.sub(r'ë‹¤ë²ˆì—­\s*ì„±ê²½\s*ì°¬ì†¡', 'ë°”ì´ë¸” ì• í”Œ', text, flags=re.IGNORECASE)
        text = re.sub(r'ë‹¤ë²ˆì—­ì„±ê²½ì°¬ì†¡', 'ë°”ì´ë¸” ì• í”Œ', text, flags=re.IGNORECASE)
        
        # ê³µë°± ë° ì¤„ë°”ê¿ˆ ì •ê·œí™” - ì¼ê´€ëœ í˜•íƒœë¡œ ë³€í™˜
        text = re.sub(r'\n{3,}', '\n\n', text)    # 3ê°œ ì´ìƒ ì¤„ë°”ê¿ˆ â†’ 2ê°œë¡œ ì œí•œ
        text = re.sub(r'[ \t]+', ' ', text)       # ì—°ì† ê³µë°±/íƒ­ â†’ ë‹¨ì¼ ê³µë°±
        text = text.strip()                       # ì•ë’¤ ê³µë°± ì œê±°
        
        logging.info(f"ì „ì²˜ë¦¬ ì™„ë£Œ: ìµœì¢… ê¸¸ì´={len(text)}")
        logging.info(f"ì „ì²˜ë¦¬ ê²°ê³¼ ë¯¸ë¦¬ë³´ê¸°: {text[:100]}...")
        
        return text

    # â˜† JSON ë¬¸ìì—´ ì´ìŠ¤ì¼€ì´í”„ ì²˜ë¦¬ (íŠ¹ìˆ˜ë¬¸ìê°€ í¬í•¨ëœ í…ìŠ¤íŠ¸ë¥¼ JSONìœ¼ë¡œ ì•ˆì „í•˜ê²Œ ë³€í™˜)
    def escape_json_string(self, text: str) -> str:
        
        if not text:
            return ""
        escaped = json_module.dumps(text, ensure_ascii=False) # ensure_ascii=False: í•œê¸€ ê¹¨ì§ ë°©ì§€
        return escaped[1:-1]  # ì•ë’¤ ë”°ì˜´í‘œ ì œê±°

    # â˜† OpenAI APIë¥¼ ì‚¬ìš©í•˜ì—¬ í…ìŠ¤íŠ¸ë¥¼ ë²¡í„°ë¡œ ë³€í™˜í•˜ëŠ” ë©”ì„œë“œ
    # ë²¡í„° ì„ë² ë”©: í…ìŠ¤íŠ¸ì˜ ì˜ë¯¸ë¥¼ ìˆ˜ì¹˜ ë°°ì—´ë¡œ í‘œí˜„í•˜ì—¬ ìœ ì‚¬ë„ ê³„ì‚° ê°€ëŠ¥
    def create_embedding(self, text: str) -> Optional[list]:

        # ë¹ˆ ë¬¸ìì—´ë¿ë§Œ ì•„ë‹ˆë¼ ê³µë°±ë§Œ ìˆëŠ” ë¬¸ìì—´ë„ ê±¸ëŸ¬ëƒ„ (ì´ëŸ° ê²½ìš° JSON ë³€í™˜ ë¶ˆê°€)
        if not text or not text.strip():
            return None
            
        try:
            with memory_cleanup(): # ë©”ëª¨ë¦¬ ëˆ„ìˆ˜ ë°©ì§€ (ë¸”ë¡ ì¢…ë£Œ ì‹œ ê°€ë¹„ì§€ ì»¬ë ‰ì…˜)
                # OpenAI Embedding API í˜¸ì¶œ
                response = self.openai_client.embeddings.create(
                    model='text-embedding-3-small',    # ë²¡í„° ì„ë² ë”© ëª¨ë¸ëª…
                    input=text[:8000]                  # í…ìŠ¤íŠ¸ ê¸¸ì´ ì œí•œ (í† í° ì œí•œ ë°©ì§€ì§€)
                )
                
                # ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±ì„ ìœ„í•´ ë²¡í„°ë§Œ ë³µì‚¬ í›„ ì‘ë‹µ ê°ì²´ ì‚­ì œ
                embedding = response.data[0].embedding.copy() # ì„ë² ë”© ë²¡í„°ë§Œ ë³µì‚¬í•˜ì—¬ ë°˜í™˜ (ê¹Šì€ ë³µì‚¬ë¡œ ë…ë¦½ì ì¸ ë¦¬ìŠ¤íŠ¸ ìƒì„±)
                del response  # ì›ë³¸ ì‘ë‹µ ê°ì²´ ì¦‰ì‹œ ì‚­ì œ (ë©”ëª¨ë¦¬ í•´ì œ)
                return embedding
                
        except Exception as e:
            logging.error(f"ì„ë² ë”© ìƒì„± ì‹¤íŒ¨: {e}")
            return None

    # â˜† Pinecone ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ìœ ì‚¬í•œ ë‹µë³€ì„ ê²€ìƒ‰í•˜ëŠ” ë©”ì„œë“œ
    # Args:
    #     query (str): ê²€ìƒ‰í•  ì§ˆë¬¸
    #     top_k (int): ê²€ìƒ‰í•  ìµœëŒ€ ê°œìˆ˜ (ê¸°ë³¸ê°’: 5)
    #     similarity_threshold (float): ìœ ì‚¬ë„ ì„ê³„ê°’ (ê¸°ë³¸ê°’: 0.3)
            
    # Returns:
    #     list: ìœ ì‚¬ ë‹µë³€ ë¦¬ìŠ¤íŠ¸ [{'score': float, 'question': str, 'answer': str, ...}, ...]
    def search_similar_answers(self, query: str, top_k: int = 5, similarity_threshold: float = 0.7, lang: str = 'ko') -> list:
        try:
            with memory_cleanup():
                # â˜… ë””ë²„ê¹… ë¡œê·¸ ì¶”ê°€
                logging.info(f"=== ê²€ìƒ‰ ì‹œì‘ ===")
                logging.info(f"ì›ë³¸ ì§ˆë¬¸: {query[:100]}")
                
                # â˜… ì˜¤íƒ€ ìˆ˜ì • ì ìš© (ë™ê¸°í™”ì™€ ë™ì¼í•˜ê²Œ!)
                if lang == 'ko':
                    corrected_query = self.fix_korean_typos_with_ai(query)
                    logging.info(f"ì˜¤íƒ€ ìˆ˜ì • í›„: {corrected_query[:100]}")
                    query_to_embed = corrected_query
                else:
                    query_to_embed = query
                
                # ì„ë² ë”© ìƒì„±
                query_vector = self.create_embedding(query_to_embed)
                
                if query_vector is None:
                    logging.error("ì„ë² ë”© ìƒì„± ì‹¤íŒ¨")
                    return []
                
                # Pinecone ê²€ìƒ‰
                results = index.query(
                    vector=query_vector,
                    top_k=top_k * 2,  # ë” ë§ì´ ê²€ìƒ‰
                    include_metadata=True
                )
                
                logging.info(f"Pinecone ê²€ìƒ‰ ê²°ê³¼: {len(results['matches'])}ê°œ")
                
                # í•œêµ­ì–´ ë²¡í„°ë¡œ ì¶”ê°€ ê²€ìƒ‰ (ì˜ì–´ ì§ˆë¬¸ì¸ ê²½ìš°)
                korean_vector = None  # ì´ˆê¸°í™”í•˜ì—¬ NameError ë°©ì§€
                if lang == 'en':
                    # ì˜ì–´ ì¿¼ë¦¬ë¥¼ í•œêµ­ì–´ë¡œ ë²ˆì—­ í›„ ì„ë² ë”© ìƒì„± (ëˆ„ë½ëœ ë¡œì§ ì¶”ê°€)
                    korean_query = self.translate_text(query_to_embed, 'en', 'ko')
                    korean_vector = self.create_embedding(korean_query)
                    if korean_vector:
                        korean_results = index.query(
                            vector=korean_vector,       # ê²€ìƒ‰í•  ë²¡í„°
                            top_k=3,                    # ë³´ì¡° ê²€ìƒ‰ì€ ì ê²Œ (3ê°œ)
                            include_metadata=True       # ë©”íƒ€ë°ì´í„° í¬í•¨ (ì§ˆë¬¸, ë‹µë³€, ì¹´í…Œê³ ë¦¬ ë“±)
                        )
                        # ê²°ê³¼ ë³‘í•© (ì¤‘ë³µ ì œê±°)
                        seen_ids = set()
                        merged_matches = []
                        for match in results['matches'] + korean_results['matches']:
                            if match['id'] not in seen_ids:
                                seen_ids.add(match['id'])
                                merged_matches.append(match)
                        results['matches'] = sorted(merged_matches, key=lambda x: x['score'], reverse=True)[:top_k]
                
                # 3. ê²°ê³¼ í•„í„°ë§ ë° êµ¬ì¡°í™”
                filtered_results = []
                for i, match in enumerate(results['matches']): # enumerateë¡œ ìˆœìœ„(rank) ìƒì„±
                    score = match['score'] # ìœ ì‚¬ë„ ì ìˆ˜ (0~1, ë†’ì„ìˆ˜ë¡ ìœ ì‚¬)
                    question = match['metadata'].get('question', '')
                    answer = match['metadata'].get('answer', '')
                    category = match['metadata'].get('category', 'ì¼ë°˜')
                    
                    # â˜… ì„ê³„ê°’ ë¡œì§ ëŒ€í­ ì™„í™” - í•­ìƒ ìµœì†Œ 3ê°œëŠ” ë°˜í™˜í•˜ë„ë¡ ìˆ˜ì •
                    include_result = False
                    
                    if score >= similarity_threshold:
                        include_result = True
                        logging.info(f"ì„ê³„ê°’ í†µê³¼: {score:.3f} >= {similarity_threshold:.2f}")
                    elif i < 3:  # ìƒìœ„ 3ê°œëŠ” ë¬´ì¡°ê±´ í¬í•¨
                        include_result = True
                        logging.info(f"ìƒìœ„ {i+1}ë²ˆì§¸ ê²°ê³¼ë¡œ ê°•ì œ í¬í•¨: {score:.3f}")
                    elif score >= 0.3:  # ë§¤ìš° ë‚®ì€ ì„ê³„ê°’ìœ¼ë¡œ ì¶”ê°€ í¬í•¨
                        include_result = True
                        logging.info(f"ë‚®ì€ ì„ê³„ê°’ í†µê³¼: {score:.3f} >= 0.3")
                    
                    if include_result:
                        filtered_results.append({
                            'score': score,
                            'question': question,
                            'answer': answer,
                            'category': category,
                            'rank': i + 1,
                            'lang': 'ko'  # ì›ë³¸ ë°ì´í„°ëŠ” í•œêµ­ì–´
                        })
                        
                        # ë””ë²„ê¹…ì„ ìœ„í•œ ìƒì„¸ ë¡œê¹…
                        logging.info(f"â˜… í¬í•¨ëœ ìœ ì‚¬ ë‹µë³€ #{i+1}: ì ìˆ˜={score:.3f}, ì¹´í…Œê³ ë¦¬={category}, ì–¸ì–´={lang}")
                        logging.info(f"ì°¸ê³  ì§ˆë¬¸: {question[:50]}...")
                        logging.info(f"ì°¸ê³  ë‹µë³€: {answer[:100]}...")
                    else:
                        logging.info(f"Ã— ì œì™¸ëœ ë‹µë³€ #{i+1}: ì ìˆ˜={score:.3f} (ë„ˆë¬´ ë‚®ìŒ)")
                        
                # 4. ë©”ëª¨ë¦¬ ì •ë¦¬
                del results # ì›ë³¸ ì‘ë‹µ ê°ì²´ ì¦‰ì‹œ ì‚­ì œ (ë©”ëª¨ë¦¬ í•´ì œ)
                if korean_vector is not None:
                    del korean_vector # í•œêµ­ì–´ ë²¡í„° ì¦‰ì‹œ ì‚­ì œ (ë©”ëª¨ë¦¬ í•´ì œ)
                del query_vector # ê²€ìƒ‰ ë²¡í„° ì¦‰ì‹œ ì‚­ì œ (ë©”ëª¨ë¦¬ í•´ì œ)
                
                logging.info(f"ì´ {len(filtered_results)}ê°œì˜ ìœ ì‚¬ ë‹µë³€ ê²€ìƒ‰ ì™„ë£Œ (ì–¸ì–´: {lang})")
                return filtered_results
                
        except Exception as e:
            logging.error(f"Pinecone ê²€ìƒ‰ ì‹¤íŒ¨: {str(e)}, query: {query[:50]}..., lang: {lang}")
            return []

    # â˜† GPTë¥¼ ì‚¬ìš©í•œ ë²ˆì—­
    # Args:
    #     text (str): ë²ˆì—­í•  í…ìŠ¤íŠ¸
    #     source_lang (str): ì›ë³¸ ì–¸ì–´
    #     target_lang (str): ë²ˆì—­ ì–¸ì–´
            
    # Returns:
    #     str: ë²ˆì—­ëœ í…ìŠ¤íŠ¸
    def translate_text(self, text: str, source_lang: str, target_lang: str) -> str:
        try:
            # ì–¸ì–´ ë§¤í•‘
            lang_map = {
                'ko': 'Korean',
                'en': 'English'
            }
            
            system_prompt = f"You are a professional translator. Translate the following text from {lang_map[source_lang]} to {lang_map[target_lang]}. Keep the same tone and style. Only provide the translation without any explanation."
            
            response = self.openai_client.chat.completions.create(
                model='gpt-3.5-turbo',
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": text}
                ],
                max_tokens=600,
                temperature=0.5
            )
            
            return response.choices[0].message.content.strip()
            
        except Exception as e:
            logging.error(f"ë²ˆì—­ ì‹¤íŒ¨: {e}")
            return text

    # â˜† AI ê¸°ë°˜ ì§ˆë¬¸ ì˜ë„ ë¶„ì„ ë©”ì„œë“œ (ì˜ë¯¸ë¡ ì  ë™ë“±ì„± ê°•í™” ë²„ì „)
    def analyze_question_intent(self, query: str) -> dict:
        """AIë¥¼ ì´ìš©í•´ ì§ˆë¬¸ì˜ ë³¸ì§ˆì  ì˜ë„ì™€ í•µì‹¬ ëª©ì ì„ ì •í™•íˆ ë¶„ì„"""
        try:
            with memory_cleanup():
                system_prompt = """ë‹¹ì‹ ì€ ë°”ì´ë¸” ì•± ë¬¸ì˜ ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. 
ê³ ê° ì§ˆë¬¸ì˜ ë³¸ì§ˆì  ì˜ë„ë¥¼ íŒŒì•…í•˜ì—¬ ì˜ë¯¸ë¡ ì ìœ¼ë¡œ ë™ë“±í•œ ì§ˆë¬¸ë“¤ì´ ê°™ì€ ê²°ê³¼ë¥¼ ì–»ë„ë¡ ë¶„ì„í•˜ì„¸ìš”.

ë¶„ì„ ê²°ê³¼ë¥¼ JSON í˜•íƒœë¡œ ë°˜í™˜:

{
  "core_intent": "í•µì‹¬ ì˜ë„ (í‘œì¤€í™”ëœ í˜•íƒœ)",
  "intent_category": "ì˜ë„ ì¹´í…Œê³ ë¦¬",
  "primary_action": "ì£¼ìš” í–‰ë™",
  "target_object": "ëŒ€ìƒ ê°ì²´",
  "constraint_conditions": ["ì œì•½ ì¡°ê±´ë“¤"],
  "standardized_query": "í‘œì¤€í™”ëœ ì§ˆë¬¸ í˜•íƒœ",
  "semantic_keywords": ["ì˜ë¯¸ë¡ ì  í•µì‹¬ í‚¤ì›Œë“œë“¤"]
}

ğŸ¯ ì˜ë¯¸ë¡ ì  ë™ë“±ì„± ë¶„ì„ ê¸°ì¤€:

1. **í•µì‹¬ ì˜ë„ íŒŒì•…**: ì§ˆë¬¸ì˜ ë³¸ì§ˆì  ëª©ì ì´ ë¬´ì—‡ì¸ì§€ íŒŒì•…
   - "ë‘ ë²ˆì—­ë³¸ì„ ë™ì‹œì— ë³´ê³  ì‹¶ë‹¤" â†’ core_intent: "multiple_translations_view"
   - "í…ìŠ¤íŠ¸ë¥¼ ë³µì‚¬í•˜ê³  ì‹¶ë‹¤" â†’ core_intent: "text_copy"
   - "ì—°ì†ìœ¼ë¡œ ë“£ê³  ì‹¶ë‹¤" â†’ core_intent: "continuous_audio_play"

2. **í‘œì¤€í™”ëœ í˜•íƒœë¡œ ë³€í™˜**: êµ¬ì²´ì  ì˜ˆì‹œë¥¼ ì œê±°í•˜ê³  ì¼ë°˜í™”
   - "ìš”í•œë³µìŒ 3ì¥ 16ì ˆ NIVì™€ KJV ë™ì‹œì—" â†’ "ì„œë¡œ ë‹¤ë¥¸ ë²ˆì—­ë³¸ ë™ì‹œ ë³´ê¸°"
   - "ê°œì—­í•œê¸€ê³¼ ê°œì—­ê°œì • ë™ì‹œì—" â†’ "ì„œë¡œ ë‹¤ë¥¸ ë²ˆì—­ë³¸ ë™ì‹œ ë³´ê¸°"

3. **ì˜ë¯¸ë¡ ì  í‚¤ì›Œë“œ ì¶”ì¶œ**: í‘œë©´ì  ë‹¨ì–´ê°€ ì•„ë‹Œ ì˜ë¯¸ì  ê°œë…
   - "ë™ì‹œì—", "í•¨ê»˜", "ë¹„êµí•˜ì—¬", "ë‚˜ë€íˆ" â†’ "simultaneous_view"
   - "NIV", "KJV", "ê°œì—­í•œê¸€", "ë²ˆì—­ë³¸" â†’ "translation_version"

4. **ì œì•½ ì¡°ê±´ ì‹ë³„**: ìš”ì²­ì˜ êµ¬ì²´ì  ì¡°ê±´ë“¤
   - "ì˜ì–´ ë²ˆì—­ë³¸ë§Œ", "í•œê¸€ ë²ˆì—­ë³¸ë§Œ", "íŠ¹ì • ì¥ì ˆ" ë“±

ì˜ˆì‹œ ë¶„ì„:
ì§ˆë¬¸1: "ìš”í•œë³µìŒ 3ì¥ 16ì ˆ ì˜ì–´ ë²ˆì—­ë³¸ NIVì™€ KJV ë™ì‹œì— ë³´ë ¤ë©´?"
ì§ˆë¬¸2: "ê°œì—­í•œê¸€ê³¼ ê°œì—­ê°œì •ì„ ë™ì‹œì— ë³´ë ¤ë©´?"
ì§ˆë¬¸3: "ë‘ ê°œì˜ ë²ˆì—­ë³¸ì„ ì–´ë–»ê²Œ ë™ì‹œì— ë³¼ ìˆ˜ ìˆì£ ?"

â†’ ëª¨ë‘ core_intent: "multiple_translations_simultaneous_view"
â†’ ëª¨ë‘ standardized_query: "ì„œë¡œ ë‹¤ë¥¸ ë²ˆì—­ë³¸ì„ ë™ì‹œì— ë³´ëŠ” ë°©ë²•"
"""

                user_prompt = f"""ë‹¤ìŒ ì§ˆë¬¸ì„ ì˜ë¯¸ë¡ ì ìœ¼ë¡œ ë¶„ì„í•˜ì—¬ ë³¸ì§ˆì  ì˜ë„ë¥¼ íŒŒì•…í•´ì£¼ì„¸ìš”:

ì§ˆë¬¸: {query}

íŠ¹íˆ ë‹¤ìŒ ì‚¬í•­ì— ì§‘ì¤‘í•˜ì„¸ìš”:
1. ì´ ì§ˆë¬¸ì´ ì •ë§ë¡œ ë¬»ê³ ì í•˜ëŠ” ë°”ê°€ ë¬´ì—‡ì¸ê°€?
2. êµ¬ì²´ì  ì˜ˆì‹œ(ì„±ê²½ êµ¬ì ˆ, ë²ˆì—­ë³¸ëª… ë“±)ë¥¼ ì œê±°í•˜ê³  ì¼ë°˜í™”í•˜ë©´?
3. ë¹„ìŠ·í•œ ì˜ë„ì˜ ë‹¤ë¥¸ ì§ˆë¬¸ë“¤ê³¼ ì–´ë–»ê²Œ í†µí•©í•  ìˆ˜ ìˆëŠ”ê°€?"""

                response = self.openai_client.chat.completions.create(
                    model='gpt-3.5-turbo',
                    messages=[
                        {"role": "system", "content": system_prompt},
                        {"role": "user", "content": user_prompt}
                    ],
                    max_tokens=400,
                    temperature=0.2  # ë” ì¼ê´€ì„± ìˆëŠ” ë¶„ì„ì„ ìœ„í•´ ë‚®ì¶¤
                )
                
                result_text = response.choices[0].message.content.strip()
                
                # JSON íŒŒì‹± ì‹œë„
                try:
                    result = json.loads(result_text)
                    logging.info(f"ê°•í™”ëœ ì˜ë„ ë¶„ì„ ê²°ê³¼: {result}")
                    
                    # ê¸°ì¡´ í˜•ì‹ê³¼ì˜ í˜¸í™˜ì„±ì„ ìœ„í•´ ì¶”ê°€ í•„ë“œ ìƒì„±
                    result['intent_type'] = result.get('intent_category', 'ì¼ë°˜ë¬¸ì˜')
                    result['main_topic'] = result.get('target_object', 'ê¸°íƒ€')
                    result['specific_request'] = result.get('standardized_query', query[:100])
                    result['keywords'] = result.get('semantic_keywords', [query[:20]])
                    result['urgency'] = 'medium'
                    result['action_type'] = result.get('primary_action', 'ê¸°íƒ€')
                    
                    return result
                except json.JSONDecodeError:
                    logging.warning(f"JSON íŒŒì‹± ì‹¤íŒ¨, ê¸°ë³¸ê°’ ë°˜í™˜: {result_text}")
                    return {
                        "core_intent": "general_inquiry",
                        "intent_category": "ì¼ë°˜ë¬¸ì˜",
                        "primary_action": "ê¸°íƒ€",
                        "target_object": "ê¸°íƒ€",
                        "constraint_conditions": [],
                        "standardized_query": query,
                        "semantic_keywords": [query[:20]],
                        # ê¸°ì¡´ í˜¸í™˜ì„± í•„ë“œ
                        "intent_type": "ì¼ë°˜ë¬¸ì˜",
                        "main_topic": "ê¸°íƒ€",
                        "specific_request": query[:100],
                        "keywords": [query[:20]],
                        "urgency": "medium",
                        "action_type": "ê¸°íƒ€"
                    }
                
        except Exception as e:
            logging.error(f"ê°•í™”ëœ ì˜ë„ ë¶„ì„ ì‹¤íŒ¨: {e}")
            return {
                "core_intent": "general_inquiry",
                "intent_category": "ì¼ë°˜ë¬¸ì˜", 
                "primary_action": "ê¸°íƒ€",
                "target_object": "ê¸°íƒ€",
                "constraint_conditions": [],
                "standardized_query": query,
                "semantic_keywords": [query[:20]],
                # ê¸°ì¡´ í˜¸í™˜ì„± í•„ë“œ
                "intent_type": "ì¼ë°˜ë¬¸ì˜",
                "main_topic": "ê¸°íƒ€",
                "specific_request": query[:100],
                "keywords": [query[:20]],
                "urgency": "medium",
                "action_type": "ê¸°íƒ€"
            }

    # â˜† ê²€ìƒ‰ëœ ìœ ì‚¬ ë‹µë³€ë“¤ì˜ í’ˆì§ˆì„ ë¶„ì„í•˜ì—¬ ìµœì ì˜ ë‹µë³€ ìƒì„± ì „ëµì„ ê²°ì •í•˜ëŠ” ë©”ì„œë“œ

    # Args:
    #     similar_answers (list): ê²€ìƒ‰ëœ ìœ ì‚¬ ë‹µë³€ ë¦¬ìŠ¤íŠ¸
    #     query (str): ì›ë³¸ ì§ˆë¬¸
    #     
    # Returns:
    #     dict: ë¶„ì„ ê²°ê³¼ ë° ê¶Œì¥ ì ‘ê·¼ ë°©ì‹
    def analyze_context_quality(self, similar_answers: list, query: str) -> dict:
        # ìœ ì‚¬ ë‹µë³€ì´ ì—†ìœ¼ë©´ ê¸°ë³¸ê°’ ë°˜í™˜
        if not similar_answers:
            return {
                'has_good_context': False,
                'best_score': 0.0,
                'recommended_approach': 'fallback',
                'quality_level': 'none'
            }
        
        # ğŸ”¥ AI ê¸°ë°˜ ì§ˆë¬¸ ì˜ë„ ë¶„ì„ ì¶”ê°€
        question_analysis = self.analyze_question_intent(query)
        question_type = question_analysis.get('intent_type', 'ì¼ë°˜ë¬¸ì˜')
        logging.info(f"AI ë¶„ì„ ê²°ê³¼: {question_analysis}")
        
        # í’ˆì§ˆ ì§€í‘œ ê³„ì‚°
        best_score = similar_answers[0]['score']
        high_quality_count = len([ans for ans in similar_answers if ans['score'] >= 0.7])
        medium_quality_count = len([ans for ans in similar_answers if 0.5 <= ans['score'] < 0.7])
        
        # ğŸ”¥ ì¹´í…Œê³ ë¦¬ ì¼ì¹˜ë„ ë¶„ì„ ì¶”ê°€
        categories = [ans['category'] for ans in similar_answers[:5]]
        category_distribution = {cat: categories.count(cat) for cat in set(categories)}
        
        # ğŸ”¥ ì§ˆë¬¸ ì˜ë„ì™€ ë‹µë³€ ì¹´í…Œê³ ë¦¬ ì¼ì¹˜ë„ ê²€ì‚¬
        context_relevance = self.check_context_relevance_ai(question_analysis, categories, query, similar_answers[:3])
        logging.info(f"ì»¨í…ìŠ¤íŠ¸ ê´€ë ¨ì„±: {context_relevance}")
        
        # ğŸ”¥ ì•ˆì •í™”ëœ ì˜ì‚¬ ê²°ì • íŠ¸ë¦¬ - ëª…í™•í•œ ê¸°ì¤€ê³¼ ë²„í¼ ì ìš©
        
        # ê´€ë ¨ì„±ë³„ ê°€ì¤‘ì¹˜ ì ìš©
        relevance_weights = {
            'high': 1.0,
            'medium': 0.8,
            'low': 0.6,
            'irrelevant': 0.0
        }
        
        adjusted_score = best_score * relevance_weights.get(context_relevance, 0.5)
        logging.info(f"ì¡°ì •ëœ ì ìˆ˜: {best_score:.3f} * {relevance_weights.get(context_relevance, 0.5)} = {adjusted_score:.3f}")
        
        # ë” ëª…í™•í•˜ê³  ì•ˆì •ì ì¸ ê¸°ì¤€ ì ìš©
        if context_relevance == 'irrelevant':
            approach = 'fallback'
            logging.warning(f"ê´€ë ¨ì„± ì—†ìŒ - í´ë°± ì²˜ë¦¬")
        elif adjusted_score >= 0.85:  # ë§¤ìš° ë†’ì€ ì‹ ë¢°ë„
            approach = 'direct_use'
            logging.info(f"ë§¤ìš° ë†’ì€ ì‹ ë¢°ë„ - ì§ì ‘ ì‚¬ìš©")
        elif adjusted_score >= 0.65:  # ë†’ì€ ì‹ ë¢°ë„
            approach = 'gpt_with_strong_context'
            logging.info(f"ë†’ì€ ì‹ ë¢°ë„ - ê°•í•œ ì»¨í…ìŠ¤íŠ¸ë¡œ GPT ìƒì„±")
        elif adjusted_score >= 0.4:   # ì¤‘ê°„ ì‹ ë¢°ë„
            approach = 'gpt_with_weak_context'
            logging.info(f"ì¤‘ê°„ ì‹ ë¢°ë„ - ì•½í•œ ì»¨í…ìŠ¤íŠ¸ë¡œ GPT ìƒì„±")
        else:                         # ë‚®ì€ ì‹ ë¢°ë„
            approach = 'fallback'
            logging.info(f"ë‚®ì€ ì‹ ë¢°ë„ - í´ë°± ì²˜ë¦¬")
        
        # ğŸ”¥ ì¶”ê°€ ì•ˆì •ì„± ê²€ì‚¬ - ê³ í’ˆì§ˆ ë‹µë³€ ê°œìˆ˜ ê³ ë ¤
        if high_quality_count >= 3 and approach == 'fallback':
            approach = 'gpt_with_weak_context'  # ê³ í’ˆì§ˆ ë‹µë³€ì´ ë§ìœ¼ë©´ GPT ì‚¬ìš©
            logging.info(f"ê³ í’ˆì§ˆ ë‹µë³€ {high_quality_count}ê°œë¡œ ì¸í•´ GPT ìƒì„±ìœ¼ë¡œ ë³€ê²½")
        
        # ë¶„ì„ ê²°ê³¼ êµ¬ì¡°í™”
        analysis = {
            'has_good_context': context_relevance in ['high', 'medium'] and best_score >= 0.4,
            'best_score': best_score,
            'high_quality_count': high_quality_count,
            'medium_quality_count': medium_quality_count,
            'category_distribution': category_distribution,
            'recommended_approach': approach,
            'question_analysis': question_analysis,
            'question_type': question_type,
            'context_relevance': context_relevance,
            'context_summary': f"ì˜ë„: {question_type}, ì£¼ì œ: {question_analysis.get('main_topic', 'N/A')}, ê´€ë ¨ì„±: {context_relevance}, ìµœê³ ì ìˆ˜: {best_score:.3f}"
        }
        
        logging.info(f"í–¥ìƒëœ ì»¨í…ìŠ¤íŠ¸ ë¶„ì„ ê²°ê³¼: {analysis}")
        return analysis

    # â˜† AI ê¸°ë°˜ ì»¨í…ìŠ¤íŠ¸ ê´€ë ¨ì„± ê²€ì‚¬ ë©”ì„œë“œ (ì •í™•ë„ ê°•í™” ë²„ì „)
    def check_context_relevance_ai(self, question_analysis: dict, answer_categories: list, query: str, top_answers: list) -> str:
        """AIë¥¼ ì´ìš©í•´ ì§ˆë¬¸ ì˜ë„ì™€ ë‹µë³€ì˜ ê´€ë ¨ì„±ì„ ì§€ëŠ¥ì ìœ¼ë¡œ ê²€ì‚¬"""
        
        try:
            # ìƒìœ„ ë‹µë³€ë“¤ì˜ ë‚´ìš© ìš”ì•½
            answer_summaries = []
            for i, answer in enumerate(top_answers[:3]):
                answer_text = answer.get('answer', '')[:200]  # ì²« 200ìë§Œ
                answer_summaries.append(f"ë‹µë³€{i+1}: {answer_text}")
            
            combined_answers = "\n".join(answer_summaries)
            
            with memory_cleanup():
                system_prompt = """ë‹¹ì‹ ì€ ë¬¸ì˜-ë‹µë³€ ê´€ë ¨ì„± ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
ê³ ê°ì˜ ì§ˆë¬¸ ì˜ë„ì™€ ê²€ìƒ‰ëœ ë‹µë³€ë“¤ì˜ ê´€ë ¨ì„±ì„ ë¶„ì„í•˜ì—¬ ë‹¤ìŒ ì¤‘ í•˜ë‚˜ë¡œ íŒì •í•˜ì„¸ìš”:

- "high": ë‹µë³€ì´ ì§ˆë¬¸ê³¼ ì§ì ‘ì ìœ¼ë¡œ ê´€ë ¨ë˜ê³  ë„ì›€ì´ ë¨
- "medium": ë‹µë³€ì´ ì–´ëŠ ì •ë„ ê´€ë ¨ì´ ìˆì§€ë§Œ ì™„ì „íˆ ì¼ì¹˜í•˜ì§€ëŠ” ì•ŠìŒ  
- "low": ë‹µë³€ì´ ì•½ê°„ ê´€ë ¨ì´ ìˆì§€ë§Œ ì§ˆë¬¸ì˜ í•µì‹¬ê³¼ëŠ” ê±°ë¦¬ê°€ ìˆìŒ
- "irrelevant": ë‹µë³€ì´ ì§ˆë¬¸ê³¼ ì „í˜€ ê´€ë ¨ì´ ì—†ìŒ

âš ï¸ ì—„ê²©í•œ ë¶„ì„ ê¸°ì¤€:
1. í–‰ë™ ìœ í˜• ì¼ì¹˜ ì—¬ë¶€ (ë³µì‚¬â‰ ì¬ìƒ, í…ìŠ¤íŠ¸â‰ ìŒì„±)
2. ì£¼ì œ ì˜ì—­ ì¼ì¹˜ ì—¬ë¶€ (ì•±ê¸°ëŠ¥, ì„±ê²½ë³¸ë¬¸, ê¸°ìˆ ì§€ì› ë“±)
3. ì§ˆë¬¸ì˜ í•µì‹¬ í‚¤ì›Œë“œì™€ ë‹µë³€ í‚¤ì›Œë“œì˜ ì˜ë¯¸ì  ì¼ì¹˜ì„±
4. ì‹¤ì œ ë¬¸ì œ í•´ê²° ë„ì›€ ì—¬ë¶€

ğŸš« íŠ¹ë³„ ì£¼ì˜ì‚¬í•­:
- í…ìŠ¤íŠ¸ ë³µì‚¬/ë¶™ì—¬ë„£ê¸° ì§ˆë¬¸ì— ìŒì„± ì¬ìƒ ë‹µë³€ â†’ "irrelevant"
- ìŒì„± ì¬ìƒ ì§ˆë¬¸ì— í…ìŠ¤íŠ¸ ë³µì‚¬ ë‹µë³€ â†’ "irrelevant"  
- ê²€ìƒ‰ ê¸°ëŠ¥ ì§ˆë¬¸ì— ì„¤ì • ë³€ê²½ ë‹µë³€ â†’ "irrelevant"
- ì˜¤ë¥˜ ì‹ ê³ ì— ì¼ë°˜ ì‚¬ìš©ë²• ë‹µë³€ â†’ "low" ë˜ëŠ” "irrelevant"

ê²°ê³¼ëŠ” "high", "medium", "low", "irrelevant" ì¤‘ í•˜ë‚˜ë§Œ ë°˜í™˜í•˜ì„¸ìš”."""

                user_prompt = f"""ì§ˆë¬¸ ë¶„ì„ ê²°ê³¼:
ì˜ë„: {question_analysis.get('intent_type', 'N/A')}
ì£¼ì œ: {question_analysis.get('main_topic', 'N/A')}
í–‰ë™ìœ í˜•: {question_analysis.get('action_type', 'N/A')}
êµ¬ì²´ì  ìš”ì²­: {question_analysis.get('specific_request', 'N/A')}

ì›ë³¸ ì§ˆë¬¸: {query}

ê²€ìƒ‰ëœ ë‹µë³€ë“¤:
{combined_answers}

âš ï¸ ì¤‘ìš”: ì§ˆë¬¸ì˜ í–‰ë™ìœ í˜•ê³¼ ë‹µë³€ì˜ í–‰ë™ìœ í˜•ì´ ë‹¤ë¥´ë©´ "irrelevant"ë¡œ íŒì •í•˜ì„¸ìš”.
ìœ„ ì§ˆë¬¸ê³¼ ë‹µë³€ë“¤ì˜ ê´€ë ¨ì„±ì„ ë¶„ì„í•´ì£¼ì„¸ìš”."""

                response = self.openai_client.chat.completions.create(
                    model='gpt-3.5-turbo',
                    messages=[
                        {"role": "system", "content": system_prompt},
                        {"role": "user", "content": user_prompt}
                    ],
                    max_tokens=50,
                    temperature=0.2
                )
                
                result = response.choices[0].message.content.strip().lower()
                
                # ê²°ê³¼ ì •ê·œí™”
                if 'high' in result:
                    return 'high'
                elif 'medium' in result:
                    return 'medium'
                elif 'low' in result:
                    return 'low'
                elif 'irrelevant' in result:
                    return 'irrelevant'
                else:
                    logging.warning(f"AI ê´€ë ¨ì„± ë¶„ì„ ê²°ê³¼ íŒŒì‹± ì‹¤íŒ¨: {result}")
                    return 'medium'  # ê¸°ë³¸ê°’
                    
        except Exception as e:
            logging.error(f"AI ê´€ë ¨ì„± ë¶„ì„ ì‹¤íŒ¨: {e}")
            # í´ë°±: ê¸°ë³¸ì ì¸ í‚¤ì›Œë“œ ë§¤ì¹­
            return self.fallback_relevance_check(query, top_answers)
    
    # â˜† í´ë°± ê´€ë ¨ì„± ê²€ì‚¬ ë©”ì„œë“œ (ì˜ë¯¸ë¡ ì  ë§¤ì¹­ ê°•í™”)
    def fallback_relevance_check(self, query: str, top_answers: list) -> str:
        """AI ë¶„ì„ ì‹¤íŒ¨ ì‹œ ì‚¬ìš©í•˜ëŠ” ì˜ë¯¸ë¡ ì  í‚¤ì›Œë“œ ë§¤ì¹­"""
        
        # ì˜ë¯¸ë¡ ì  í‚¤ì›Œë“œ ê·¸ë£¹ ì •ì˜
        semantic_groups = {
            'text_copy': ['ë³µì‚¬', 'ë¶™ì—¬ë„£ê¸°', 'ì›Œë“œ', 'í…ìŠ¤íŠ¸', 'ë³µì‚¬í•´ì„œ', 'ì˜®ê¸°', 'ë‚´ë³´ë‚´ê¸°', 'ì €ì¥'],
            'audio_play': ['ì¬ìƒ', 'ë“£ê¸°', 'ìŒì„±', 'ì†Œë¦¬', 'ì—°ì†ì¬ìƒ', 'ë°˜ë³µ', 'ë“¤ì„', 'ë“£ê³ '],
            'search_find': ['ê²€ìƒ‰', 'ì°¾ê¸°', 'ì°¾ì•„ì„œ', 'ê²€ìƒ‰í•´ì„œ', 'ì°¾ì„', 'ì°¾ëŠ”'],
            'download': ['ë‹¤ìš´ë¡œë“œ', 'ë‹¤ìš´ë°›ê¸°', 'ë°›ê¸°', 'ì €ì¥'],
            'error_report': ['ì˜¤ë¥˜', 'ì—ëŸ¬', 'ì•ˆë¨', 'ì•ˆë˜', 'ë¬¸ì œ', 'ê³ ì¥', 'ë²„ê·¸'],
            'setting_config': ['ì„¤ì •', 'ë³€ê²½', 'ì¡°ì •', 'ì˜µì…˜', 'í™˜ê²½ì„¤ì •']
        }
        
        # ì§ˆë¬¸ì—ì„œ ì˜ë¯¸ ê·¸ë£¹ ì‹ë³„
        query_lower = query.lower()
        query_semantic_groups = set()
        for group_name, keywords in semantic_groups.items():
            if any(keyword in query_lower for keyword in keywords):
                query_semantic_groups.add(group_name)
        
        max_relevance = 0
        for answer in top_answers:
            answer_lower = answer.get('answer', '').lower()
            answer_semantic_groups = set()
            
            # ë‹µë³€ì—ì„œ ì˜ë¯¸ ê·¸ë£¹ ì‹ë³„
            for group_name, keywords in semantic_groups.items():
                if any(keyword in answer_lower for keyword in keywords):
                    answer_semantic_groups.add(group_name)
            
            # ì˜ë¯¸ ê·¸ë£¹ ì¼ì¹˜ë„ ê³„ì‚°
            if query_semantic_groups and answer_semantic_groups:
                semantic_overlap = len(query_semantic_groups & answer_semantic_groups)
                semantic_total = len(query_semantic_groups | answer_semantic_groups)
                semantic_ratio = semantic_overlap / semantic_total if semantic_total > 0 else 0
            else:
                semantic_ratio = 0
            
            # í‚¤ì›Œë“œ ì¼ì¹˜ë„ë„ í•¨ê»˜ ê³ ë ¤
            query_words = set(self.extract_keywords(query_lower))
            answer_words = set(self.extract_keywords(answer_lower))
            keyword_overlap = len(query_words & answer_words)
            keyword_ratio = keyword_overlap / max(len(query_words), 1)
            
            # ì˜ë¯¸ë¡ ì  ë§¤ì¹­ê³¼ í‚¤ì›Œë“œ ë§¤ì¹­ì„ ì¡°í•© (ì˜ë¯¸ë¡ ì  ë§¤ì¹­ì— ë” ë†’ì€ ê°€ì¤‘ì¹˜)
            combined_relevance = semantic_ratio * 0.7 + keyword_ratio * 0.3
            max_relevance = max(max_relevance, combined_relevance)
        
        # ğŸš« ì˜ë¯¸ ê·¸ë£¹ì´ ì™„ì „íˆ ë‹¤ë¥¸ ê²½ìš° irrelevant ì²˜ë¦¬
        if query_semantic_groups and any(answer.get('answer', '') for answer in top_answers):
            all_answer_groups = set()
            for answer in top_answers:
                answer_lower = answer.get('answer', '').lower()
                for group_name, keywords in semantic_groups.items():
                    if any(keyword in answer_lower for keyword in keywords):
                        all_answer_groups.add(group_name)
            
            # í…ìŠ¤íŠ¸-ìŒì„±, ê²€ìƒ‰-ì„¤ì • ë“± ìƒë°˜ëœ ê·¸ë£¹ì¸ ê²½ìš°
            conflicting_pairs = [
                ('text_copy', 'audio_play'),
                ('search_find', 'setting_config'),
                ('error_report', 'search_find')
            ]
            
            for q_group in query_semantic_groups:
                for a_group in all_answer_groups:
                    if (q_group, a_group) in conflicting_pairs or (a_group, q_group) in conflicting_pairs:
                        return 'irrelevant'
        
        # ê´€ë ¨ì„± ì ìˆ˜ì— ë”°ë¥¸ íŒì •
        if max_relevance >= 0.6:
            return 'high'
        elif max_relevance >= 0.4:
            return 'medium'
        elif max_relevance >= 0.2:
            return 'low'
        else:
            return 'irrelevant'
    
    # â˜† í•µì‹¬ í‚¤ì›Œë“œ ì¶”ì¶œ ë©”ì„œë“œ
    def extract_keywords(self, text: str) -> list:
        """í…ìŠ¤íŠ¸ì—ì„œ í•µì‹¬ í‚¤ì›Œë“œ ì¶”ì¶œ"""
        # ë¶ˆìš©ì–´ ì œê±°ìš© ë¦¬ìŠ¤íŠ¸
        stop_words = {'ëŠ”', 'ì€', 'ì´', 'ê°€', 'ì„', 'ë¥¼', 'ì—', 'ì—ì„œ', 'ë¡œ', 'ìœ¼ë¡œ', 'ì™€', 'ê³¼', 'ì˜', 'ë„', 'ë§Œ', 'ê¹Œì§€', 'ë¶€í„°', 'ê»˜ì„œ', 'ì—ê²Œ', 'í•œí…Œ', 'ë¡œë¶€í„°', 'ìœ¼ë¡œë¶€í„°'}
        
        # íŠ¹ìˆ˜ë¬¸ì ì œê±° ë° ë‹¨ì–´ ë¶„ë¦¬
        
        words = re.findall(r'[ê°€-í£a-zA-Z0-9]+', text)
        
        # ë¶ˆìš©ì–´ ì œê±° ë° 2ê¸€ì ì´ìƒ ë‹¨ì–´ë§Œ ì„ íƒ
        keywords = [word for word in words if len(word) >= 2 and word not in stop_words]
        
        return keywords
    

    # â˜† ì°¸ê³  ë‹µë³€ì—ì„œ ì¸ì‚¬ë§ê³¼ ëë§ºìŒë§ì„ ì œê±°í•˜ëŠ” ë©”ì„œë“œ
    # Args:
    #     text (str): ì œê±°í•  í…ìŠ¤íŠ¸
    #     lang (str): ì–¸ì–´ (ê¸°ë³¸ê°’: í•œêµ­ì–´)
            
    # Returns:
    #     str: ì œê±°ëœ í…ìŠ¤íŠ¸
    def remove_greeting_and_closing(self, text: str, lang: str = 'ko') -> str:
        # null ì²´í¬
        if not text:
            return ""
        
        if lang == 'ko':
            # í•œêµ­ì–´ ì¸ì‚¬ë§ ì œê±° íŒ¨í„´
            greeting_patterns = [
                r'^ì•ˆë…•í•˜ì„¸ìš”[^.]*\.\s*',
                r'^GOODTV\s+ë°”ì´ë¸”\s*ì• í”Œ[^.]*\.\s*',
                r'^ë°”ì´ë¸”\s*ì• í”Œ[^.]*\.\s*',
                r'^ì„±ë„ë‹˜[^.]*\.\s*',
                r'^ê³ ê°ë‹˜[^.]*\.\s*',
                r'^ê°ì‚¬í•©ë‹ˆë‹¤[^.]*\.\s*',
                r'^ê°ì‚¬ë“œë¦½ë‹ˆë‹¤[^.]*\.\s*',
                r'^ë°”ì´ë¸”\s*ì• í”Œì„\s*ì´ìš©í•´ì£¼ì…”ì„œ[^.]*\.\s*',
                r'^ë°”ì´ë¸”\s*ì• í”Œì„\s*ì• ìš©í•´\s*ì£¼ì…”ì„œ[^.]*\.\s*'
            ]
            
            # í•œêµ­ì–´ ëë§ºìŒë§ ì œê±° íŒ¨í„´ë“¤
            closing_patterns = [
                r'\s*ê°ì‚¬í•©ë‹ˆë‹¤[^.]*\.?\s*$',
                r'\s*ê°ì‚¬ë“œë¦½ë‹ˆë‹¤[^.]*\.?\s*$',
                r'\s*í‰ì•ˆí•˜ì„¸ìš”[^.]*\.?\s*$',
                r'\s*ì£¼ë‹˜\s*ì•ˆì—ì„œ[^.]*\.?\s*$',
                r'\s*í•¨ê»˜\s*ê¸°ë„í•˜ë©°[^.]*\.?\s*$',
                r'\s*í•­ìƒ[^.]*ë°”ì´ë¸”\s*ì• í”Œ[^.]*\.?\s*$',
                r'\s*í•­ìƒ\s*ì£¼ë‹˜\s*ì•ˆì—ì„œ[^.]*\.?\s*$',
                r'\s*ì£¼ë‹˜\s*ì•ˆì—ì„œ\s*í‰ì•ˆí•˜ì„¸ìš”[^.]*\.?\s*$',
                r'\s*ì£¼ë‹˜ì˜\s*ì€ì´ì´[^.]*\.?\s*$',
                r'\s*ê¸°ë„ë“œë¦¬ê² ìŠµë‹ˆë‹¤[^.]*\.?\s*$'
            ]

        else:  # ì˜ì–´ ì¸ì‚¬ë§ ì œê±° íŒ¨í„´
            greeting_patterns = [
                r'^Hello[^.]*\.\s*',
                r'^Hi[^.]*\.\s*',
                r'^Dear[^.]*\.\s*',
                r'^Thank you[^.]*\.\s*',
                r'^Thanks[^.]*\.\s*',
                r'^This is GOODTV Bible App[^.]*\.\s*',
            ]
            
            # ì˜ì–´ ëë§ºìŒë§ ì œê±° íŒ¨í„´
            closing_patterns = [
                r'\s*Thank you[^.]*\.?\s*$',
                r'\s*Thanks[^.]*\.?\s*$',
                r'\s*Best regards[^.]*\.?\s*$',
                r'\s*Sincerely[^.]*\.?\s*$',
                r'\s*God bless[^.]*\.?\s*$',
                r'\s*May God[^.]*\.?\s*$',
            ]
        
        # ì¸ì‚¬ë§ ì œê±°
        for pattern in greeting_patterns:
            text = re.sub(pattern, '', text, flags=re.IGNORECASE)
        
        # ëë§ºìŒë§ ì œê±°
        for pattern in closing_patterns:
            text = re.sub(pattern, '', text, flags=re.IGNORECASE)
        
        # ë¬¸ì¥ ëì˜ ëë§ºìŒë§ë“¤ë„ ì œê±°
        text = re.sub(r'[,.!?]\s*í•­ìƒ\s*ì£¼ë‹˜\s*ì•ˆì—ì„œ[^.]*\.?\s*$', '', text, flags=re.IGNORECASE)
        text = re.sub(r'[,.!?]\s*ê°ì‚¬í•©ë‹ˆë‹¤[^.]*\.?\s*$', '', text, flags=re.IGNORECASE)
        text = re.sub(r'[,.!?]\s*í‰ì•ˆí•˜ì„¸ìš”[^.]*\.?\s*$', '', text, flags=re.IGNORECASE)
        
        # ì•ë’¤ ê³µë°± ì •ë¦¬
        text = text.strip()
        
        return text

    # â˜† GPT ë‹µë³€ ìƒì„±ì„ ìœ„í•œ í–¥ìƒëœ ì»¨í…ìŠ¤íŠ¸ ìƒì„± ë©”ì„œë“œ
    # 
    # ì»¨í…ìŠ¤íŠ¸ ìƒì„± ì „ëµ:
    # 1. í’ˆì§ˆë³„ ë‹µë³€ ê·¸ë£¹í•‘ (ê³ /ì¤‘/ë‚®ì€ í’ˆì§ˆ)
    # 2. ê³ í’ˆì§ˆ ë‹µë³€ ìš°ì„  ì„ íƒ (ìµœëŒ€ 4ê°œ)
    # 3. ì¤‘í’ˆì§ˆ ë‹µë³€ìœ¼ë¡œ ë³´ì™„ (ìµœëŒ€ 3ê°œ)
    # 4. ìµœì†Œ ê°œìˆ˜ ë¯¸ë‹¬ì‹œ ì¤‘ê°„ í’ˆì§ˆ ë‹µë³€ ì¶”ê°€
    # 5. í…ìŠ¤íŠ¸ ì •ì œ ë° ê¸¸ì´ ì œí•œ (ì¸ì‚¬ë§/ëë§ºìŒë§ ì œê±°)
    # 
    # ì´ë ‡ê²Œ êµ¬ì„±ëœ ì»¨í…ìŠ¤íŠ¸ëŠ” GPTì—ê²Œ ì°¸ê³  ìë£Œë¡œ ì œê³µë˜ì–´
    # ì¼ê´€ëœ ìŠ¤íƒ€ì¼ê³¼ ì •í™•í•œ ì •ë³´ë¡œ ë‹µë³€ì„ ìƒì„±í•˜ê²Œ í•¨
    # 
    # Args:
    #     similar_answers (list): ê²€ìƒ‰ëœ ìœ ì‚¬ ë‹µë³€ ë¦¬ìŠ¤íŠ¸
    #     max_answers (int): í¬í•¨í•  ìµœëŒ€ ë‹µë³€ ê°œìˆ˜ (ê¸°ë³¸ê°’: 7)
    #     
    # Returns:
    #     str: GPTìš© ì»¨í…ìŠ¤íŠ¸ ë¬¸ìì—´
    def create_enhanced_context(self, similar_answers: list, max_answers: int = 7, target_lang: str = 'ko') -> str:
        if not similar_answers:
            return ""
        
        context_parts = [] # ì»¨í…ìŠ¤íŠ¸ ë¶€ë¶„ë“¤ì„ ì €ì¥í•  ë¦¬ìŠ¤íŠ¸
        used_answers = 0 # ì‚¬ìš©ëœ ë‹µë³€ ê°œìˆ˜
        
        # ìœ ì‚¬ë„ ì ìˆ˜ì— ë”°ë¥¸ ë‹µë³€ ê·¸ë£¹í•‘
        # ê³„ì¸µì  ê·¸ë£¹í•‘ìœ¼ë¡œ í’ˆì§ˆë³„ ë‹µë³€ì„ ë¶„ë¥˜: ì´ëŠ” ë‚˜ì¤‘ì— ìš°ì„ ìˆœìœ„ì— ë”°ë¼ ì„ íƒí•˜ê¸° ìœ„í•¨
        high_score = [ans for ans in similar_answers if ans['score'] >= 0.7]      # ê³ í’ˆì§ˆ (70% ì´ìƒ ìœ ì‚¬)
        medium_score = [ans for ans in similar_answers if 0.5 <= ans['score'] < 0.7]  # ì¤‘í’ˆì§ˆ (50-70%)
        medium_low_score = [ans for ans in similar_answers if 0.5 <= ans['score'] < 0.6] # ë‚®ì€ í’ˆì§ˆ (50-60%)

        # 1ë‹¨ê³„: ê³ í’ˆì§ˆ ë‹µë³€ ìš°ì„  í¬í•¨ (ìµœëŒ€ 4ê°œ)
        for ans in high_score[:4]:
            if used_answers >= max_answers:
                break
            # ì œì–´ ë¬¸ì(ì¤„ë°”ê¿ˆ, íƒ­, ê°œí–‰ ë“±) ë° HTML íƒœê·¸ ì œê±°
            clean_answer = re.sub(r'[\b\r\f\v\x00-\x08\x0B\x0C\x0E-\x1F\x7F]|<[^>]+>', '', ans['answer'])
            clean_answer = self.remove_greeting_and_closing(clean_answer, 'ko') # ì¸ì‚¬ë§ê³¼ ëë§ºìŒë§ ì œê±°í•˜ì—¬ ë³¸ë¬¸ë§Œ ì¶”ì¶œ
            
            # ì˜ì–´ ì§ˆë¬¸ì¸ ê²½ìš° ë‹µë³€ì„ ë²ˆì—­
            if target_lang == 'en' and ans.get('lang', 'ko') == 'ko':
                clean_answer = self.translate_text(clean_answer, 'ko', 'en')
            
            # ìœ íš¨ì„± ê²€ì‚¬ ë° ê¸¸ì´ ì œí•œ (ìµœì†Œ 20ì ì´ìƒí™•ì¸, 400ìë¡œ ì˜ë¼ì„œ ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´ ì œí•œ ë°©ì§€)
            if self.is_valid_text(clean_answer, target_lang) and len(clean_answer.strip()) > 20:
                context_parts.append(f"[ì°¸ê³ ë‹µë³€ {used_answers+1} - ì ìˆ˜: {ans['score']:.2f}]\n{clean_answer[:400]}")
                used_answers += 1
        
        # 2ë‹¨ê³„: ì¤‘í’ˆì§ˆ ë‹µë³€ìœ¼ë¡œ ë³´ì™„ (ìµœëŒ€ 3ê°œ)
        for ans in medium_score[:3]:
            if used_answers >= max_answers:
                break
            # ì œì–´ ë¬¸ì(ì¤„ë°”ê¿ˆ, íƒ­, ê°œí–‰ ë“±) ë° HTML íƒœê·¸ ì œê±°
            clean_answer = re.sub(r'[\b\r\f\v\x00-\x08\x0B\x0C\x0E-\x1F\x7F]|<[^>]+>', '', ans['answer'])
            clean_answer = self.remove_greeting_and_closing(clean_answer, 'ko') # ì¸ì‚¬ë§ê³¼ ëë§ºìŒë§ ì œê±°í•˜ì—¬ ë³¸ë¬¸ë§Œ ì¶”ì¶œ
            
            # ì˜ì–´ ì§ˆë¬¸ì¸ ê²½ìš° ë‹µë³€ì„ ë²ˆì—­
            if target_lang == 'en' and ans.get('lang', 'ko') == 'ko':
                clean_answer = self.translate_text(clean_answer, 'ko', 'en')
            
            if self.is_valid_text(clean_answer, target_lang) and len(clean_answer.strip()) > 20:
                context_parts.append(f"[ì°¸ê³ ë‹µë³€ {used_answers+1} - ì ìˆ˜: {ans['score']:.2f}]\n{clean_answer[:300]}")
                used_answers += 1

        # 3ë‹¨ê³„: ë‹µë³€ì´ ë¶€ì¡±í•œ ê²½ìš° ì¤‘ê°„ í’ˆì§ˆ ë‹µë³€ ì¶”ê°€ (50-60% êµ¬ê°„)
        if used_answers < 3:  # ìµœì†Œ 3ê°œ ì´ìƒ í™•ë³´í•˜ê¸° ìœ„í•¨
            for ans in medium_low_score[:2]:
                if used_answers >= max_answers:
                    break
                clean_answer = re.sub(r'[\b\r\f\v\x00-\x08\x0B\x0C\x0E-\x1F\x7F]|<[^>]+>', '', ans['answer'])
                clean_answer = self.remove_greeting_and_closing(clean_answer, 'ko')
                
                # ì˜ì–´ ì§ˆë¬¸ì¸ ê²½ìš° ë‹µë³€ì„ ë²ˆì—­
                if target_lang == 'en' and ans.get('lang', 'ko') == 'ko':
                    clean_answer = self.translate_text(clean_answer, 'ko', 'en')
                
                if self.is_valid_text(clean_answer, target_lang) and len(clean_answer.strip()) > 20:
                    context_parts.append(f"[ì°¸ê³ ë‹µë³€ {used_answers+1} - ì ìˆ˜: {ans['score']:.2f}]\n{clean_answer[:250]}")
                    used_answers += 1
        
        logging.info(f"ì»¨í…ìŠ¤íŠ¸ ìƒì„±: {used_answers}ê°œì˜ ë‹µë³€ í¬í•¨ (ì–¸ì–´: {target_lang})")
        
        # ìµœì¢… ì»¨í…ìŠ¤íŠ¸ ì¡°í•© (êµ¬ë¶„ì„ ìœ¼ë¡œ ë‹µë³€ë“¤ ë¶„ë¦¬)
        return "\n\n" + "="*50 + "\n\n".join(context_parts)

    # â˜† ì´ì „ ì•± ì´ë¦„ì„ ì œê±°í•˜ëŠ” ë©”ì„œë“œ (êµ¬ ë‹¤ë²ˆì—­ì„±ê²½ì°¬ì†¡ ë“±)
    # Args:
    #     text (str): ì œê±°í•  í…ìŠ¤íŠ¸
            
    # Returns:
    #     str: ì œê±°ëœ í…ìŠ¤íŠ¸
    def remove_old_app_name(self, text: str) -> str:
        patterns_to_remove = [
            r'\s*\(êµ¬\)\s*ë‹¤ë²ˆì—­ì„±ê²½ì°¬ì†¡',
            r'\s*\(êµ¬\)ë‹¤ë²ˆì—­ì„±ê²½ì°¬ì†¡',
            r'ë°”ì´ë¸”\s*ì• í”Œ\s*\(êµ¬\)\s*ë‹¤ë²ˆì—­ì„±ê²½ì°¬ì†¡',
            r'ë°”ì´ë¸”ì• í”Œ\s*\(êµ¬\)ë‹¤ë²ˆì—­ì„±ê²½ì°¬ì†¡',
        ]
        
        for pattern in patterns_to_remove:
            text = re.sub(pattern, '', text, flags=re.IGNORECASE)
        
        text = re.sub(r'(GOODTV\s+ë°”ì´ë¸”\s*ì• í”Œ)\s+', r'\1', text)
        
        return text

    # â˜† ë‹µë³€ í…ìŠ¤íŠ¸ë¥¼ HTML ë‹¨ë½ í˜•ì‹ìœ¼ë¡œ í¬ë§·íŒ…í•˜ëŠ” ë©”ì„œë“œ
    def format_answer_with_html_paragraphs(self, text: str, lang: str = 'ko') -> str:
        if not text:
            return ""
        
        text = self.remove_old_app_name(text)
        
        # ë¬¸ì¥ì„ ë§ˆì¹¨í‘œ, ëŠë‚Œí‘œ, ë¬¼ìŒí‘œë¡œ ë¶„ë¦¬
        sentences = re.split(r'(?<=[.!?])\s+', text)
        
        paragraphs = [] # ë‹¨ë½ë“¤ì„ ì €ì¥í•  ë¦¬ìŠ¤íŠ¸
        current_paragraph = [] # í˜„ì¬ ë‹¨ë½ì„ ì €ì¥í•  ë¦¬ìŠ¤íŠ¸
        
        # ë‹¨ë½ ë¶„ë¦¬ íŠ¸ë¦¬ê±° í‚¤ì›Œë“œë“¤ (ë” í¬ê´„ì ìœ¼ë¡œ í™•ì¥)
        # ì ‘ì†ì‚¬ë‚˜ ì¸ì‚¬ë§ë¡œ ì‹œì‘í•˜ëŠ” ë¬¸ì¥ì€ ë“¤ì—¬ì“°ê¸°ë¥¼ í†µí•´ ìƒˆ ë‹¨ë½ìœ¼ë¡œ ë¶„ë¦¬
        if lang == 'ko':
            paragraph_triggers = [
                # ì¸ì‚¬ ë° ê°ì‚¬
                'ì•ˆë…•í•˜ì„¸ìš”', 'ê°ì‚¬í•©ë‹ˆë‹¤', 'ê°ì‚¬ë“œë¦½ë‹ˆë‹¤', 'ë°”ì´ë¸” ì• í”Œì„',
                # ì ‘ì†ì–´ ë° ì—°ê²°ì–´
                'ë”°ë¼ì„œ', 'ê·¸ëŸ¬ë¯€ë¡œ', 'ë˜í•œ', 'ê·¸ë¦¬ê³ ', 'ë˜ëŠ”', 'í•˜ì§€ë§Œ', 'ê·¸ëŸ°ë°',
                'ì´ì™¸', 'ì´ì—', 'ì´ë¥¼', 'ì´ë¡œ', 'ì´ì™€', 'ì´ì—', 'ì´ìƒ', 'ì´í•˜',
                # ìƒí™© ì„¤ëª…
                'í˜„ì¬', 'ì§€ê¸ˆ', 'í˜„ì¬ë¡œ', 'í˜„ì¬ê¹Œì§€', 'í˜„ì¬ë¡œì„œëŠ”',
                'ë‚´ë¶€ì ìœ¼ë¡œ', 'ì™¸ë¶€ì ìœ¼ë¡œ', 'ê¸°ìˆ ì ìœ¼ë¡œ', 'ìš´ì˜ìƒ',
                # ì¡°ê±´ ë° ê°€ì •
                'ë§Œì•½', 'í˜¹ì‹œ', 'ë§Œì¼', 'ë§Œì•½ì—', 'ë§Œì•½ì˜',
                'í•´ë‹¹', 'ì´', 'ê·¸', 'ì €', 'ì´ëŸ°', 'ê·¸ëŸ°', 'ì €ëŸ°',
                # ìš”ì²­ ë° ì•ˆë‚´
                'ì„±ë„ë‹˜', 'ê³ ê°ë‹˜', 'ì´ìš©ì', 'ì‚¬ìš©ì',
                'ë²ˆê±°ë¡œìš°ì‹œ', 'ë¶ˆí¸í•˜ì‹œ', 'ì£„ì†¡í•˜ì§€ë§Œ', 'ì°¸ê³ ë¡œ',
                'ì–‘í•´ë¶€íƒë“œë¦½ë‹ˆë‹¤', 'ì–‘í•´í•´ì£¼ì‹œê¸°', 'ì´í•´í•´ì£¼ì‹œê¸°',
                # ì‹œê°„ ê´€ë ¨
                'í•­ìƒ', 'ëŠ˜', 'ì•ìœ¼ë¡œë„', 'ì§€ì†ì ìœ¼ë¡œ', 'ê³„ì†ì ìœ¼ë¡œ',
                'ì‹œê°„ì´', 'ì†Œìš”ë ', 'ê±¸ë¦´', 'í•„ìš”í•œ',
                # ê¸°ëŠ¥ ê´€ë ¨
                'ê¸°ëŠ¥', 'ê¸°ëŠ¥ì€', 'ê¸°ëŠ¥ì˜', 'ê¸°ëŠ¥ì´', 'ê¸°ëŠ¥ì„',
                'ìŠ¤í”¼ì»¤', 'ë²„íŠ¼', 'ë©”ë‰´', 'í™”ë©´', 'ì„¤ì •', 'ì˜µì…˜',
                # ì˜ê²¬ ë° ì „ë‹¬
                'ì˜ê²¬ì€', 'ì˜ê²¬ì„', 'ì „ë‹¬í• ', 'ì „ë‹¬í•˜ê² ìŠµë‹ˆë‹¤', 'ì „ë‹¬ë“œë¦¬ê² ìŠµë‹ˆë‹¤',
                'í† ì˜ê°€', 'ê²€í† ê°€', 'ê²€í† ë¥¼', 'ë…¼ì˜ê°€', 'ë…¼ì˜ë¥¼'
            ]
        else:  # ì˜ì–´
            paragraph_triggers = [
                # Greetings and appreciation
                'Hello', 'Hi', 'Dear', 'Thank', 'Thanks', 'Appreciate',
                'Grateful', 'Welcome', 'Greetings',
                
                # Conjunctions and transitions
                'Therefore', 'However', 'Additionally', 'Furthermore', 
                'Moreover', 'Nevertheless', 'Nonetheless', 'Meanwhile',
                'Subsequently', 'Consequently', 'Hence', 'Thus', 'Besides',
                'Although', 'Though', 'While', 'Whereas', 'Instead',
                
                # Situation descriptions
                'Currently', 'Presently', 'At the moment', 'Now',
                'At this time', 'As of now', 'Recently', 'Lately',
                'Technically', 'Internally', 'Externally', 'Generally',
                'Specifically', 'Basically', 'Essentially', 'Fundamentally',
                
                # Conditions and assumptions
                'If', 'When', 'Where', 'Whether', 'Unless', 'Provided',
                'Assuming', 'Suppose', 'In case', 'Should', 'Would',
                'Could', 'Might', 'May',
                
                # Requests and guidance
                'Please', 'Kindly', 'We recommend', 'We suggest',
                'You can', 'You may', 'You should', 'You might',
                'Try', 'Consider', 'Note that', 'Be aware',
                'Remember', 'Keep in mind', 'Important',
                
                # Apologies and understanding
                'Sorry', 'Apologize', 'Apologies', 'Unfortunately',
                'Regret', 'Understand', 'Realize', 'Acknowledge',
                'We know', 'We understand', 'We appreciate',
                
                # Time-related
                'Always', 'Usually', 'Often', 'Sometimes', 'Occasionally',
                'Frequently', 'Regularly', 'Continuously', 'Constantly',
                'Soon', 'Shortly', 'Eventually', 'Later', 'Previously',
                
                # Feature and function related
                'Feature', 'Function', 'Option', 'Setting', 'Button',
                'Menu', 'Screen', 'Tab', 'Page', 'Section', 'Tool',
                'Service', 'System', 'Application', 'Update', 'Version',
                
                # Problem-solving related
                'To fix', 'To solve', 'To resolve', 'To address',
                'Solution', 'Resolution', 'Workaround', 'Alternative',
                'Issue', 'Problem', 'Error', 'Bug', 'Trouble',
                
                # Feedback and communication
                'Your feedback', 'Your suggestion', 'Your opinion',
                'We will', 'We are', 'We have', 'Our team',
                'Will be', 'Has been', 'Have been', 'Working on',
                'Looking into', 'Reviewing', 'Considering', 'Planning',
                
                # Instructions and steps
                'First', 'Second', 'Third', 'Next', 'Then', 'After',
                'Before', 'Finally', 'Lastly', 'To begin', 'To start',
                'Step', 'Follow', 'Navigate', 'Click', 'Tap', 'Select',
                
                # Emphasis and clarification
                'Indeed', 'In fact', 'Actually', 'Certainly', 'Definitely',
                'Clearly', 'Obviously', 'Importantly', 'Notably',
                'Particularly', 'Especially', 'Specifically'
            ]
        
        for i, sentence in enumerate(sentences):
            sentence = sentence.strip()
            if not sentence:
                continue
            
            # ì²« ë²ˆì§¸ ë¬¸ì¥ (ì¸ì‚¬ë§)ì€ í•­ìƒ ë³„ë„ ë‹¨ë½
            if i == 0:
                if current_paragraph:
                    paragraphs.append(' '.join(current_paragraph))
                    current_paragraph = []
                paragraphs.append(sentence)
                continue
            
            should_break = False
            
            # íŠ¸ë¦¬ê±° í‚¤ì›Œë“œë¡œ ì‹œì‘í•˜ëŠ” ë¬¸ì¥ì€ ìƒˆ ë‹¨ë½
            for trigger in paragraph_triggers:
                if sentence.startswith(trigger):
                    should_break = True
                    break
            
            # í˜„ì¬ ë‹¨ë½ì— 2ê°œ ì´ìƒ ë¬¸ì¥ì´ ìˆìœ¼ë©´ ìƒˆ ë‹¨ë½
            if current_paragraph and len(current_paragraph) >= 2:
                should_break = True

            # ëë§ºìŒë§ì´ í¬í•¨ëœ ë¬¸ì¥ì€ ìƒˆ ë‹¨ë½
            if any(closing in sentence for closing in ['ê°ì‚¬í•©ë‹ˆë‹¤', 'ê°ì‚¬ë“œë¦½ë‹ˆë‹¤', 'í‰ì•ˆí•˜ì„¸ìš”', 'ì£¼ë‹˜ ì•ˆì—ì„œ']):
                should_break = True
            
            # ë¬¸ì¥ ê¸¸ì´ê°€ 50ì ì´ìƒì´ê³  í˜„ì¬ ë‹¨ë½ì´ ìˆìœ¼ë©´ ìƒˆ ë‹¨ë½
            if len(sentence) > 50 and current_paragraph:
                should_break = True
            
            # ìƒˆ ë‹¨ë½ ë¶„ë¦¬
            if should_break and current_paragraph:
                paragraphs.append(' '.join(current_paragraph))
                current_paragraph = [sentence]
            else:
                current_paragraph.append(sentence)
        
        # ë§ˆì§€ë§‰ ë‹¨ë½ ì¶”ê°€
        if current_paragraph:
            paragraphs.append(' '.join(current_paragraph))
        
        # Quill ì—ë””í„° í˜¸í™˜ì„ ìœ„í•œ HTML ë‹¨ë½ìœ¼ë¡œ ë³€í™˜
        # ê° ë‹¨ë½ì„ <p> íƒœê·¸ë¡œ ê°ì‹¸ê³ , ë‹¨ë½ ì‚¬ì´ì— <p><br></p> íƒœê·¸ë¡œ ë¹ˆ ì¤„ ì¶”ê°€
        html_paragraphs = []
        for i, paragraph in enumerate(paragraphs):
            html_paragraphs.append(f"<p>{paragraph}</p>")
            
            # ë‹¨ë½ ì‚¬ì´ì— ë¹ˆ ì¤„ ì¶”ê°€ (ë§ˆì§€ë§‰ ë‹¨ë½ ì œì™¸)
            if i < len(paragraphs) - 1:
                html_paragraphs.append("<p><br></p>")
        
        return ''.join(html_paragraphs)

    # â˜† ë‹µë³€ í…ìŠ¤íŠ¸ë¥¼ ì •ë¦¬í•˜ê³  í¬ë§·íŒ…í•˜ëŠ” ë©”ì„œë“œ (Quill ì—ë””í„°ìš©)
    def clean_answer_text(self, text: str) -> str:
        if not text:
            return ""
        
        # ì œì–´ ë¬¸ìë§Œ ì œê±°í•˜ê³  HTML íƒœê·¸ëŠ” ìœ ì§€
        text = re.sub(r'[\b\r\f\v]', '', text)
        text = re.sub(r'[\x00-\x08\x0B\x0C\x0E-\x1F\x7F]', '', text)

        # HTML íƒœê·¸ ì œê±°í•˜ì§€ ì•ŠìŒ (Quill ì—ë””í„°ìš©)
        text = re.sub(r'\*\*([^*]+)\*\*', r'\1', text)
        text = re.sub(r'\*([^*]+)\*', r'\1', text)
        
        # HTML íƒœê·¸ ë‚´ë¶€ì˜ ê³µë°±ë§Œ ì •ë¦¬ (íƒœê·¸ ìì²´ëŠ” ìœ ì§€)
        text = re.sub(r'>\s+<', '><', text)  # íƒœê·¸ ì‚¬ì´ ê³µë°± ì œê±°
        text = re.sub(r'<p>\s+', '<p>', text)  # <p> íƒœê·¸ ë‚´ë¶€ ì• ê³µë°± ì œê±°
        text = re.sub(r'\s+</p>', '</p>', text)  # </p> íƒœê·¸ ì• ê³µë°± ì œê±°
        
        text = self.remove_old_app_name(text)
        text = self.format_answer_with_html_paragraphs(text)
        
        return text

    # â˜† í…ìŠ¤íŠ¸ ìœ íš¨ì„± ê²€ì¦ ë©”ì„œë“œ
    def is_valid_text(self, text: str, lang: str = 'ko') -> bool:
        if not text or len(text.strip()) < 3:
            return False
        
        if lang == 'ko':
            return self.is_valid_korean_text(text)
        else:  # ì˜ì–´
            return self.is_valid_english_text(text)

    # â˜† í•œêµ­ì–´ í…ìŠ¤íŠ¸ì˜ ìœ íš¨ì„±ì„ ê²€ì¦í•˜ëŠ” ë©”ì„œë“œ
    def is_valid_korean_text(self, text: str) -> bool:
        if not text or len(text.strip()) < 3:
            logging.info(f"í•œêµ­ì–´ ê²€ì¦ ì‹¤íŒ¨: í…ìŠ¤íŠ¸ê°€ ë„ˆë¬´ ì§§ìŒ (ê¸¸ì´: {len(text.strip()) if text else 0})")
            return False
        
        korean_chars = len(re.findall(r'[ê°€-í£]', text))
        total_chars = len(re.sub(r'\s', '', text))
        
        if total_chars == 0:
            logging.info("í•œêµ­ì–´ ê²€ì¦ ì‹¤íŒ¨: ì´ ê¸€ì ìˆ˜ê°€ 0")
            return False
            
        korean_ratio = korean_chars / total_chars
        logging.info(f"í•œêµ­ì–´ ë¹„ìœ¨: {korean_ratio:.3f} (í•œêµ­ì–´: {korean_chars}, ì „ì²´: {total_chars})")
        
        # í•œêµ­ì–´ ë¹„ìœ¨ ê¸°ì¤€ì„ ì™„í™” (0.2 â†’ 0.1)
        if korean_ratio < 0.1:
            logging.info(f"í•œêµ­ì–´ ê²€ì¦ ì‹¤íŒ¨: í•œêµ­ì–´ ë¹„ìœ¨ ë¶€ì¡± ({korean_ratio:.3f} < 0.1)")
            return False
        
        # ë¬´ì˜ë¯¸í•œ íŒ¨í„´ ê°ì§€ (GPT í• ë£¨ì‹œë„¤ì´ì…˜ ë°©ì§€)
        meaningless_patterns = [
            r'^[a-z\s\.,;:\(\)\[\]\-_&\/\'"]+$', # ì˜ì–´
            r'^[A-Z\s\.,;:\(\)\[\]\-_&\/\'"]+$', # ì˜ì–´ ëŒ€ë¬¸ì
            r'^[\s\.,;:\(\)\[\]\-_&\/\'"]+$',    # ê³µë°±
            r'^[0-9\s\.,;:\(\)\[\]\-_&\/\'"]+$', # ìˆ«ì
            r'.*[Ğ°-Ñ].*',                        # ëŸ¬ì‹œì•„ì–´
            r'.*[Î±-Ï‰].*',                        # ê·¸ë¦¬ìŠ¤ì–´
        ]
        
        for pattern in meaningless_patterns:
            if re.match(pattern, text, re.IGNORECASE):
                logging.info(f"í•œêµ­ì–´ ê²€ì¦ ì‹¤íŒ¨: ë¬´ì˜ë¯¸í•œ íŒ¨í„´ ê°ì§€")
                return False
        
        # ë°˜ë³µ ë¬¸ì ê°ì§€: ê°™ì€ ë¬¸ìê°€ 5ë²ˆ ì´ìƒ ì—°ì†ìœ¼ë¡œ ë‚˜íƒ€ë‚˜ë©´ ë¹„ì •ìƒ í…ìŠ¤íŠ¸ë¡œ ê°„ì£¼
        if re.search(r'(.)\1{5,}', text):
            logging.info("í•œêµ­ì–´ ê²€ì¦ ì‹¤íŒ¨: ë°˜ë³µ ë¬¸ì ê°ì§€")
            return False
        
        # ì˜ì–´ ë¹„ìœ¨ ê²€ì‚¬ë¥¼ ì™„í™” (0.5 â†’ 0.7)
        random_pattern = r'[a-zA-Z]{8,}'
        if re.search(random_pattern, text) and korean_ratio < 0.3:
            logging.info(f"í•œêµ­ì–´ ê²€ì¦ ì‹¤íŒ¨: ê¸´ ì˜ì–´ ë‹¨ì–´ì™€ ë‚®ì€ í•œêµ­ì–´ ë¹„ìœ¨")
            return False
        
        logging.info("í•œêµ­ì–´ ê²€ì¦ ì„±ê³µ")
        return True

    # â˜† ì˜ì–´ í…ìŠ¤íŠ¸ì˜ ìœ íš¨ì„±ì„ ê²€ì¦í•˜ëŠ” ë©”ì„œë“œ
    def is_valid_english_text(self, text: str) -> bool:
        if not text or len(text.strip()) < 3:
            return False
        
        english_chars = len(re.findall(r'[a-zA-Z]', text))
        total_chars = len(re.sub(r'\s', '', text))
        
        if total_chars == 0:
            return False
            
        english_ratio = english_chars / total_chars
        
        if english_ratio < 0.7:  # ì˜ì–´ ë¹„ìœ¨ì´ 70% ë¯¸ë§Œì´ë©´ ë¬´íš¨
            return False
        
        # ë°˜ë³µ ë¬¸ì ê°ì§€
        if re.search(r'(.)\1{5,}', text):
            return False
        
        return True

    # â˜† ìƒì„±ëœ í…ìŠ¤íŠ¸ë¥¼ ì •ë¦¬í•˜ê³  ê²€ì¦í•˜ëŠ” ë©”ì„œë“œ
    def clean_generated_text(self, text: str) -> str:
        if not text:
            return ""
        # ì œì–´ ë¬¸ì ì œê±°
        text = re.sub(r'[\x00-\x08\x0B\x0C\x0E-\x1F\x7F]', '', text)
        text = re.sub(r'[\b\r\f\v]', '', text)

        # ì˜ì–´ ì•½ì–´ ì œê±°
        text = re.sub(r'\b[a-z]{1,2}\b(?:\s+[a-z]{1,2}\b)*', '', text, flags=re.IGNORECASE)
        text = re.sub(r'[Ğ°-Ñ]+', '', text)
        text = re.sub(r'[Î±-Ï‰]+', '', text)

        # í•œê¸€ ë¬¸ì ì œê±°
        text = re.sub(r'[^\w\sê°€-í£.,!?()"\'-]{3,}', '', text)
        text = re.sub(r'[.,;:!?]{3,}', '.', text)

        # ê³µë°± ì •ë¦¬
        text = re.sub(r'\s+', ' ', text)
        text = text.strip()
        
        return text

    # â˜† í†µì¼ëœ GPT í”„ë¡¬í”„íŠ¸ ìƒì„± ë©”ì„œë“œ (ëª¨ë“ˆí™”)
    def get_gpt_prompts(self, query: str, context: str, lang: str = 'ko') -> tuple:
        """ì–¸ì–´ë³„ GPT í”„ë¡¬í”„íŠ¸ ìƒì„±"""
        if lang == 'en': # ì˜ì–´
            system_prompt = """You are a GOODTV Bible App customer service representative.

Guidelines:
1. Follow the style and content of the provided reference answers faithfully
2. Find and apply solutions from similar situations in the reference answers
3. Adapt to the customer's specific situation while maintaining the tone and style of the reference answers

âš ï¸ Absolute Prohibitions:
- Do not guide non-existent features or menus
- Do not create specific settings methods or button locations
- If a feature is not in the reference answers, say "Sorry, this feature is currently not available"
- If uncertain, respond with "We will review this internally"

4. For feature requests or improvement suggestions, use:
   - "Thank you for your valuable feedback"
   - "We will discuss/review this internally"
   - "We will forward this as an improvement"

5. Address customers as 'Dear user' or similar polite forms
6. Use 'GOODTV Bible App' or 'Bible App' as the app name

ğŸš« Do NOT generate greetings or closings:
- Do not use "Hello", "Thank you", "Best regards", etc.
- Do not use "God bless", "In Christ", etc.
- Only write the main content

7. Do not use HTML tags, write in natural sentences"""

            user_prompt = f"""Customer inquiry: {query}

Reference answers (main content only, greetings and closings removed):
{context}

Based on the reference answers' solution methods and tone, write a specific answer to the customer's problem.
Important: Do not include greetings or closings. Only write the main content."""

        else:  # í•œêµ­ì–´
            system_prompt = """ë‹¹ì‹ ì€ GOODTV ë°”ì´ë¸” ì• í”Œ ê³ ê°ì„¼í„° ìƒë‹´ì›ì…ë‹ˆë‹¤.

ğŸ† ë°”ì´ë¸” ì• í”Œ í•µì‹¬ ê¸°ëŠ¥ (ì ˆëŒ€ ì¤€ìˆ˜):
- ë°”ì´ë¸” ì• í”Œì€ **ìì²´ì ìœ¼ë¡œ ì—¬ëŸ¬ ë²ˆì—­ë³¸ì„ ë™ì‹œì— ë³¼ ìˆ˜ ìˆëŠ” ê¸°ëŠ¥ì„ ì œê³µ**í•©ë‹ˆë‹¤
- NIV, KJV, ê°œì—­ê°œì •, ê°œì—­í•œê¸€ ë“± ë‹¤ì–‘í•œ ë²ˆì—­ë³¸ì„ **í•œ í™”ë©´ì—ì„œ ë¹„êµ ê°€ëŠ¥**í•©ë‹ˆë‹¤
- ë‹¤ë¥¸ ì•± ë‹¤ìš´ë¡œë“œë‚˜ ì™¸ë¶€ ì„œë¹„ìŠ¤ ì´ìš©ì€ **ì ˆëŒ€ ì•ˆë‚´í•˜ì§€ ë§ˆì„¸ìš”**
- ë°”ì´ë¸” ì• í”Œ ë‚´ë¶€ ê¸°ëŠ¥ë§Œìœ¼ë¡œ ëª¨ë“  ë²ˆì—­ë³¸ ë¹„êµê°€ ê°€ëŠ¥í•©ë‹ˆë‹¤

ğŸš¨ ì ˆëŒ€ ê¸ˆì§€ì‚¬í•­ (í• ë£¨ì‹œë„¤ì´ì…˜ ë°©ì§€):
- âŒ "Parallel Bible" ì•±ì´ë‚˜ ë‹¤ë¥¸ ì•± ë‹¤ìš´ë¡œë“œ ì¶”ì²œ ê¸ˆì§€
- âŒ ë°”ì´ë¸” ì• í”Œì— ì—†ëŠ” ê¸°ëŠ¥ì´ë‚˜ ë©”ë‰´ ì–¸ê¸‰ ê¸ˆì§€  
- âŒ í™•ì‹¤í•˜ì§€ ì•Šì€ ì •ë³´ë‚˜ ì¶”ì¸¡ì„± ë‹µë³€ ê¸ˆì§€
- âŒ ë‹µë³€ ì¤‘ê°„ì— ë‹¤ë¥¸ ë²ˆì—­ë³¸ì´ë‚˜ ì–¸ì–´ë¡œ ë‚´ìš© ë³€ê²½ ê¸ˆì§€
- âŒ ì°¸ê³ ë‹µë³€ì— ì—†ëŠ” ìƒˆë¡œìš´ í•´ê²°ì±… ì°½ì‘ ê¸ˆì§€

ğŸ¯ í•µì‹¬ ì›ì¹™ (ì°¸ê³ ë‹µë³€ ì ˆëŒ€ ì¤€ìˆ˜):
1. **ì°¸ê³ ë‹µë³€ 100% í™œìš©**: ì œê³µëœ ì°¸ê³ ë‹µë³€ì˜ í•´ê²° ë°©ë²•ì„ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•˜ì„¸ìš”
2. **ì§ˆë¬¸ ë‚´ìš© ê³ ì •**: ì§ˆë¬¸ì—ì„œ ì–¸ê¸‰í•œ ë²ˆì—­ë³¸/ê¸°ëŠ¥ì„ ì ˆëŒ€ ë°”ê¾¸ì§€ ë§ˆì„¸ìš”
3. **ì¼ê´€ì„± ì² ì € ìœ ì§€**: ë‹µë³€ ì²˜ìŒë¶€í„° ëê¹Œì§€ ë™ì¼í•œ ë‚´ìš©ê³¼ ë²ˆì—­ë³¸ ìœ ì§€
4. **ë„ë©”ì¸ ì§€ì‹ ì¤€ìˆ˜**: ë°”ì´ë¸” ì• í”Œì˜ ì‹¤ì œ ê¸°ëŠ¥ ë²”ìœ„ ë‚´ì—ì„œë§Œ ë‹µë³€

ğŸ“‹ ì°¸ê³ ë‹µë³€ í™œìš© ì§€ì¹¨:

âœ… ì°¸ê³ ë‹µë³€ ë¶„ì„ ìš°ì„  ìˆœìœ„:
1. ê³ ê° ì§ˆë¬¸ê³¼ ì˜ë¯¸ì ìœ¼ë¡œ ê°€ì¥ ìœ ì‚¬í•œ ì°¸ê³ ë‹µë³€ ì‹ë³„
2. í•´ë‹¹ ì°¸ê³ ë‹µë³€ì˜ í•µì‹¬ í•´ê²° ë‹¨ê³„ì™€ ë°©ë²• ì¶”ì¶œ  
3. ì°¸ê³ ë‹µë³€ì— ëª…ì‹œëœ êµ¬ì²´ì  ê¸°ëŠ¥ëª…, ë©”ë‰´ëª…, ë²„íŠ¼ëª… íŒŒì•…
4. ì°¸ê³ ë‹µë³€ì˜ í†¤ì•¤ë§¤ë„ˆì™€ ì„¤ëª… ìŠ¤íƒ€ì¼ í•™ìŠµ

ğŸ” ì°¸ê³ ë‹µë³€ ê¸°ë°˜ ë‹µë³€ ì‘ì„±:
- **í•µì‹¬ í•´ê²°ì±… ìœ ì§€**: ì°¸ê³ ë‹µë³€ì˜ ì£¼ìš” í•´ê²° ë°©ë²•ì„ ê·¸ëŒ€ë¡œ í™œìš©
- **êµ¬ì²´ì  ì •ë³´ ë³´ì¡´**: ì°¸ê³ ë‹µë³€ì— ë‚˜ì˜¨ ì„¤ì • ìœ„ì¹˜, ë²„íŠ¼ëª…, ë©”ë‰´ ê²½ë¡œë¥¼ ì •í™•íˆ ë°˜ì˜
- **ë‹¨ê³„ë³„ ìˆœì„œ ì¤€ìˆ˜**: ì°¸ê³ ë‹µë³€ì˜ í•´ê²° ë‹¨ê³„ ìˆœì„œë¥¼ ìœ ì§€í•˜ê±°ë‚˜ ê°œì„ 
- **ì „ë¬¸ ìš©ì–´ ì¼ì¹˜**: ì°¸ê³ ë‹µë³€ì— ì‚¬ìš©ëœ ì•± ì „ë¬¸ ìš©ì–´ì™€ í‘œí˜„ ë°©ì‹ ë”°ë¥´ê¸°

âš ï¸ ì°¸ê³ ë‹µë³€ ì¶©ì‹¤ì„± ê²€ì¦:
- ì°¸ê³ ë‹µë³€ì— ì—†ëŠ” ìƒˆë¡œìš´ ê¸°ëŠ¥ì´ë‚˜ ë°©ë²• ì¶”ê°€ ê¸ˆì§€
- ì°¸ê³ ë‹µë³€ê³¼ ìƒì¶©ë˜ëŠ” í•´ê²°ì±… ì œì‹œ ê¸ˆì§€
- ì°¸ê³ ë‹µë³€ì˜ í•µì‹¬ ë‚´ìš©ì„ ëˆ„ë½í•˜ê±°ë‚˜ ë³€í˜•í•˜ì§€ ë§ ê²ƒ
- ë¶ˆí™•ì‹¤í•œ ì •ë³´ë³´ë‹¤ëŠ” ì°¸ê³ ë‹µë³€ì—ì„œ í™•ì¸ëœ ë‚´ìš©ë§Œ í™œìš©

ğŸš« ì ˆëŒ€ ê¸ˆì§€ì‚¬í•­:
- ì¸ì‚¬ë§("ì•ˆë…•í•˜ì„¸ìš”", "ê°ì‚¬í•©ë‹ˆë‹¤" ë“±) ì‚¬ìš© ê¸ˆì§€
- ëë§ºìŒë§("í‰ì•ˆí•˜ì„¸ìš”", "ì£¼ë‹˜ ì•ˆì—ì„œ" ë“±) ì‚¬ìš© ê¸ˆì§€  
- ë³¸ë¬¸ ë‚´ìš©ë§Œ ì‘ì„±í•˜ê³  ê²©ì‹ì  í‘œí˜„ ìƒëµ

ğŸš¨ ë¹ˆ ì•½ì† ê¸ˆì§€ (ë§¤ìš° ì¤‘ìš”):
- "ì•ˆë‚´í•´ë“œë¦¬ê² ìŠµë‹ˆë‹¤", "ë„ì›€ë“œë¦¬ê² ìŠµë‹ˆë‹¤", "ì„¤ëª…ë“œë¦¬ê² ìŠµë‹ˆë‹¤" ë“±ì˜ ì•½ì† í‘œí˜„ ì‚¬ìš© ì‹œ 
  ë°˜ë“œì‹œ êµ¬ì²´ì ì¸ ì‹¤í–‰ ë‚´ìš©ì´ ë°”ë¡œ ë’¤ë”°ë¼ì•¼ í•©ë‹ˆë‹¤
- ì•½ì†ë§Œ í•˜ê³  ì‹¤ì œ ì•ˆë‚´/ë„ì›€/ì„¤ëª… ë‚´ìš©ì´ ì—†ìœ¼ë©´ ì ˆëŒ€ ì•ˆë©ë‹ˆë‹¤
- ì˜ˆì‹œ: âŒ "ë°©ë²•ì„ ì•ˆë‚´í•´ë“œë¦¬ê² ìŠµë‹ˆë‹¤." (ë) 
         âœ… "ë°©ë²•ì„ ì•ˆë‚´í•´ë“œë¦¬ê² ìŠµë‹ˆë‹¤. 1. í™”ë©´ ìƒë‹¨ì˜ ì„¤ì • ë©”ë‰´ë¥¼ í„°ì¹˜í•˜ì„¸ìš”..."

ğŸ’¡ ì°¸ê³ ë‹µë³€ ê¸°ë°˜ êµ¬ì²´ì  ì‘ì„±ë²•:
- **ì°¸ê³ ë‹µë³€ ë‹¨ê³„ ì¬í˜„**: ì°¸ê³ ë‹µë³€ì˜ í•´ê²° ë‹¨ê³„ë¥¼ ìˆœì„œëŒ€ë¡œ ì„¤ëª…
- **ì°¸ê³ ë‹µë³€ ìš©ì–´ ì‚¬ìš©**: ì°¸ê³ ë‹µë³€ì— ë‚˜ì˜¨ ì •í™•í•œ ê¸°ëŠ¥ëª…ê³¼ ìœ„ì¹˜ í‘œí˜„ í™œìš©
- **ì°¸ê³ ë‹µë³€ ìŠ¤íƒ€ì¼ ë°˜ì˜**: ì°¸ê³ ë‹µë³€ì˜ ì„¤ëª… ë°©ì‹ê³¼ êµ¬ì²´ì„± ìˆ˜ì¤€ ìœ ì§€
- **ê²€ì¦ëœ ì •ë³´ ìš°ì„ **: ì°¸ê³ ë‹µë³€ì—ì„œ ê²€ì¦ëœ ì •ë³´ë¥¼ ì°½ì˜ì  ì¶”ì¸¡ë³´ë‹¤ ìš°ì„ 

ğŸ’¡ ì°¸ê³ ë‹µë³€ ë¶€ì¡±ì‹œ ëŒ€ì‘:
- ì°¸ê³ ë‹µë³€ì´ ë¶€ì¡±í•´ë„ ê·¸ ë²”ìœ„ ë‚´ì—ì„œë§Œ í™•ì¥í•˜ì—¬ ë‹µë³€
- ì°¸ê³ ë‹µë³€ì˜ í•µì‹¬ ì›ë¦¬ë¥¼ ê³ ê° ìƒí™©ì— ë§ê²Œ ì ìš©
- ë°”ì´ë¸” ì• í”Œì˜ ì‹¤ì œ ì„œë¹„ìŠ¤ ë²”ìœ„ ë‚´ì—ì„œë§Œ í˜„ì‹¤ì ì¸ ë‹µë³€ ì œê³µ"""

            user_prompt = f"""ê³ ê° ë¬¸ì˜: {query}

ì°¸ê³  ë‹µë³€ë“¤ (í•µì‹¬ ì •ë³´):
{context}

ğŸ¯ ì°¸ê³ ë‹µë³€ ìš°ì„  í™œìš© ì§€ì‹œì‚¬í•­:
ìœ„ ì°¸ê³  ë‹µë³€ë“¤ì„ ë©´ë°€íˆ ë¶„ì„í•˜ê³  ë‹¤ìŒ ì›ì¹™ì— ë”°ë¼ ë‹µë³€í•˜ì„¸ìš”:

1. **ì°¸ê³ ë‹µë³€ ìµœìš°ì„  ë¶„ì„**: 
   - ê³ ê° ì§ˆë¬¸ê³¼ ì˜ë¯¸ì ìœ¼ë¡œ ê°€ì¥ ì¼ì¹˜í•˜ëŠ” ì°¸ê³ ë‹µë³€ì„ ì‹ë³„
   - í•´ë‹¹ ì°¸ê³ ë‹µë³€ì˜ í•´ê²° ë°©ë²•, ë‹¨ê³„, ê¸°ëŠ¥ëª…ì„ ì •í™•íˆ íŒŒì•…
   - ì°¸ê³ ë‹µë³€ì— ë‚˜ì˜¨ êµ¬ì²´ì  ìš©ì–´ì™€ ì„¤ëª… ë°©ì‹ì„ í•™ìŠµ

2. **ì°¸ê³ ë‹µë³€ ì¶©ì‹¤í•œ í™œìš©**:
   - ì°¸ê³ ë‹µë³€ì˜ í•µì‹¬ í•´ê²°ì±…ì„ ê·¸ëŒ€ë¡œ í™œìš©í•˜ì—¬ ë‹µë³€ ì‘ì„±
   - ì°¸ê³ ë‹µë³€ì— ëª…ì‹œëœ ì„¤ì • ìœ„ì¹˜, ë²„íŠ¼ëª…, ë©”ë‰´ ê²½ë¡œë¥¼ ì •í™•íˆ ë°˜ì˜
   - ì°¸ê³ ë‹µë³€ì˜ ë‹¨ê³„ë³„ ìˆœì„œì™€ ì„¤ëª… ìŠ¤íƒ€ì¼ì„ ë”°ë¼ ë‹µë³€ êµ¬ì„±
   - ì°¸ê³ ë‹µë³€ì— ì‚¬ìš©ëœ ì „ë¬¸ ìš©ì–´ì™€ í‘œí˜„ ë°©ì‹ì„ ë™ì¼í•˜ê²Œ ì‚¬ìš©

3. **ì°¸ê³ ë‹µë³€ ê¸°ë°˜ í™•ì¥**:
   - ì°¸ê³ ë‹µë³€ì˜ ë²”ìœ„ ë‚´ì—ì„œë§Œ ê³ ê° ìƒí™©ì— ë§ê²Œ ë‚´ìš© ì¡°ì •
   - ì°¸ê³ ë‹µë³€ì— ì—†ëŠ” ìƒˆë¡œìš´ ê¸°ëŠ¥ì´ë‚˜ ë°©ë²• ì¶”ê°€ ì ˆëŒ€ ê¸ˆì§€
   - ì°¸ê³ ë‹µë³€ê³¼ ìƒì¶©ë˜ëŠ” í•´ê²°ì±… ì œì‹œ ê¸ˆì§€

ğŸš¨ í•„ìˆ˜ ìš”êµ¬ì‚¬í•­:
1. **ì°¸ê³ ë‹µë³€ ìš°ì„ **: ì°½ì˜ì  í•´ê²°ì±…ë³´ë‹¤ ì°¸ê³ ë‹µë³€ì˜ ê²€ì¦ëœ ë°©ë²• ìš°ì„  í™œìš©
2. **êµ¬ì²´ì  ì‹¤í–‰**: "ì•ˆë‚´í•´ë“œë¦¬ê² ìŠµë‹ˆë‹¤" ë“±ì˜ ì•½ì† í›„ ë°˜ë“œì‹œ êµ¬ì²´ì  ë‚´ìš© ì œì‹œ
3. **ì •í™•í•œ ìš©ì–´**: ì°¸ê³ ë‹µë³€ì˜ ì •í™•í•œ ê¸°ëŠ¥ëª…, ë©”ë‰´ëª…, ë²„íŠ¼ëª… ì‚¬ìš©
4. **ë‹¨ê³„ë³„ ì„¤ëª…**: ì°¸ê³ ë‹µë³€ì˜ í•´ê²° ë‹¨ê³„ë¥¼ ìˆœì„œëŒ€ë¡œ ëª…í™•íˆ ì„¤ëª…
5. **ë³¸ë¬¸ë§Œ ì‘ì„±**: ì¸ì‚¬ë§ì´ë‚˜ ëë§ºìŒë§ ì—†ì´ í•µì‹¬ ë‚´ìš©ë§Œ ì‘ì„±

ğŸ”’ í• ë£¨ì‹œë„¤ì´ì…˜ ì—„ê²© ê¸ˆì§€:
- ì§ˆë¬¸ì—ì„œ ì–¸ê¸‰í•œ ë²ˆì—­ë³¸ì´ë‚˜ ê¸°ëŠ¥ì„ ì ˆëŒ€ ë°”ê¾¸ì§€ ë§ˆì„¸ìš”
- ë‹µë³€ ì¤‘ê°„ì— ë‹¤ë¥¸ ë‚´ìš©ìœ¼ë¡œ ë³€ê²½í•˜ëŠ” ê²ƒì„ ì ˆëŒ€ ê¸ˆì§€í•©ë‹ˆë‹¤
- ë°”ì´ë¸” ì• í”Œ ì™¸ë¶€ ì•±ì´ë‚˜ ì„œë¹„ìŠ¤ ì¶”ì²œì„ ì ˆëŒ€ í•˜ì§€ ë§ˆì„¸ìš”
- ì°¸ê³ ë‹µë³€ì— ì—†ëŠ” ê¸°ëŠ¥ì´ë‚˜ ë°©ë²•ì„ ì°½ì‘í•˜ì§€ ë§ˆì„¸ìš”
- í™•ì‹¤í•˜ì§€ ì•Šì€ ì •ë³´ëŠ” ì ˆëŒ€ ì–¸ê¸‰í•˜ì§€ ë§ˆì„¸ìš”

âœ… ì¼ê´€ì„± ê²€ì¦:
- ë‹µë³€ ì „ì²´ì—ì„œ ë™ì¼í•œ ë²ˆì—­ë³¸/ê¸°ëŠ¥ ìœ ì§€
- ì§ˆë¬¸ì˜ í•µì‹¬ ìš”êµ¬ì‚¬í•­ì—ì„œ ì ˆëŒ€ ë²—ì–´ë‚˜ì§€ ì•Šê¸°
- ë°”ì´ë¸” ì• í”Œ ìì²´ ê¸°ëŠ¥ë§Œìœ¼ë¡œ í•´ê²°ì±… ì œì‹œ

âŒ ì ˆëŒ€ ê¸ˆì§€: ì°¸ê³ ë‹µë³€ ë¬´ì‹œ, ì™¸ë¶€ ì•± ì¶”ì²œ, ë‚´ìš© ë³€ê²½
âœ… ë°˜ë“œì‹œ ì¤€ìˆ˜: ì°¸ê³ ë‹µë³€ ë°©ë²•ì„ ì§ˆë¬¸ì— ì •í™•íˆ ì ìš©, ì¼ê´€ì„± ìœ ì§€

ì§€ê¸ˆ ì¦‰ì‹œ ì°¸ê³ ë‹µë³€ì— 100% ì¶©ì‹¤í•˜ë©´ì„œ ì§ˆë¬¸ ë‚´ìš©ì„ ì ˆëŒ€ ë°”ê¾¸ì§€ ì•Šê³  ë‹µë³€í•˜ì„¸ìš”."""

        return system_prompt, user_prompt

    # â˜† í–¥ìƒëœ GPT ìƒì„± - ì¼ê´€ì„±ê³¼ í’ˆì§ˆ ë³´ì¥
    def generate_with_enhanced_gpt(self, query: str, similar_answers: list, context_analysis: dict, lang: str = 'ko') -> str:
        try:
            with memory_cleanup():
                approach = context_analysis['recommended_approach']
                context = self.create_enhanced_context(similar_answers, target_lang=lang)
                
                if not context:
                    logging.warning("ìœ íš¨í•œ ì»¨í…ìŠ¤íŠ¸ê°€ ì—†ì–´ GPT ìƒì„± ì¤‘ë‹¨")
                    return ""
                
                # í†µì¼ëœ í”„ë¡¬í”„íŠ¸ ìƒì„±
                system_prompt, user_prompt = self.get_gpt_prompts(query, context, lang)
                
                # ğŸ”¥ ì¼ê´€ì„±ì„ ìœ„í•œ ë³´ìˆ˜ì  temperature ì„¤ì •
                if approach == 'gpt_with_strong_context':
                    # ì¼ê´€ì„± ìš°ì„ ìœ¼ë¡œ ë‚®ì€ temperature
                    temperature = 0.3 if context_analysis.get('context_relevance') == 'high' else 0.4
                    max_tokens = 700
                elif approach == 'gpt_with_weak_context':
                    # ì•½ê°„ ë” ì°½ì˜ì ì´ì§€ë§Œ ì—¬ì „íˆ ë³´ìˆ˜ì 
                    temperature = 0.4
                    max_tokens = 650
                else: # fallbackì´ë‚˜ ê¸°íƒ€
                    return ""
                
                # ğŸ”¥ ë‹µë³€ í’ˆì§ˆ ë³´ì¥ì„ ìœ„í•œ 3íšŒ ì¬ì‹œë„ ë©”ì»¤ë‹ˆì¦˜
                max_attempts = 3
                for attempt in range(max_attempts):
                    # GPT API í˜¸ì¶œ
                    response = self.openai_client.chat.completions.create(
                        model=GPT_MODEL,
                        messages=[
                            {"role": "system", "content": system_prompt},
                            {"role": "user", "content": user_prompt}
                        ],
                        max_tokens=max_tokens,
                        temperature=temperature,
                        top_p=0.9,  # ë” ë³´ìˆ˜ì ìœ¼ë¡œ ì„¤ì •
                        frequency_penalty=0.1,
                        presence_penalty=0.1
                    )
                    
                    generated = response.choices[0].message.content.strip()
                    del response
                    
                    # ìƒì„±ëœ í…ìŠ¤íŠ¸ ì •ë¦¬
                    generated = self.clean_generated_text(generated)
                    
                    # ğŸ”¥ ë‹µë³€ ì™„ì„±ë„ ê²€ì¦ (ìƒˆë¡œ ì¶”ê°€)
                    completeness_score = self.check_answer_completeness(generated, query, lang)
                    logging.info(f"ì‹œë„ #{attempt+1} ë‹µë³€ ì™„ì„±ë„: {completeness_score:.2f}")
                    
                    # ğŸ”¥ í• ë£¨ì‹œë„¤ì´ì…˜ ë° ì¼ê´€ì„± ê²€ì¦ (ìƒˆë¡œ ì¶”ê°€)
                    hallucination_check = self.detect_hallucination_and_inconsistency(generated, query, lang)
                    hallucination_score = hallucination_check['overall_score']
                    detected_issues = hallucination_check['detected_issues']
                    
                    logging.info(f"ì‹œë„ #{attempt+1} í• ë£¨ì‹œë„¤ì´ì…˜ ê²€ì¦: {hallucination_score:.2f}")
                    if detected_issues:
                        logging.warning(f"ê°ì§€ëœ ë¬¸ì œë“¤: {detected_issues}")
                    
                    # ì™„ì„±ë„ì™€ í• ë£¨ì‹œë„¤ì´ì…˜ ì ìˆ˜ ëª¨ë‘ ê³ ë ¤
                    combined_score = completeness_score * 0.6 + hallucination_score * 0.4
                    
                    # ğŸš¨ í• ë£¨ì‹œë„¤ì´ì…˜ì´ ê°ì§€ë˜ë©´ ì¦‰ì‹œ ì¬ì‹œë„
                    if hallucination_score < 0.3:
                        logging.error(f"ì‹œë„ #{attempt+1}: ì‹¬ê°í•œ í• ë£¨ì‹œë„¤ì´ì…˜ ê°ì§€ (ì ìˆ˜: {hallucination_score:.2f})")
                        logging.error(f"ê°ì§€ëœ ë¬¸ì œ: {detected_issues}")
                        continue  # ì¦‰ì‹œ ë‹¤ìŒ ì‹œë„ë¡œ
                    
                    # ì™„ì„±ë„ê°€ ì¶©ë¶„í•œì§€ ê²€ì‚¬
                    if combined_score >= 0.7 and completeness_score >= 0.6:
                        # ê´€ë ¨ì„± ê²€ì¦
                        if self.validate_answer_relevance_ai(generated, query, context_analysis.get('question_analysis', {})):
                            logging.info(f"GPT ìƒì„± ì„±ê³µ (ì‹œë„ #{attempt+1}, {approach}): ì™„ì„±ë„={completeness_score:.2f}, í• ë£¨ì‹œë„¤ì´ì…˜={hallucination_score:.2f}")
                            return generated[:650]
                        else:
                            logging.warning(f"ì‹œë„ #{attempt+1}: ê´€ë ¨ì„± ê²€ì¦ ì‹¤íŒ¨")
                    else:
                        logging.warning(f"ì‹œë„ #{attempt+1}: í’ˆì§ˆ ë¶€ì¡± - ì™„ì„±ë„={completeness_score:.2f}, í• ë£¨ì‹œë„¤ì´ì…˜={hallucination_score:.2f}, ì¢…í•©={combined_score:.2f}")
                    
                    # ë§ˆì§€ë§‰ ì‹œë„ê°€ ì•„ë‹ˆë©´ temperature ì¡°ì •
                    if attempt < max_attempts - 1:
                        temperature = min(temperature + 0.1, 0.6)  # ì ì§„ì ìœ¼ë¡œ ì¦ê°€
                
                logging.warning("ëª¨ë“  GPT ìƒì„± ì‹œë„ ì‹¤íŒ¨")
                return ""
                
        except Exception as e:
            logging.error(f"í–¥ìƒëœ GPT ìƒì„± ì‹¤íŒ¨: {e}")
            return ""

    # â˜† AI ê¸°ë°˜ ë‹µë³€ ê´€ë ¨ì„± ê²€ì¦ ë©”ì„œë“œ (ì—„ê²©í•œ ê²€ì¦ ë²„ì „)
    def validate_answer_relevance_ai(self, answer: str, query: str, question_analysis: dict) -> bool:
        """AIë¥¼ ì´ìš©í•´ ìƒì„±ëœ ë‹µë³€ì´ ì§ˆë¬¸ê³¼ ê´€ë ¨ì„±ì´ ìˆëŠ”ì§€ ì—„ê²©í•˜ê²Œ ê²€ì¦"""
        
        try:
            with memory_cleanup():
                system_prompt = """ë‹¹ì‹ ì€ ë‹µë³€ í’ˆì§ˆ ê²€ì¦ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
ìƒì„±ëœ ë‹µë³€ì´ ê³ ê°ì˜ ì§ˆë¬¸ì— ì ì ˆíˆ ëŒ€ì‘í•˜ëŠ”ì§€ ì—„ê²©í•˜ê²Œ í‰ê°€í•˜ì„¸ìš”.

âš ï¸ ì—„ê²©í•œ í‰ê°€ ê¸°ì¤€:
1. ë‹µë³€ì´ ì§ˆë¬¸ì˜ í•µì‹¬ í–‰ë™ ìš”ì²­ê³¼ ì¼ì¹˜í•˜ëŠ”ê°€? (ë³µì‚¬â‰ ì¬ìƒ)
2. ë‹µë³€ì´ ì§ˆë¬¸ì˜ ì£¼ì œ ì˜ì—­ê³¼ ì¼ì¹˜í•˜ëŠ”ê°€? (í…ìŠ¤íŠ¸â‰ ìŒì„±)
3. ë‹µë³€ì´ ì‹¤ì œ ë¬¸ì œ í•´ê²°ì— ì§ì ‘ì ìœ¼ë¡œ ë„ì›€ì´ ë˜ëŠ”ê°€?
4. ë‹µë³€ì—ì„œ ì–¸ê¸‰í•˜ëŠ” ê¸°ëŠ¥ì´ ì§ˆë¬¸ì—ì„œ ìš”ì²­í•œ ê¸°ëŠ¥ê³¼ ê°™ì€ê°€?

ğŸš« ë¶€ì ì ˆí•œ ë‹µë³€ ì˜ˆì‹œ:
- í…ìŠ¤íŠ¸ ë³µì‚¬ ì§ˆë¬¸ì— ìŒì„± ì¬ìƒ ë‹µë³€
- ê²€ìƒ‰ ê¸°ëŠ¥ ì§ˆë¬¸ì— ì„¤ì • ë³€ê²½ ë‹µë³€  
- ì˜¤ë¥˜ ì‹ ê³ ì— ì¼ë°˜ ì‚¬ìš©ë²• ë‹µë³€
- êµ¬ì²´ì  ê¸°ëŠ¥ ì§ˆë¬¸ì— ì¶”ìƒì  ì•ˆë‚´ ë‹µë³€

ê²°ê³¼: "relevant" ë˜ëŠ” "irrelevant" ì¤‘ í•˜ë‚˜ë§Œ ë°˜í™˜í•˜ì„¸ìš”."""

                user_prompt = f"""ì§ˆë¬¸ ë¶„ì„:
ì˜ë„: {question_analysis.get('intent_type', 'N/A')}
ì£¼ì œ: {question_analysis.get('main_topic', 'N/A')}
í–‰ë™ìœ í˜•: {question_analysis.get('action_type', 'N/A')}
ìš”ì²­ì‚¬í•­: {question_analysis.get('specific_request', 'N/A')}

ì›ë³¸ ì§ˆë¬¸: {query}

ìƒì„±ëœ ë‹µë³€: {answer}

âš ï¸ íŠ¹íˆ ì£¼ì˜: ì§ˆë¬¸ì˜ í–‰ë™ìœ í˜•ê³¼ ë‹µë³€ì—ì„œ ë‹¤ë£¨ëŠ” í–‰ë™ì´ ë‹¤ë¥´ë©´ "irrelevant"ì…ë‹ˆë‹¤.
ì´ ë‹µë³€ì´ ì§ˆë¬¸ì— ì ì ˆí•œì§€ ì—„ê²©í•˜ê²Œ í‰ê°€í•´ì£¼ì„¸ìš”."""

                response = self.openai_client.chat.completions.create(
                    model='gpt-3.5-turbo',
                    messages=[
                        {"role": "system", "content": system_prompt},
                        {"role": "user", "content": user_prompt}
                    ],
                    max_tokens=30,
                    temperature=0.1
                )
                
                result = response.choices[0].message.content.strip().lower()
                
                is_relevant = 'relevant' in result and 'irrelevant' not in result
                
                logging.info(f"AI ë‹µë³€ ê´€ë ¨ì„± ê²€ì¦: {result} -> {is_relevant}")
                
                return is_relevant
                
        except Exception as e:
            logging.error(f"AI ë‹µë³€ ê´€ë ¨ì„± ê²€ì¦ ì‹¤íŒ¨: {e}")
            # í´ë°±: ê¸°ë³¸ì ì¸ í‚¤ì›Œë“œ ë§¤ì¹­
            query_keywords = set(self.extract_keywords(query.lower()))
            answer_keywords = set(self.extract_keywords(answer.lower()))
            
            keyword_overlap = len(query_keywords & answer_keywords)
            keyword_relevance = keyword_overlap / max(len(query_keywords), 1)
            
            return keyword_relevance >= 0.2  # 20% ì´ìƒ í‚¤ì›Œë“œ ì¼ì¹˜ì‹œ ê´€ë ¨ì„± ìˆìŒìœ¼ë¡œ íŒë‹¨

    # â˜† ë‹µë³€ ì™„ì„±ë„ ê²€ì¦ ë©”ì„œë“œ (ìƒˆë¡œ ì¶”ê°€)
    def check_answer_completeness(self, answer: str, query: str, lang: str = 'ko') -> float:
        """ìƒì„±ëœ ë‹µë³€ì˜ ì™„ì„±ë„ì™€ ìœ ìš©ì„±ì„ í‰ê°€"""
        
        try:
            # 1. ê¸°ë³¸ ê¸¸ì´ ê²€ì‚¬
            if len(answer.strip()) < 10:
                return 0.0
                
            # 2. ì‹¤ì§ˆì  ë‚´ìš© ë¹„ìœ¨ ê²€ì‚¬
            meaningful_content_ratio = self.calculate_meaningful_content_ratio(answer, lang)
            
            # 3. ì§ˆë¬¸-ë‹µë³€ ê´€ë ¨ì„± í‚¤ì›Œë“œ ë§¤ì¹­
            query_keywords = set(self.extract_keywords(query.lower()))
            answer_keywords = set(self.extract_keywords(answer.lower()))
            keyword_overlap = len(query_keywords & answer_keywords)
            keyword_relevance = keyword_overlap / max(len(query_keywords), 1) if query_keywords else 0.5
            
            # 4. ë‹µë³€ ì™„ê²°ì„± ê²€ì‚¬ (ë¬¸ì¥ì´ ì™„ì„±ë˜ì–´ ìˆëŠ”ì§€)
            completeness_score = self.check_sentence_completeness(answer, lang)
            
            # 5. êµ¬ì²´ì„± ê²€ì‚¬ (êµ¬ì²´ì ì¸ ì •ë³´ê°€ í¬í•¨ë˜ì–´ ìˆëŠ”ì§€)
            specificity_score = self.check_answer_specificity(answer, query, lang)
            
            # 6. ì¢…í•© ì ìˆ˜ ê³„ì‚° (ê°€ì¤‘ í‰ê· )
            final_score = (
                meaningful_content_ratio * 0.3 +    # ì˜ë¯¸ìˆëŠ” ë‚´ìš© ë¹„ìœ¨
                keyword_relevance * 0.25 +          # í‚¤ì›Œë“œ ê´€ë ¨ì„±
                completeness_score * 0.25 +         # ë¬¸ì¥ ì™„ê²°ì„±
                specificity_score * 0.2             # êµ¬ì²´ì„±
            )
            
            logging.info(f"ë‹µë³€ ì™„ì„±ë„ ë¶„ì„: ë‚´ìš©ë¹„ìœ¨={meaningful_content_ratio:.2f}, "
                        f"í‚¤ì›Œë“œê´€ë ¨ì„±={keyword_relevance:.2f}, ì™„ê²°ì„±={completeness_score:.2f}, "
                        f"êµ¬ì²´ì„±={specificity_score:.2f}, ìµœì¢…ì ìˆ˜={final_score:.2f}")
            
            return min(final_score, 1.0)
            
        except Exception as e:
            logging.error(f"ë‹µë³€ ì™„ì„±ë„ ê²€ì¦ ì‹¤íŒ¨: {e}")
            return 0.5  # ì˜¤ë¥˜ì‹œ ì¤‘ê°„ê°’ ë°˜í™˜

    # â˜† ì˜ë¯¸ìˆëŠ” ë‚´ìš© ë¹„ìœ¨ ê³„ì‚°
    def calculate_meaningful_content_ratio(self, text: str, lang: str = 'ko') -> float:
        """í…ìŠ¤íŠ¸ì—ì„œ ì˜ë¯¸ìˆëŠ” ë‚´ìš©ì˜ ë¹„ìœ¨ì„ ê³„ì‚°"""
        
        if not text:
            return 0.0
            
        # HTML íƒœê·¸ ì œê±°
        clean_text = re.sub(r'<[^>]+>', '', text)
        
        if lang == 'ko':
            # í•œêµ­ì–´ ë¶ˆìš©êµ¬ ì œê±°
            filler_patterns = [
                r'ì•ˆë…•í•˜ì„¸ìš”[^.]*\.',
                r'ê°ì‚¬[ë“œë¦½]*ë‹ˆë‹¤[^.]*\.',
                r'í‰ì•ˆí•˜ì„¸ìš”[^.]*\.',
                r'ì£¼ë‹˜\s*ì•ˆì—ì„œ[^.]*\.',
                r'ë°”ì´ë¸”\s*ì• í”Œ[^.]*\.',
                r'GOODTV[^.]*\.',
                r'ë¬¸ì˜[í•´ì£¼ì…”ì„œ]*\s*ê°ì‚¬[^.]*\.',
                r'ì•ˆë‚´[í•´]*ë“œë¦¬ê² ìŠµë‹ˆë‹¤[^.]*\.',
                r'ë„ì›€ì´\s*[ë˜]*[ì‹œ]*[ê¸¸]*[ë°”ë¼]*[ë©°]*[^.]*\.',
                r'í•­ìƒ[^.]*ë°”ì´ë¸”\s*ì• í”Œ[^.]*\.'
            ]
        else:
            # ì˜ì–´ ë¶ˆìš©êµ¬ ì œê±°
            filler_patterns = [
                r'Hello[^.]*\.',
                r'Thank you[^.]*\.',
                r'Best regards[^.]*\.',
                r'God bless[^.]*\.',
                r'Bible App[^.]*\.',
                r'GOODTV[^.]*\.',
                r'We will[^.]*\.',
                r'Please contact[^.]*\.'
            ]
        
        # ë¶ˆìš©êµ¬ ì œê±°
        for pattern in filler_patterns:
            clean_text = re.sub(pattern, '', clean_text, flags=re.IGNORECASE)
        
        # ê³µë°± ì •ë¦¬
        clean_text = re.sub(r'\s+', ' ', clean_text).strip()
        
        # ì˜ë¯¸ìˆëŠ” ë‚´ìš© ë¹„ìœ¨ ê³„ì‚°
        original_length = len(re.sub(r'<[^>]+>', '', text).strip())
        meaningful_length = len(clean_text)
        
        if original_length == 0:
            return 0.0
            
        ratio = meaningful_length / original_length
        return min(ratio, 1.0)

    # â˜† ë¬¸ì¥ ì™„ê²°ì„± ê²€ì‚¬
    def check_sentence_completeness(self, text: str, lang: str = 'ko') -> float:
        """ë¬¸ì¥ì´ ì™„ì„±ë˜ì–´ ìˆëŠ”ì§€ ê²€ì‚¬"""
        
        if not text:
            return 0.0
            
        # HTML íƒœê·¸ ì œê±°
        clean_text = re.sub(r'<[^>]+>', '', text).strip()
        
        if len(clean_text) < 5:
            return 0.0
        
        # ë¬¸ì¥ ë í‘œì‹œ í™•ì¸
        if lang == 'ko':
            sentence_endings = r'[.!?ë‹ˆë‹¤ìš”ìŒë©ë‹¤ìŒê¹Œë‹¤í•˜ì„¸ìš”ìŠµë‹ˆë‹¤ë‹ˆê¹Œ]'
        else:
            sentence_endings = r'[.!?]'
        
        # ë§ˆì§€ë§‰ ë¬¸ì¥ì´ ì™„ì„±ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸
        if re.search(sentence_endings + r'\s*$', clean_text):
            return 1.0
        
        # ì¤‘ê°„ì— ì™„ì„±ëœ ë¬¸ì¥ì´ ìˆëŠ”ì§€ í™•ì¸
        sentences = re.split(sentence_endings, clean_text)
        if len(sentences) > 1:
            return 0.7  # ë¶€ë¶„ì ìœ¼ë¡œ ì™„ì„±ë¨
        
        # ë¬¸ì¥ì´ ë¶ˆì™„ì „í•œ ê²½ìš°
        return 0.3

    # â˜† ë‹µë³€ êµ¬ì²´ì„± ê²€ì‚¬ (ë¹ˆ ì•½ì† íŒ¨í„´ ê°ì§€ ê°•í™”)
    def check_answer_specificity(self, answer: str, query: str, lang: str = 'ko') -> float:
        """ë‹µë³€ì´ êµ¬ì²´ì ì¸ ì •ë³´ë¥¼ í¬í•¨í•˜ëŠ”ì§€ ê²€ì‚¬ (ë¹ˆ ì•½ì† íŒ¨í„´ ì—„ê²© ê°ì§€)"""
        
        if not answer:
            return 0.0
        
        # ğŸ”¥ ë¹ˆ ì•½ì† íŒ¨í„´ ì—„ê²© ê°ì§€ (Empty Promise Detection)
        empty_promise_score = self.detect_empty_promises(answer, lang)
        if empty_promise_score < 0.3:  # ë¹ˆ ì•½ì†ì´ ê°ì§€ë˜ë©´ ë§¤ìš° ë‚®ì€ ì ìˆ˜
            logging.warning(f"ë¹ˆ ì•½ì† íŒ¨í„´ ê°ì§€! ì ìˆ˜: {empty_promise_score:.2f}")
            return empty_promise_score
            
        specificity_score = 0.0
        
        if lang == 'ko':
            # êµ¬ì²´ì  ì •ë³´ íŒ¨í„´ (í•œêµ­ì–´) - ë” ì—„ê²©í•˜ê²Œ ê°•í™”
            specific_patterns = [
                r'\d+[ê°€ì§€ê°œë‹¨ê³„ë²ˆì§¸ì°¨ë¡€]',  # ìˆ«ìê°€ í¬í•¨ëœ ë‹¨ê³„
                r'[ë©”ë‰´ì„¤ì •í™”ë©´ë²„íŠ¼íƒ­]ì—ì„œ',    # êµ¬ì²´ì  ìœ„ì¹˜
                r'ë‹¤ìŒê³¼\s*ê°™[ì€ì´]',         # êµ¬ì²´ì  ë°©ë²• ì œì‹œ
                r'[í´ë¦­ì„ íƒí„°ì¹˜ëˆ„ë¥´]',         # êµ¬ì²´ì  ë™ì‘
                r'[ë°©ë²•ë‹¨ê³„ì ˆì°¨ê³¼ì •]',         # êµ¬ì²´ì  í”„ë¡œì„¸ìŠ¤
                r'\w+\s*ë²„íŠ¼',               # ë²„íŠ¼ëª…
                r'\w+\s*ë©”ë‰´',               # ë©”ë‰´ëª…
                r'NIV|KJV|ESV|ë²ˆì—­ë³¸',       # êµ¬ì²´ì  ë²ˆì—­ë³¸
                r'[ìƒí•˜ì¢Œìš°]ë‹¨[ì—ì˜]',         # êµ¬ì²´ì  ìœ„ì¹˜
                r'ì„¤ì •[ì—ì„œìœ¼ë¡œ]',            # ì„¤ì • ê´€ë ¨
                r'í™”ë©´\s*[ìƒí•˜ì¢Œìš°ì¤‘ì•™]',      # í™”ë©´ ìœ„ì¹˜
                r'íƒ­í•˜ì—¬|í´ë¦­í•˜ì—¬|í„°ì¹˜í•˜ì—¬',    # êµ¬ì²´ì  í–‰ë™
                r'ë‹¤ìŒ\s*ìˆœì„œ',              # ìˆœì„œ ì•ˆë‚´
                r'ë¨¼ì €|ê·¸ë‹¤ìŒ|ë§ˆì§€ë§‰ìœ¼ë¡œ'       # ë‹¨ê³„ë³„ ì•ˆë‚´
            ]
            
            # ğŸ”¥ ë¹ˆ ì•½ì†/ëª¨í˜¸í•œ í‘œí˜„ íŒ¨í„´ (ë” ì—„ê²©í•˜ê²Œ)
            vague_patterns = [
                r'ì•ˆë‚´[í•´]*ë“œë¦¬ê² ìŠµë‹ˆë‹¤',
                r'ë„ì›€[ì„ì´]\s*ë“œë¦¬ê² ìŠµë‹ˆë‹¤',
                r'í™•ì¸[í•˜ê³ í•˜ì—¬í•´ì„œ]',
                r'ê²€í† [í•˜ê³ í•˜ì—¬]',
                r'ì¤€ë¹„[í•˜ê³ í•˜ê² ìŠµë‹ˆë‹¤]',
                r'ì „ë‹¬[í•˜ê³ í•˜ê² ë“œë¦¬ê² ]',
                r'ì œê³µ[í•˜ê³ í•˜ê² ë“œë¦¬ê² ]',
                r'ë…¸ë ¥[í•˜ê³ í•˜ê² ]',
                r'ì‚´í´[ë³´ê³ ë³´ê² ]',
                r'ë°©ë²•[ì„ì´]\s*ì°¾ì•„[ë“œë¦¬ê² ë³´ê² ]'
            ]
        else:
            # êµ¬ì²´ì  ì •ë³´ íŒ¨í„´ (ì˜ì–´)
            specific_patterns = [
                r'\d+\s*steps?',
                r'follow\s+these',
                r'click\s+on',
                r'go\s+to',
                r'select\s+\w+',
                r'settings?\s+menu',
                r'NIV|KJV|ESV|translation',
                r'top\s+of\s+screen',
                r'button\s+\w+'
            ]
            
            vague_patterns = [
                r'we\s+will\s+review',
                r'we\s+are\s+working',
                r'please\s+contact',
                r'will\s+be\s+available'
            ]
        
        # êµ¬ì²´ì„± ì ìˆ˜ ê³„ì‚°
        specific_count = 0
        for pattern in specific_patterns:
            specific_count += len(re.findall(pattern, answer, re.IGNORECASE))
        
        vague_count = 0
        for pattern in vague_patterns:
            vague_count += len(re.findall(pattern, answer, re.IGNORECASE))
        
        # êµ¬ì²´ì  ì •ë³´ê°€ ë§ê³  ëª¨í˜¸í•œ í‘œí˜„ì´ ì ì„ìˆ˜ë¡ ë†’ì€ ì ìˆ˜
        if specific_count > 0:
            specificity_score = specific_count / (specific_count + vague_count + 1)
        else:
            specificity_score = 0.1 if vague_count == 0 else 0.0
        
        return min(specificity_score, 1.0)

    # â˜† ë¹ˆ ì•½ì† íŒ¨í„´ ê°ì§€ ë©”ì„œë“œ (ìƒˆë¡œ ì¶”ê°€)
    def detect_empty_promises(self, answer: str, lang: str = 'ko') -> float:
        """ì•½ì†ë§Œ í•˜ê³  ì‹¤ì œ ë‚´ìš©ì´ ì—†ëŠ” ë¹ˆ ì•½ì† íŒ¨í„´ì„ ê°ì§€"""
        
        if not answer:
            return 0.0
        
        # HTML íƒœê·¸ ì œê±°í•˜ì—¬ ìˆœìˆ˜ í…ìŠ¤íŠ¸ë¡œ ë¶„ì„
        clean_text = re.sub(r'<[^>]+>', '', answer)
        
        if lang == 'ko':
            # ğŸ”¥ ìœ„í—˜í•œ ì•½ì† í‘œí˜„ë“¤ (ì´í›„ ì‹¤ì œ ë‚´ìš©ì´ ì™€ì•¼ í•¨)
            promise_patterns = [
                r'ì•ˆë‚´[í•´]*ë“œë¦¬ê² ìŠµë‹ˆë‹¤',
                r'ë„ì›€[ì„ì´]?\s*ë“œë¦¬ê² ìŠµë‹ˆë‹¤',
                r'ë°©ë²•[ì„ì´]?\s*ì•ˆë‚´[í•´]*ë“œë¦¬ê² ìŠµë‹ˆë‹¤',
                r'ì„¤ëª…[í•´]*ë“œë¦¬ê² ìŠµë‹ˆë‹¤',
                r'ì•Œë ¤[ë“œë¦¬ê² ë“œë¦´]',
                r'ì œê³µ[í•´]*ë“œë¦¬ê² ìŠµë‹ˆë‹¤',
                r'ë„ì™€[ë“œë¦¬ê² ë“œë¦´]',
                r'ì°¾ì•„[ë“œë¦¬ê² ë“œë¦´]'
            ]
            
            # ì‹¤ì œ ë‚´ìš©ì„ ë‚˜íƒ€ë‚´ëŠ” íŒ¨í„´ë“¤
            content_patterns = [
                r'\d+\.\s*',                    # ë²ˆí˜¸ ë§¤ê¸°ê¸° (1., 2., ...)
                r'ë¨¼ì €',                       # ë‹¨ê³„ë³„ ì„¤ëª… ì‹œì‘
                r'ë‹¤ìŒê³¼?\s*ê°™[ì€ì´]',           # êµ¬ì²´ì  ë°©ë²• ì œì‹œ
                r'[ë©”ë‰´ì„¤ì •í™”ë©´ë²„íŠ¼]',           # êµ¬ì²´ì  UI ìš”ì†Œ
                r'í´ë¦­|í„°ì¹˜|ì„ íƒ|ì´ë™',          # êµ¬ì²´ì  í–‰ë™
                r'NIV|KJV|ESV',               # êµ¬ì²´ì  ë²ˆì—­ë³¸
                r'ìƒë‹¨|í•˜ë‹¨|ì¢Œì¸¡|ìš°ì¸¡',         # êµ¬ì²´ì  ìœ„ì¹˜
                r'ì„¤ì •ì—ì„œ|ë©”ë‰´ì—ì„œ',           # êµ¬ì²´ì  ê²½ë¡œ
                r'ë‹¤ìŒ\s*[ìˆœì„œë‹¨ê³„ë°©ë²•ì ˆì°¨]',    # ë‹¨ê³„ë³„ ì•ˆë‚´
                r'[0-9]+[ë²ˆì§¸ë‹¨ê³„]',           # ìˆœì„œ í‘œì‹œ
                r'í™”ë©´\s*[ìƒí•˜ì¢Œìš°ì¤‘ì•™]'        # ìœ„ì¹˜ ì„¤ëª…
            ]
        else:  # ì˜ì–´
            promise_patterns = [
                r'will\s+guide\s+you',
                r'will\s+help\s+you',
                r'will\s+show\s+you',
                r'will\s+provide',
                r'let\s+me\s+help',
                r'here[\'\"]s\s+how'
            ]
            
            content_patterns = [
                r'\d+\.\s*',
                r'first|second|third',
                r'step\s+\d+',
                r'click|tap|select',
                r'menu|setting|screen',
                r'NIV|KJV|ESV',
                r'top|bottom|left|right'
            ]
        
        # ì•½ì† í‘œí˜„ ì°¾ê¸°
        promise_count = 0
        promise_positions = []
        
        for pattern in promise_patterns:
            matches = list(re.finditer(pattern, clean_text, re.IGNORECASE))
            promise_count += len(matches)
            promise_positions.extend([match.start() for match in matches])
        
        if promise_count == 0:
            return 0.8  # ì•½ì† í‘œí˜„ì´ ì—†ìœ¼ë©´ ì¤‘ê°„ ì ìˆ˜
        
        # ì•½ì† ì´í›„ì— ì‹¤ì œ ë‚´ìš©ì´ ìˆëŠ”ì§€ í™•ì¸
        content_after_promise = 0
        total_text_after_promises = 0
        
        for pos in promise_positions:
            # ì•½ì† í‘œí˜„ ì´í›„ì˜ í…ìŠ¤íŠ¸ ì¶”ì¶œ
            text_after = clean_text[pos:]
            
            # ëë§ºìŒë§ ì œê±°í•˜ì—¬ ì‹¤ì œ ë‚´ìš©ë§Œ ê²€ì‚¬
            text_after = re.sub(r'í•­ìƒ\s*ì„±ë„ë‹˜ê»˜[^.]*\.', '', text_after, flags=re.IGNORECASE)
            text_after = re.sub(r'ê°ì‚¬í•©ë‹ˆë‹¤[^.]*\.', '', text_after, flags=re.IGNORECASE)
            text_after = re.sub(r'ì£¼ë‹˜\s*ì•ˆì—ì„œ[^.]*\.', '', text_after, flags=re.IGNORECASE)
            text_after = re.sub(r'í‰ì•ˆí•˜ì„¸ìš”[^.]*\.', '', text_after, flags=re.IGNORECASE)
            
            total_text_after_promises += len(text_after.strip())
            
            # ì‹¤ì œ ë‚´ìš© íŒ¨í„´ì´ ìˆëŠ”ì§€ í™•ì¸
            for content_pattern in content_patterns:
                if re.search(content_pattern, text_after, re.IGNORECASE):
                    content_after_promise += 1
                    break
        
        # ì ìˆ˜ ê³„ì‚°
        if promise_count > 0:
            # ì•½ì† ëŒ€ë¹„ ì‹¤ì œ ë‚´ìš© ë¹„ìœ¨
            content_ratio = content_after_promise / promise_count
            
            # ì•½ì† ì´í›„ í…ìŠ¤íŠ¸ ê¸¸ì´ ë¹„ìœ¨ (í‰ê· )
            avg_length_after = total_text_after_promises / len(promise_positions) if promise_positions else 0
            length_score = min(avg_length_after / 100, 1.0)  # 100ì ê¸°ì¤€ìœ¼ë¡œ ì •ê·œí™”
            
            # ìµœì¢… ì ìˆ˜ (ë‚´ìš© ë¹„ìœ¨ê³¼ ê¸¸ì´ë¥¼ ê³ ë ¤)
            final_score = content_ratio * 0.7 + length_score * 0.3
            
            logging.info(f"ë¹ˆ ì•½ì† ë¶„ì„: ì•½ì†={promise_count}ê°œ, ì‹¤ì œë‚´ìš©={content_after_promise}ê°œ, "
                        f"ë‚´ìš©ë¹„ìœ¨={content_ratio:.2f}, ê¸¸ì´ì ìˆ˜={length_score:.2f}, ìµœì¢…ì ìˆ˜={final_score:.2f}")
            
            return final_score
        
        return 0.5  # ê¸°ë³¸ê°’

    # â˜† í• ë£¨ì‹œë„¤ì´ì…˜ ë° ì¼ê´€ì„± ê²€ì¦ ë©”ì„œë“œ (ìƒˆë¡œ ì¶”ê°€)
    def detect_hallucination_and_inconsistency(self, answer: str, query: str, lang: str = 'ko') -> dict:
        """ìƒì„±ëœ ë‹µë³€ì—ì„œ í• ë£¨ì‹œë„¤ì´ì…˜ê³¼ ì¼ê´€ì„± ë¬¸ì œë¥¼ ê°ì§€"""
        
        issues = {
            'external_app_recommendation': False,
            'bible_app_domain_violation': False,
            'content_inconsistency': False,
            'translation_switching': False,
            'invalid_features': False,
            'overall_score': 1.0,
            'detected_issues': []
        }
        
        if not answer:
            return issues
        
        # HTML íƒœê·¸ ì œê±°í•˜ì—¬ ìˆœìˆ˜ í…ìŠ¤íŠ¸ë¡œ ë¶„ì„
        clean_answer = re.sub(r'<[^>]+>', '', answer)
        clean_query = re.sub(r'<[^>]+>', '', query)
        
        if lang == 'ko':
            # 1. ğŸš¨ ì™¸ë¶€ ì•± ì¶”ì²œ ê°ì§€ (ì¹˜ëª…ì )
            external_app_patterns = [
                r'Parallel\s*Bible',
                r'ë³‘ë ¬\s*ì„±ê²½\s*ì•±',
                r'ë‹¤ë¥¸\s*ì•±ì„?\s*(ë‹¤ìš´ë¡œë“œ|ì„¤ì¹˜)',
                r'ì•±\s*ìŠ¤í† ì–´ì—ì„œ\s*(ê²€ìƒ‰|ë‹¤ìš´ë¡œë“œ)',
                r'êµ¬ê¸€\s*í”Œë ˆì´\s*ìŠ¤í† ì–´',
                r'ì™¸ë¶€\s*(ì•±|ì–´í”Œë¦¬ì¼€ì´ì…˜)',
                r'ë³„ë„[ì˜]*\s*(ì•±|ì–´í”Œ)',
                r'ì¶”ê°€ë¡œ\s*(ì•±ì„|ì–´í”Œì„)\s*ì„¤ì¹˜'
            ]
            
            for pattern in external_app_patterns:
                if re.search(pattern, clean_answer, re.IGNORECASE):
                    issues['external_app_recommendation'] = True
                    issues['detected_issues'].append(f"ì™¸ë¶€ ì•± ì¶”ì²œ ê°ì§€: {pattern}")
                    issues['overall_score'] -= 0.8  # ë§¤ìš° ì‹¬ê°í•œ ê°ì 
            
            # 2. ğŸš¨ ë°”ì´ë¸” ì• í”Œ ë„ë©”ì¸ ìœ„ë°˜ ê°ì§€
            domain_violation_patterns = [
                r'ë°”ì´ë¸”\s*ì• í”Œì—[ì„œëŠ”]*\s*ì§€ì›[í•˜ì§€]*\s*ì•Š',
                r'ë°”ì´ë¸”\s*ì• í”Œë¡œ[ëŠ”]*\s*(ë¶ˆê°€ëŠ¥|ì•ˆ\s*ë¨)',
                r'ë‹¤ë¥¸\s*(ë°©ë²•|ì„œë¹„ìŠ¤)ì„\s*ì´ìš©',
                r'ì™¸ë¶€\s*ì„œë¹„ìŠ¤ë¥¼\s*í†µí•´',
                r'ë°”ì´ë¸”\s*ì• í”Œ\s*ë°–ì—ì„œ'
            ]
            
            for pattern in domain_violation_patterns:
                if re.search(pattern, clean_answer, re.IGNORECASE):
                    issues['bible_app_domain_violation'] = True
                    issues['detected_issues'].append(f"ë„ë©”ì¸ ìœ„ë°˜: {pattern}")
                    issues['overall_score'] -= 0.6
            
            # 3. ğŸš¨ ë²ˆì—­ë³¸ ë³€ê²½/êµì²´ ê°ì§€ (ì§ˆë¬¸ vs ë‹µë³€)
            query_translations = self.extract_translations_from_text(clean_query)
            answer_translations = self.extract_translations_from_text(clean_answer)
            
            if query_translations and answer_translations:
                # ì§ˆë¬¸ì—ì„œ ì–¸ê¸‰í•œ ë²ˆì—­ë³¸ì´ ë‹µë³€ì—ì„œ ë‹¤ë¥¸ ë²ˆì—­ë³¸ìœ¼ë¡œ ë°”ë€Œì—ˆëŠ”ì§€ í™•ì¸
                query_set = set(query_translations)
                answer_set = set(answer_translations)
                
                # ì§ˆë¬¸ì— ì—†ë˜ ë²ˆì—­ë³¸ì´ ë‹µë³€ì— ì¶”ê°€ë˜ì—ˆëŠ”ì§€ í™•ì¸
                unexpected_translations = answer_set - query_set
                if unexpected_translations:
                    # ë‹¨, ì¼ë°˜ì ì¸ í™•ì¥(ì˜ˆ: ê°œì—­ê°œì • â†’ ê°œì—­ê°œì •+ê°œì—­í•œê¸€)ì€ í—ˆìš©
                    # í•˜ì§€ë§Œ ì™„ì „íˆ ë‹¤ë¥¸ ë²ˆì—­ë³¸(ì˜ˆ: ê°œì—­í•œê¸€ â†’ ì˜ë¬¸ì„±ê²½)ì€ ê¸ˆì§€
                    problematic = False
                    for trans in unexpected_translations:
                        if any(forbidden in trans.lower() for forbidden in ['ì˜ì–´', 'english', 'niv', 'kjv', 'esv']) and \
                           not any(allowed in q_trans.lower() for q_trans in query_translations for allowed in ['ì˜ì–´', 'english', 'niv', 'kjv', 'esv']):
                            problematic = True
                            break
                        elif any(forbidden in trans.lower() for forbidden in ['í•œê¸€', 'ê°œì—­', 'korean']) and \
                             not any(allowed in q_trans.lower() for q_trans in query_translations for allowed in ['í•œê¸€', 'ê°œì—­', 'korean']):
                            problematic = True
                            break
                    
                    if problematic:
                        issues['translation_switching'] = True
                        issues['detected_issues'].append(f"ë²ˆì—­ë³¸ ë³€ê²½: {query_translations} â†’ {list(unexpected_translations)}")
                        issues['overall_score'] -= 0.7
            
            # 4. ğŸš¨ ë‚´ìš© ì¼ê´€ì„± ê²€ì‚¬ (ë‹µë³€ ë‚´ë¶€ì—ì„œ ë‚´ìš©ì´ ë°”ë€ŒëŠ”ì§€)
            answer_sentences = re.split(r'[.!?]\s+', clean_answer)
            if len(answer_sentences) >= 3:
                # ë‹µë³€ ì „ë°˜ë¶€ì™€ í›„ë°˜ë¶€ì˜ ë²ˆì—­ë³¸ ì–¸ê¸‰ì´ ë‹¤ë¥¸ì§€ í™•ì¸
                first_half = ' '.join(answer_sentences[:len(answer_sentences)//2])
                second_half = ' '.join(answer_sentences[len(answer_sentences)//2:])
                
                first_translations = self.extract_translations_from_text(first_half)
                second_translations = self.extract_translations_from_text(second_half)
                
                if first_translations and second_translations:
                    if set(first_translations) != set(second_translations):
                        # ì™„ì „íˆ ë‹¤ë¥¸ ë²ˆì—­ë³¸ìœ¼ë¡œ ë°”ë€Œì—ˆëŠ”ì§€ í™•ì¸
                        if not (set(first_translations) & set(second_translations)):  # êµì§‘í•©ì´ ì—†ìœ¼ë©´
                            issues['content_inconsistency'] = True
                            issues['detected_issues'].append(f"ë‚´ìš© ì¼ê´€ì„± ìœ„ë°˜: {first_translations} â†’ {second_translations}")
                            issues['overall_score'] -= 0.8
            
            # 5. ì¡´ì¬í•˜ì§€ ì•ŠëŠ” ê¸°ëŠ¥ ì–¸ê¸‰ ê°ì§€
            invalid_feature_patterns = [
                r'í™”ë©´\s*ë¶„í• \s*ê¸°ëŠ¥',
                r'ë³‘ë ¬\s*ëª¨ë“œ',
                r'ë¶„í• \s*í™”ë©´\s*ì„¤ì •',
                r'ë™ì‹œ\s*ì‹¤í–‰\s*ëª¨ë“œ'
            ]
            
            for pattern in invalid_feature_patterns:
                if re.search(pattern, clean_answer, re.IGNORECASE):
                    issues['invalid_features'] = True
                    issues['detected_issues'].append(f"ì¡´ì¬í•˜ì§€ ì•ŠëŠ” ê¸°ëŠ¥: {pattern}")
                    issues['overall_score'] -= 0.4
        
        # ìµœì¢… ì ìˆ˜ ì •ê·œí™”
        issues['overall_score'] = max(issues['overall_score'], 0.0)
        
        # ì‹¬ê°í•œ ë¬¸ì œê°€ í•˜ë‚˜ë¼ë„ ìˆìœ¼ë©´ ì „ì²´ ì ìˆ˜ë¥¼ ë§¤ìš° ë‚®ê²Œ
        critical_issues = [
            issues['external_app_recommendation'],
            issues['bible_app_domain_violation'],
            issues['translation_switching'],
            issues['content_inconsistency']
        ]
        
        if any(critical_issues):
            issues['overall_score'] = min(issues['overall_score'], 0.2)
        
        logging.info(f"í• ë£¨ì‹œë„¤ì´ì…˜ ê²€ì¦ ê²°ê³¼: ì ìˆ˜={issues['overall_score']:.2f}, ë¬¸ì œ={len(issues['detected_issues'])}ê°œ")
        
        return issues

    # â˜† í…ìŠ¤íŠ¸ì—ì„œ ë²ˆì—­ë³¸ ì¶”ì¶œí•˜ëŠ” í—¬í¼ ë©”ì„œë“œ
    def extract_translations_from_text(self, text: str) -> list:
        """í…ìŠ¤íŠ¸ì—ì„œ ì„±ê²½ ë²ˆì—­ë³¸ëª…ì„ ì¶”ì¶œ"""
        
        translation_patterns = [
            r'NIV',
            r'KJV', 
            r'ESV',
            r'ê°œì—­ê°œì •',
            r'ê°œì—­í•œê¸€',
            r'ê°œì—­\s*ê°œì •',
            r'ê°œì—­\s*í•œê¸€',
            r'ì˜ì–´\s*ë²ˆì—­ë³¸',
            r'ì˜ë¬¸\s*ì„±ê²½',
            r'í•œê¸€\s*ë²ˆì—­ë³¸',
            r'í•œêµ­ì–´\s*ì„±ê²½'
        ]
        
        found_translations = []
        for pattern in translation_patterns:
            matches = re.findall(pattern, text, re.IGNORECASE)
            found_translations.extend(matches)
        
        # ì¤‘ë³µ ì œê±° ë° ì •ê·œí™”
        normalized = []
        for trans in found_translations:
            trans = re.sub(r'\s+', '', trans)  # ê³µë°± ì œê±°
            if trans not in normalized:
                normalized.append(trans)
        
        return normalized

    # â˜† ìµœì ì˜ í´ë°± ë‹µë³€ ì„ íƒ ë©”ì„œë“œ (ì§ì ‘ ì‚¬ìš© ë‹µë³€ í¬í•¨)
    def get_best_fallback_answer(self, similar_answers: list, lang: str = 'ko') -> str:
        logging.info(f"=== get_best_fallback_answer ì‹œì‘ ===")
        logging.info(f"ì…ë ¥ëœ similar_answers ê°œìˆ˜: {len(similar_answers)}")
        
        if not similar_answers:
            logging.warning("similar_answersê°€ ë¹„ì–´ìˆìŒ")
            return ""
        
        # ì…ë ¥ëœ ë‹µë³€ë“¤ ë¯¸ë¦¬ë³´ê¸°
        for i, ans in enumerate(similar_answers[:3]):
            logging.info(f"ë‹µë³€ #{i+1}: ì ìˆ˜={ans['score']:.3f}, ê¸¸ì´={len(ans.get('answer', ''))}, ë‚´ìš©ë¯¸ë¦¬ë³´ê¸°={ans.get('answer', '')[:50]}...")
        
        # ì ìˆ˜ì™€ í…ìŠ¤íŠ¸ í’ˆì§ˆì„ ì¢…í•© í‰ê°€
        best_answer = ""
        best_score = 0
        
        for i, ans in enumerate(similar_answers[:3]): # ìƒìœ„ 3ê°œë§Œ ê²€í†  (í’ˆì§ˆ í–¥ìƒ)
            logging.info(f"--- ë‹µë³€ #{i+1} ì²˜ë¦¬ ì‹œì‘ ---")
            score = ans['score']
            answer_text = ans['answer']  # ì›ë³¸ ë‹µë³€ í…ìŠ¤íŠ¸ ì‚¬ìš©
            logging.info(f"ì›ë³¸ ë‹µë³€ ê¸¸ì´: {len(answer_text)}, ë‚´ìš©: {answer_text[:100]}...")
            
            # ğŸ”¥ ê¸´ê¸‰ ìˆ˜ì •: 0.9 ì´ìƒ ì ìˆ˜ë©´ ì „ì²˜ë¦¬ ì—†ì´ ë°”ë¡œ ë°˜í™˜
            if score >= 0.9:
                logging.info(f"ğŸ”¥ ë§¤ìš° ë†’ì€ ìœ ì‚¬ë„({score:.3f}) - ì „ì²˜ë¦¬ ì—†ì´ ì›ë³¸ ë‹µë³€ ë°”ë¡œ ë°˜í™˜")
                print(f"ğŸ”¥ ë§¤ìš° ë†’ì€ ìœ ì‚¬ë„({score:.3f}) - ì „ì²˜ë¦¬ ì—†ì´ ì›ë³¸ ë‹µë³€ ë°”ë¡œ ë°˜í™˜")
                # ìµœì†Œí•œì˜ ì •ë¦¬ë§Œ
                clean_answer = answer_text.strip()
                if clean_answer:
                    logging.info(f"ğŸ”¥ ì›ë³¸ ë‹µë³€ ì§ì ‘ ë°˜í™˜: ê¸¸ì´={len(clean_answer)}")
                    return clean_answer
            
            # ê¸°ë³¸ ì •ë¦¬ë§Œ ìˆ˜í–‰
            answer_text = self.preprocess_text(answer_text)
            logging.info(f"ì „ì²˜ë¦¬ í›„ ê¸¸ì´: {len(answer_text)}, ë‚´ìš©: {answer_text[:100]}...")
            
            # ì˜ì–´ ì§ˆë¬¸ì¸ ê²½ìš° ë‹µë³€ì„ ë²ˆì—­
            if lang == 'en' and ans.get('lang', 'ko') == 'ko':
                answer_text = self.translate_text(answer_text, 'ko', 'en')
                logging.info(f"ë²ˆì—­ í›„ ê¸¸ì´: {len(answer_text)}")
            
            # ìœ íš¨ì„± ê²€ì‚¬ (ì„ì‹œë¡œ ìš°íšŒ - ë””ë²„ê¹…ìš©)
            is_valid = self.is_valid_text(answer_text, lang)
            logging.info(f"ìœ íš¨ì„± ê²€ì‚¬ ê²°ê³¼: {is_valid}")
            
            # ì„ì‹œë¡œ ìœ íš¨ì„± ê²€ì‚¬ ë¬´ì‹œí•˜ê³  ì§„í–‰
            if not is_valid:
                logging.warning(f"âš ï¸ ë‹µë³€ #{i+1} ìœ íš¨ì„± ê²€ì‚¬ ì‹¤íŒ¨í–ˆì§€ë§Œ ê°•ì œë¡œ ì§„í–‰")
                # continueë¥¼ ì£¼ì„ ì²˜ë¦¬í•˜ì—¬ ìœ íš¨ì„± ê²€ì‚¬ ì‹¤íŒ¨í•´ë„ ê³„ì† ì§„í–‰
            
            # ë†’ì€ ìœ ì‚¬ë„(0.8+)ì¸ ê²½ìš° ê°„ë‹¨í•˜ê²Œ ì²« ë²ˆì§¸ ë‹µë³€ ì„ íƒ
            if score >= 0.8:
                logging.info(f"ë†’ì€ ìœ ì‚¬ë„({score:.3f})ë¡œ ë‹µë³€ #{i+1} ì§ì ‘ ì„ íƒ")
                logging.info(f"ì„ íƒëœ ë‹µë³€ ìµœì¢… ê¸¸ì´: {len(answer_text)}")
                # ğŸ”¥ ìœ íš¨ì„± ê²€ì‚¬ ì‹¤íŒ¨í•´ë„ ê°•ì œë¡œ ë°˜í™˜
                if answer_text and len(answer_text.strip()) > 0:
                    return answer_text
                else:
                    logging.error(f"ğŸ”¥ ì „ì²˜ë¦¬ í›„ ë‹µë³€ì´ ë¹„ì–´ìˆìŒ! ì›ë³¸ìœ¼ë¡œ í´ë°±")
                    return ans['answer'].strip()
            
            # ì¢…í•© ì ìˆ˜ ê³„ì‚° (ìœ ì‚¬ë„ + í…ìŠ¤íŠ¸ ê¸¸ì´ + ì™„ì„±ë„)
            length_score = min(len(answer_text) / 200, 1.0) # 200ì ê¸°ì¤€ ì •ê·œí™”
            completeness_score = 1.0 if answer_text.endswith(('.', '!', '?')) else 0.8
            
            total_score = score * 0.8 + length_score * 0.1 + completeness_score * 0.1  # ìœ ì‚¬ë„ ê°€ì¤‘ì¹˜ ì¦ê°€
            
            logging.info(f"ë‹µë³€ #{i+1} ì¢…í•© ì ìˆ˜: {total_score:.3f} (ìœ ì‚¬ë„:{score:.3f}, ê¸¸ì´:{length_score:.3f}, ì™„ì„±ë„:{completeness_score:.3f})")
            
            if total_score > best_score:
                best_score = total_score
                best_answer = answer_text
                logging.info(f"ìƒˆë¡œìš´ ìµœê³  ì ìˆ˜ ë‹µë³€ìœ¼ë¡œ ì„ íƒë¨")
        
        logging.info(f"=== get_best_fallback_answer ì™„ë£Œ ===")
        logging.info(f"ìµœì¢… ì„ íƒëœ ë‹µë³€ ì ìˆ˜: {best_score:.3f}")
        logging.info(f"ìµœì¢… ë‹µë³€ ê¸¸ì´: {len(best_answer)}")
        logging.info(f"ìµœì¢… ë‹µë³€ ë¯¸ë¦¬ë³´ê¸°: {best_answer[:100] if best_answer else 'None'}...")
        
        # ğŸ”¥ ìµœì¢… ë‹µë³€ í’ˆì§ˆ ê²€ì¦
        if best_answer:
            final_completeness = self.check_answer_completeness(best_answer, "", lang)
            logging.info(f"ìµœì¢… í´ë°± ë‹µë³€ ì™„ì„±ë„: {final_completeness:.2f}")
            
            # ì™„ì„±ë„ê°€ ë„ˆë¬´ ë‚®ìœ¼ë©´ ë‹¤ë¥¸ ë‹µë³€ ì‹œë„
            if final_completeness < 0.4 and len(similar_answers) > 1:
                logging.warning("ìµœì¢… ë‹µë³€ ì™„ì„±ë„ê°€ ë‚®ìŒ, ëŒ€ì•ˆ ë‹µë³€ ê²€ìƒ‰ ì¤‘...")
                
                # ë‹¤ë¥¸ ë‹µë³€ë“¤ë„ ê²€í† 
                for i, alt_ans in enumerate(similar_answers[1:4], 1):  # 2-4ë²ˆì§¸ ë‹µë³€ ê²€í† 
                    alt_processed = self.preprocess_text(alt_ans['answer'])
                    if lang == 'en' and alt_ans.get('lang', 'ko') == 'ko':
                        alt_processed = self.translate_text(alt_processed, 'ko', 'en')
                    
                    alt_completeness = self.check_answer_completeness(alt_processed, "", lang)
                    logging.info(f"ëŒ€ì•ˆ ë‹µë³€ #{i} ì™„ì„±ë„: {alt_completeness:.2f}")
                    
                    if alt_completeness > final_completeness and alt_completeness >= 0.5:
                        logging.info(f"ë” ë‚˜ì€ ëŒ€ì•ˆ ë‹µë³€ #{i} ì„ íƒ")
                        return alt_processed
        
        # ğŸ”¥ ê¸´ê¸‰ ì•ˆì „ì¥ì¹˜: ë‹µë³€ì´ ë¹„ì–´ìˆìœ¼ë©´ ì²« ë²ˆì§¸ ì›ë³¸ ë‹µë³€ ê°•ì œ ë°˜í™˜
        if not best_answer and similar_answers:
            logging.error("ğŸš¨ ìµœì¢… ë‹µë³€ì´ ë¹„ì–´ìˆìŒ! ì²« ë²ˆì§¸ ì›ë³¸ ë‹µë³€ ê°•ì œ ë°˜í™˜")
            print("ğŸš¨ ìµœì¢… ë‹µë³€ì´ ë¹„ì–´ìˆìŒ! ì²« ë²ˆì§¸ ì›ë³¸ ë‹µë³€ ê°•ì œ ë°˜í™˜")
            emergency_answer = similar_answers[0]['answer'].strip()
            logging.info(f"ğŸš¨ ê¸´ê¸‰ ë‹µë³€ ê¸¸ì´: {len(emergency_answer)}")
            return emergency_answer
        
        return best_answer

    # â˜† ë” ë³´ìˆ˜ì ì¸ GPT-3.5-turbo ìƒì„± ë©”ì„œë“œ (ê¸°ì¡´ ì½”ë“œì™€ì˜ í˜¸í™˜ì„± ìœ ì§€)
    # ë³´ìˆ˜ì ì´ê³  ì°¸ê³  ë‹µë³€ì— ì¶©ì‹¤í•œ GPT-3.5-turbo í…ìŠ¤íŠ¸ ìƒì„±
    @profile
    def generate_ai_answer(self, query: str, similar_answers: list, lang: str) -> str:
        
        # 1. ì–¸ì–´ ê°ì§€ (lang íŒŒë¼ë¯¸í„°ê°€ ì—†ê±°ë‚˜ 'auto'ì¸ ê²½ìš°)
        if not lang or lang == 'auto':
            detected_lang = self.detect_language(query)
            lang = detected_lang
            logging.info(f"ê°ì§€ëœ ì–¸ì–´: {lang}")
        
        # 2. ìœ ì‚¬ ë‹µë³€ì´ ì—†ëŠ” ê²½ìš°
        if not similar_answers:
            logging.error("ğŸš¨ ìœ ì‚¬ ë‹µë³€ì´ ì „í˜€ ì—†ìŒ - Pinecone ê²€ìƒ‰ ì‹¤íŒ¨ ë˜ëŠ” ë°ì´í„° ì—†ìŒ")
            print(f"ğŸš¨ CRITICAL: ìœ ì‚¬ ë‹µë³€ì´ ì „í˜€ ì—†ìŒ! query='{query[:50]}...', lang='{lang}'")
            if lang == 'en':
                default_msg = "<p>We need more detailed information to provide an accurate answer to your inquiry.</p><p><br></p><p>Please contact our customer service center for prompt assistance.</p>"
            else:
                default_msg = "<p>ì•ˆë…•í•˜ì„¸ìš”, GOODTV ë°”ì´ë¸” ì• í”Œì…ë‹ˆë‹¤.</p><p><br></p><p>ë°”ì´ë¸” ì• í”Œì„ ì´ìš©í•´ ì£¼ì…”ì„œ ì§„ì‹¬ìœ¼ë¡œ ê°ì‚¬ë“œë¦½ë‹ˆë‹¤.</p><p><br></p><p>ë‚¨ê²¨ì£¼ì‹  ë¬¸ì˜ëŠ” í˜„ì¬ ë‹´ë‹¹ìê°€ ì§ì ‘ í™•ì¸í•˜ê³  ìˆìŠµë‹ˆë‹¤.</p><p><br></p><p>ì„±ë„ë‹˜ê»˜ ë„ì›€ì´ ë  ìˆ˜ ìˆë„ë¡ ë‚´ìš©ì„ ê¼¼ê¼¼íˆ ì‚´í´</p><p><br></p><p>ì •í™•í•˜ê³  êµ¬ì²´ì ì¸ ë‹µë³€ì„ ì¤€ë¹„í•˜ê² ìŠµë‹ˆë‹¤.</p><p><br></p><p>ë‹µë³€ì€ ìµœëŒ€ í•˜ë£¨ ì´ë‚´ì— ë“œë¦´ ì˜ˆì •ì´ì˜¤ë‹ˆ</p><p><br></p><p>ì¡°ê¸ˆë§Œ ê¸°ë‹¤ë ¤ ì£¼ì‹œë©´ ê°ì‚¬í•˜ê² ìŠµë‹ˆë‹¤.</p><p><br></p><p>í•­ìƒ ì£¼ë‹˜ ì•ˆì—ì„œ í‰ì•ˆí•˜ì„¸ìš”, ê°ì‚¬í•©ë‹ˆë‹¤.</p>"
            return default_msg
        
        # 3. ì»¨í…ìŠ¤íŠ¸ ë¶„ì„
        context_analysis = self.analyze_context_quality(similar_answers, query)
        
        # 4. ê²€ìƒ‰ ê²°ê³¼ ìƒì„¸ ë¡œê¹…
        logging.info(f"âœ… ê²€ìƒ‰ëœ ìœ ì‚¬ ë‹µë³€ ê°œìˆ˜: {len(similar_answers)}")
        print(f"âœ… ê²€ìƒ‰ëœ ìœ ì‚¬ ë‹µë³€ ê°œìˆ˜: {len(similar_answers)}")
        
        if similar_answers:
            for i, ans in enumerate(similar_answers[:3]):
                log_msg = f"ğŸ“ ë‹µë³€ #{i+1}: ì ìˆ˜={ans['score']:.3f}, ì¹´í…Œê³ ë¦¬={ans['category']}"
                logging.info(log_msg)
                print(log_msg)
        
        # 5. ë‹µë³€ì´ ì „í˜€ ì—†ì„ ë•Œë§Œ ê¸°ë³¸ ë©”ì‹œì§€ ë°˜í™˜ (ì¤‘ë³µ ì²´í¬ ì œê±°)
        # ì´ë¯¸ ìœ„ì—ì„œ ì²´í¬í–ˆìœ¼ë¯€ë¡œ ì´ ë¶€ë¶„ì€ ì‹¤í–‰ë˜ì§€ ì•Šì•„ì•¼ í•¨
        

        try:
            approach = context_analysis['recommended_approach']
            logging.info(f"=== ì ‘ê·¼ ë°©ì‹ ê²°ì • ===")
            logging.info(f"ğŸ¯ ì„ íƒëœ ì ‘ê·¼ ë°©ì‹: {approach}, ì–¸ì–´: {lang}")
            logging.info(f"ğŸ“Š ì»¨í…ìŠ¤íŠ¸ ë¶„ì„: {context_analysis}")
            
            # ì½˜ì†”ì—ë„ ì¶œë ¥
            print(f"ğŸ¯ ì„ íƒëœ ì ‘ê·¼ ë°©ì‹: {approach}, ì–¸ì–´: {lang}")
            print(f"ğŸ“Š ì»¨í…ìŠ¤íŠ¸ ë¶„ì„: {context_analysis}")
            
            base_answer = ""
            
            if approach == 'direct_use':
                logging.info("=== ì§ì ‘ ì‚¬ìš© ë°©ì‹ ì‹œì‘ ===")
                base_answer = self.get_best_fallback_answer(similar_answers[:3], lang)
                logging.info(f"ì§ì ‘ ì‚¬ìš© ê²°ê³¼ ê¸¸ì´: {len(base_answer) if base_answer else 0}")
                logging.info(f"ì§ì ‘ ì‚¬ìš© ê²°ê³¼ ë¯¸ë¦¬ë³´ê¸°: {base_answer[:100] if base_answer else 'None'}...")
                
            elif approach in ['gpt_with_strong_context', 'gpt_with_weak_context']:
                logging.info(f"=== GPT ìƒì„± ë°©ì‹ ì‹œì‘: {approach} ===")
                base_answer = self.generate_with_enhanced_gpt(query, similar_answers, context_analysis, lang)
                logging.info(f"GPT ìƒì„± ê²°ê³¼ ê¸¸ì´: {len(base_answer) if base_answer else 0}")
                
                if not base_answer or not self.is_valid_text(base_answer, lang):
                    logging.warning("GPT ìƒì„± ì‹¤íŒ¨, í´ë°± ë‹µë³€ ì‚¬ìš©")
                    base_answer = self.get_best_fallback_answer(similar_answers, lang)
                    logging.info(f"í´ë°± ë‹µë³€ ê¸¸ì´: {len(base_answer) if base_answer else 0}")
                    
            else:
                logging.info("=== í´ë°± ë°©ì‹ ì‚¬ìš© ===")
                base_answer = self.get_best_fallback_answer(similar_answers, lang)
                logging.info(f"í´ë°± ë‹µë³€ ê¸¸ì´: {len(base_answer) if base_answer else 0}")
            
            # ìµœì¢… ê²€ì¦ ì „ ìƒì„¸ ë¡œê¹…
            logging.info(f"=== ìµœì¢… ê²€ì¦ ì‹œì‘ ===")
            logging.info(f"base_answer ì¡´ì¬ ì—¬ë¶€: {base_answer is not None and base_answer != ''}")
            if base_answer:
                logging.info(f"base_answer ê¸¸ì´: {len(base_answer)}")
                logging.info(f"base_answer ë¯¸ë¦¬ë³´ê¸°: {base_answer[:200]}...")
                is_valid = self.is_valid_text(base_answer, lang)
                logging.info(f"is_valid_text ê²°ê³¼: {is_valid}")
                if not is_valid:
                    logging.warning(f"ìœ íš¨ì„± ê²€ì‚¬ ì‹¤íŒ¨ ì‚¬ìœ  ë¶„ì„ ì¤‘...")
                    # í•œêµ­ì–´ í…ìŠ¤íŠ¸ ë¶„ì„
                    if lang == 'ko':
                        korean_chars = len(re.findall(r'[ê°€-í£]', base_answer))
                        total_chars = len(re.sub(r'\s', '', base_answer))
                        korean_ratio = korean_chars / total_chars if total_chars > 0 else 0
                        logging.warning(f"í•œêµ­ì–´ ë¹„ìœ¨: {korean_ratio:.2f} (ê¸°ì¤€: 0.2 ì´ìƒ)")
                        logging.warning(f"í•œêµ­ì–´ ê¸€ì ìˆ˜: {korean_chars}, ì „ì²´ ê¸€ì ìˆ˜: {total_chars}")
            else:
                logging.error("base_answerê°€ ë¹„ì–´ìˆìŒ")
            
            # ğŸ”¥ ê¸´ê¸‰ ìˆ˜ì •: is_valid_text ê²€ì¦ ì™„ì „íˆ ìš°íšŒ
            if not base_answer:
                logging.error("=== base_answerê°€ ë¹„ì–´ìˆìŒ ===")
                logging.error(f"similar_answers ê°œìˆ˜: {len(similar_answers)}")
                if similar_answers:
                    logging.error(f"ì²« ë²ˆì§¸ ë‹µë³€ ì ìˆ˜: {similar_answers[0]['score']}")
                    logging.error(f"ì²« ë²ˆì§¸ ë‹µë³€ ë‚´ìš©: {similar_answers[0]['answer'][:100]}...")
                
                if lang == 'en':
                    return "<p>We need more detailed information to provide an accurate answer to your inquiry.</p><p><br></p><p>Please contact our customer service center for prompt assistance.</p>"
                else:
                    return "<p>ì•ˆë…•í•˜ì„¸ìš”, GOODTV ë°”ì´ë¸” ì• í”Œì…ë‹ˆë‹¤.</p><p><br></p><p>ë°”ì´ë¸” ì• í”Œì„ ì´ìš©í•´ ì£¼ì…”ì„œ ì§„ì‹¬ìœ¼ë¡œ ê°ì‚¬ë“œë¦½ë‹ˆë‹¤.</p><p><br></p><p>ë‚¨ê²¨ì£¼ì‹  ë¬¸ì˜ëŠ” í˜„ì¬ ë‹´ë‹¹ìê°€ ì§ì ‘ í™•ì¸í•˜ê³  ìˆìŠµë‹ˆë‹¤.</p><p><br></p><p>ì„±ë„ë‹˜ê»˜ ë„ì›€ì´ ë  ìˆ˜ ìˆë„ë¡ ë‚´ìš©ì„ ê¼¼ê¼¼íˆ ì‚´í´</p><p><br></p><p>ì •í™•í•˜ê³  êµ¬ì²´ì ì¸ ë‹µë³€ì„ ì¤€ë¹„í•˜ê² ìŠµë‹ˆë‹¤.</p><p><br></p><p>ë‹µë³€ì€ ìµœëŒ€ í•˜ë£¨ ì´ë‚´ì— ë“œë¦´ ì˜ˆì •ì´ì˜¤ë‹ˆ</p><p><br></p><p>ì¡°ê¸ˆë§Œ ê¸°ë‹¤ë ¤ ì£¼ì‹œë©´ ê°ì‚¬í•˜ê² ìŠµë‹ˆë‹¤.</p><p><br></p><p>í•­ìƒ ì£¼ë‹˜ ì•ˆì—ì„œ í‰ì•ˆí•˜ì„¸ìš”, ê°ì‚¬í•©ë‹ˆë‹¤.</p>"
            
            # ğŸ”¥ is_valid_text ê²€ì¦ì„ ì„ì‹œë¡œ ì£¼ì„ ì²˜ë¦¬
            # elif not self.is_valid_text(base_answer, lang):
            elif False:  # í•­ìƒ Falseê°€ ë˜ì–´ ì´ ë¸”ë¡ì€ ì‹¤í–‰ë˜ì§€ ì•ŠìŒ
                logging.warning(f"ìœ íš¨ì„± ê²€ì‚¬ ì‹¤íŒ¨í–ˆì§€ë§Œ ë‹µë³€ ì¡´ì¬í•¨ - ê°•ì œ ì§„í–‰")
                # ì´ ë¸”ë¡ì€ ì‹¤í–‰ë˜ì§€ ì•ŠìŒ
            
            # ğŸ”¥ ì„±ê³µ ë¡œê·¸ ì¶”ê°€
            logging.info("ğŸ‰ ìœ íš¨ì„± ê²€ì‚¬ ìš°íšŒ ì„±ê³µ - ë‹µë³€ í¬ë§·íŒ… ì‹œì‘")
            print("ğŸ‰ ìœ íš¨ì„± ê²€ì‚¬ ìš°íšŒ ì„±ê³µ - ë‹µë³€ í¬ë§·íŒ… ì‹œì‘")
            
            # ğŸ”¥ ê°•í™”ëœ ë‹µë³€ ì™„ì„±ë„ ê²€ì¦ ë° ì¬ìƒì„± ë¡œì§
            base_completeness = self.check_answer_completeness(base_answer, query, lang)
            logging.info(f"ìµœì¢… ë‹µë³€ ì™„ì„±ë„ ì ìˆ˜: {base_completeness:.2f}")
            
            # ğŸ”¥ ë¹ˆ ì•½ì† íŒ¨í„´ íŠ¹ë³„ ê²€ì‚¬
            empty_promise_score = self.detect_empty_promises(base_answer, lang)
            logging.info(f"ë¹ˆ ì•½ì† íŒ¨í„´ ê²€ì‚¬ ì ìˆ˜: {empty_promise_score:.2f}")
            
            # ğŸ”¥ í• ë£¨ì‹œë„¤ì´ì…˜ ë° ì¼ê´€ì„± ìµœì¢… ê²€ì¦ (ìƒˆë¡œ ì¶”ê°€)
            final_hallucination_check = self.detect_hallucination_and_inconsistency(base_answer, query, lang)
            final_hallucination_score = final_hallucination_check['overall_score']
            final_detected_issues = final_hallucination_check['detected_issues']
            
            logging.info(f"ìµœì¢… í• ë£¨ì‹œë„¤ì´ì…˜ ê²€ì¦ ì ìˆ˜: {final_hallucination_score:.2f}")
            if final_detected_issues:
                logging.error(f"ìµœì¢… ë‹µë³€ì—ì„œ ê°ì§€ëœ ë¬¸ì œë“¤: {final_detected_issues}")
            
            # ğŸš¨ í• ë£¨ì‹œë„¤ì´ì…˜ì´ ì¹˜ëª…ì ì´ë©´ ì¦‰ì‹œ í´ë°±ìœ¼ë¡œ ë³€ê²½
            if final_hallucination_score < 0.3:
                logging.error("ğŸš¨ ìµœì¢… ë‹µë³€ì—ì„œ ì¹˜ëª…ì  í• ë£¨ì‹œë„¤ì´ì…˜ ê°ì§€! í´ë°± ë‹µë³€ìœ¼ë¡œ ê°•ì œ ë³€ê²½")
                approach = 'fallback'
                base_answer = self.get_best_fallback_answer(similar_answers, lang)
                
                # í´ë°± ë‹µë³€ë„ ê²€ì¦
                if base_answer:
                    fallback_hallucination = self.detect_hallucination_and_inconsistency(base_answer, query, lang)
                    logging.info(f"í´ë°± ë‹µë³€ í• ë£¨ì‹œë„¤ì´ì…˜ ì ìˆ˜: {fallback_hallucination['overall_score']:.2f}")
            
            # ì¬ìƒì„± ì¡°ê±´ ê²€ì‚¬ (í• ë£¨ì‹œë„¤ì´ì…˜ ì ìˆ˜ ì¶”ê°€)
            needs_regeneration = (
                base_completeness < 0.6 or 
                empty_promise_score < 0.3 or
                final_hallucination_score < 0.5  # í• ë£¨ì‹œë„¤ì´ì…˜ ì ìˆ˜ê°€ ë‚®ìœ¼ë©´ ì¬ìƒì„±
            )
            
            if needs_regeneration and approach in ['gpt_with_strong_context', 'gpt_with_weak_context']:
                logging.warning(f"ë‹µë³€ í’ˆì§ˆ ë¶€ì¡± - ì™„ì„±ë„: {base_completeness:.2f}, ë¹ˆì•½ì†: {empty_promise_score:.2f}")
                
                # ğŸ”¥ ë” ê°•í•œ ì¬ìƒì„± ì‹œë„ (ìµœëŒ€ 2íšŒ)
                for attempt in range(2):
                    logging.info(f"ì¬ìƒì„± ì‹œë„ #{attempt+1}")
                    
                    retry_analysis = context_analysis.copy()
                    retry_analysis['recommended_approach'] = 'gpt_with_strong_context'
                    
                    retry_answer = self.generate_with_enhanced_gpt(query, similar_answers, retry_analysis, lang)
                    if retry_answer:
                        retry_completeness = self.check_answer_completeness(retry_answer, query, lang)
                        retry_empty_promise = self.detect_empty_promises(retry_answer, lang)
                        
                        # ğŸ”¥ ì¬ìƒì„± ë‹µë³€ë„ í• ë£¨ì‹œë„¤ì´ì…˜ ê²€ì¦ (ìƒˆë¡œ ì¶”ê°€)
                        retry_hallucination_check = self.detect_hallucination_and_inconsistency(retry_answer, query, lang)
                        retry_hallucination_score = retry_hallucination_check['overall_score']
                        retry_detected_issues = retry_hallucination_check['detected_issues']
                        
                        logging.info(f"ì¬ìƒì„± #{attempt+1} - ì™„ì„±ë„: {retry_completeness:.2f}, ë¹ˆì•½ì†: {retry_empty_promise:.2f}, í• ë£¨ì‹œë„¤ì´ì…˜: {retry_hallucination_score:.2f}")
                        
                        if retry_detected_issues:
                            logging.warning(f"ì¬ìƒì„± #{attempt+1} ê°ì§€ëœ ë¬¸ì œ: {retry_detected_issues}")
                        
                        # ğŸš¨ ì¬ìƒì„± ë‹µë³€ì— ì¹˜ëª…ì  í• ë£¨ì‹œë„¤ì´ì…˜ì´ ìˆìœ¼ë©´ ì‚¬ìš©í•˜ì§€ ì•ŠìŒ
                        if retry_hallucination_score < 0.3:
                            logging.error(f"ì¬ìƒì„± #{attempt+1}ì— ì¹˜ëª…ì  í• ë£¨ì‹œë„¤ì´ì…˜ ê°ì§€ - ì‚¬ìš© ì•ˆí•¨")
                            continue
                        
                        # ì¬ìƒì„± ë‹µë³€ì´ ë” ë‚˜ì€ì§€ í™•ì¸ (í• ë£¨ì‹œë„¤ì´ì…˜ ì ìˆ˜ ì¶”ê°€)
                        is_better = (
                            retry_completeness > base_completeness and 
                            retry_empty_promise > empty_promise_score and
                            retry_hallucination_score > final_hallucination_score
                        )
                        
                        if is_better:
                            logging.info(f"ì¬ìƒì„± ë‹µë³€ #{attempt+1}ì´ ë” ìš°ìˆ˜í•¨ - ì‚¬ìš©")
                            base_answer = retry_answer
                            base_completeness = retry_completeness
                            empty_promise_score = retry_empty_promise
                            final_hallucination_score = retry_hallucination_score
                            break
                        else:
                            logging.info(f"ì¬ìƒì„± ë‹µë³€ #{attempt+1}ì´ ê°œì„ ë˜ì§€ ì•ŠìŒ")
                
                # ì—¬ì „íˆ ë‚®ìœ¼ë©´ í´ë°± ë‹µë³€ìœ¼ë¡œ ê°•ì œ ë³€ê²½ (í• ë£¨ì‹œë„¤ì´ì…˜ ê²€ì¦ í¬í•¨)
                if base_completeness < 0.5 or empty_promise_score < 0.3 or final_hallucination_score < 0.5:
                    logging.warning("ëª¨ë“  ì¬ìƒì„± ì‹¤íŒ¨, í´ë°± ë‹µë³€ìœ¼ë¡œ ê°•ì œ ë³€ê²½")
                    
                    # ìƒìœ„ 3ê°œ ë‹µë³€ ì¤‘ ê°€ì¥ ì¢‹ì€ ê²ƒ ì„ íƒ (í• ë£¨ì‹œë„¤ì´ì…˜ ê²€ì¦ í¬í•¨)
                    best_fallback = None
                    best_fallback_score = 0
                    
                    for i, candidate in enumerate(similar_answers[:3]):
                        candidate_text = self.preprocess_text(candidate['answer'])
                        if lang == 'en' and candidate.get('lang', 'ko') == 'ko':
                            candidate_text = self.translate_text(candidate_text, 'ko', 'en')
                        
                        candidate_completeness = self.check_answer_completeness(candidate_text, query, lang)
                        candidate_empty_promise = self.detect_empty_promises(candidate_text, lang)
                        
                        # ğŸ”¥ í´ë°± í›„ë³´ë„ í• ë£¨ì‹œë„¤ì´ì…˜ ê²€ì¦ (ìƒˆë¡œ ì¶”ê°€)
                        candidate_hallucination_check = self.detect_hallucination_and_inconsistency(candidate_text, query, lang)
                        candidate_hallucination_score = candidate_hallucination_check['overall_score']
                        
                        # ğŸš¨ í• ë£¨ì‹œë„¤ì´ì…˜ì´ ì‹¬ê°í•˜ë©´ í›„ë³´ì—ì„œ ì œì™¸
                        if candidate_hallucination_score < 0.3:
                            logging.warning(f"í´ë°± í›„ë³´ #{i+1} í• ë£¨ì‹œë„¤ì´ì…˜ìœ¼ë¡œ ì œì™¸")
                            continue
                        
                        # ì¢…í•© ì ìˆ˜ ê³„ì‚° (í• ë£¨ì‹œë„¤ì´ì…˜ ì ìˆ˜ í¬í•¨)
                        combined_score = (
                            candidate_completeness * 0.4 + 
                            candidate_empty_promise * 0.3 + 
                            candidate_hallucination_score * 0.3
                        )
                        
                        logging.info(f"í´ë°± í›„ë³´ #{i+1} ì¢…í•©ì ìˆ˜: {combined_score:.2f} (ì™„ì„±ë„={candidate_completeness:.2f}, ë¹ˆì•½ì†={candidate_empty_promise:.2f}, í• ë£¨ì‹œë„¤ì´ì…˜={candidate_hallucination_score:.2f})")
                        
                        if combined_score > best_fallback_score:
                            best_fallback = candidate_text
                            best_fallback_score = combined_score
                    
                    if best_fallback and best_fallback_score > 0.4:
                        base_answer = best_fallback
                        approach = 'fallback'
                        logging.info(f"ìµœê³  í´ë°± ë‹µë³€ ì„ íƒ (ì ìˆ˜: {best_fallback_score:.2f})")
                    else:
                        logging.error("ëª¨ë“  ë‹µë³€ì´ í’ˆì§ˆ ê¸°ì¤€ ë¯¸ë‹¬, ì›ë³¸ ìœ ì§€")
            
            # ì–¸ì–´ë³„ í¬ë§·íŒ…
            if lang == 'en':
                # ì˜ì–´ ë‹µë³€ í¬ë§·íŒ…
                base_answer = self.remove_old_app_name(base_answer)
                
                # ê¸°ì¡´ ì¸ì‚¬ë§/ëë§ºìŒë§ ì œê±°
                base_answer = re.sub(r'^Hello[^.]*\.\s*', '', base_answer, flags=re.IGNORECASE)
                base_answer = re.sub(r'^This is GOODTV Bible App[^.]*\.\s*', '', base_answer, flags=re.IGNORECASE)
                base_answer = re.sub(r'\s*Thank you[^.]*\.?\s*$', '', base_answer, flags=re.IGNORECASE)
                base_answer = re.sub(r'\s*Best regards[^.]*\.?\s*$', '', base_answer, flags=re.IGNORECASE)
                base_answer = re.sub(r'\s*God bless[^.]*\.?\s*$', '', base_answer, flags=re.IGNORECASE)
                
                formatted_body = self.format_answer_with_html_paragraphs(base_answer.strip(), 'en')
                
                # ì˜ì–´ ê³ ì • ì¸ì‚¬ë§ê³¼ ëë§ºìŒë§
                final_answer = "<p>Hello, this is GOODTV Bible Apple App customer service team.</p><p><br></p><p>Thank you very much for using our app and for taking the time to contact us.</p><p><br></p>"
                final_answer += formatted_body
                final_answer += "<p><br></p><p>Thank you once again for sharing your thoughts with us!</p><p><br></p><p>May God's peace and grace always be with you.</p>"
                
            else:  # í•œêµ­ì–´
                # í•œêµ­ì–´ ë‹µë³€ ìµœì¢… í¬ë§·íŒ… (Quill ì—ë””í„°ìš© HTML í˜•ì‹ ìœ ì§€)
                # ì•± ì´ë¦„ ì •ë¦¬ ë° ê³ ê°ë‹˜ â†’ ì„±ë„ë‹˜ ë³€ê²½
                base_answer = self.remove_old_app_name(base_answer)
                base_answer = re.sub(r'ê³ ê°ë‹˜', 'ì„±ë„ë‹˜', base_answer)
                
                # ê¸°ì¡´ ì¸ì‚¬ë§/ëë§ºìŒë§ ì œê±° (ì¼ë°˜ í…ìŠ¤íŠ¸ì—ì„œ)
                # ì¸ì‚¬ë§ ì œê±°
                base_answer = re.sub(r'^ì•ˆë…•í•˜ì„¸ìš”[^.]*\.\s*', '', base_answer, flags=re.IGNORECASE)
                base_answer = re.sub(r'^GOODTV\s+ë°”ì´ë¸”\s*ì• í”Œ[^.]*\.\s*', '', base_answer, flags=re.IGNORECASE)
                base_answer = re.sub(r'^ë°”ì´ë¸”\s*ì• í”Œ[^.]*\.\s*', '', base_answer, flags=re.IGNORECASE)
                base_answer = re.sub(r'ê³ ê°ì„¼í„°[^.]*\.\s*', '', base_answer, flags=re.IGNORECASE)
                
                # ëë§ºìŒë§ ì œê±° (ë” ê°•í™”ëœ íŒ¨í„´) - 'í•­ìƒ' ì¤‘ë³µ ì œê±° í¬í•¨
                base_answer = re.sub(r'\s*ê°ì‚¬í•©ë‹ˆë‹¤[^.]*\.?\s*$', '', base_answer, flags=re.IGNORECASE)
                base_answer = re.sub(r'\s*í‰ì•ˆí•˜ì„¸ìš”[^.]*\.?\s*$', '', base_answer, flags=re.IGNORECASE)
                base_answer = re.sub(r'\s*ì£¼ë‹˜\s*ì•ˆì—ì„œ[^.]*\.?\s*$', '', base_answer, flags=re.IGNORECASE)
                base_answer = re.sub(r'\s*í•¨ê»˜\s*ê¸°ë„í•˜ë©°[^.]*\.?\s*$', '', base_answer, flags=re.IGNORECASE)
                base_answer = re.sub(r'\s*í•­ìƒ[^.]*ë°”ì´ë¸”\s*ì• í”Œ[^.]*\.?\s*$', '', base_answer, flags=re.IGNORECASE)
                
                # ì¶”ê°€ ëë§ºìŒë§ íŒ¨í„´ë“¤ (ë” í¬ê´„ì ìœ¼ë¡œ)
                base_answer = re.sub(r'\s*í•­ìƒ\s*ì£¼ë‹˜\s*ì•ˆì—ì„œ[^.]*\.?\s*$', '', base_answer, flags=re.IGNORECASE)
                base_answer = re.sub(r'\s*ì£¼ë‹˜\s*ì•ˆì—ì„œ\s*í‰ì•ˆí•˜ì„¸ìš”[^.]*\.?\s*$', '', base_answer, flags=re.IGNORECASE)
                base_answer = re.sub(r'\s*í‰ì•ˆí•˜ì„¸ìš”[^.]*\.?\s*$', '', base_answer, flags=re.IGNORECASE)
                
                # ë¬¸ì¥ ëì˜ ëë§ºìŒë§ë“¤ë„ ì œê±°
                base_answer = re.sub(r'[,.!?]\s*í•­ìƒ\s*ì£¼ë‹˜\s*ì•ˆì—ì„œ[^.]*\.?\s*$', '', base_answer, flags=re.IGNORECASE)
                base_answer = re.sub(r'[,.!?]\s*ê°ì‚¬í•©ë‹ˆë‹¤[^.]*\.?\s*$', '', base_answer, flags=re.IGNORECASE)
                base_answer = re.sub(r'[,.!?]\s*í‰ì•ˆí•˜ì„¸ìš”[^.]*\.?\s*$', '', base_answer, flags=re.IGNORECASE)
                
                # ğŸ”¥ êµ¬ ì•± ì´ë¦„ì„ ë°”ì´ë¸” ì• í”Œë¡œ ì™„ì „ êµì²´ (ì¤‘ë³µ ë°©ì§€)
                # ì¤‘ë³µ ë°©ì§€ë¥¼ ìœ„í•´ ìˆœì„œë¥¼ ì¡°ì •: ì „ì²´ íŒ¨í„´ë¶€í„° ì²˜ë¦¬
                base_answer = re.sub(r'ë°”ì´ë¸”\s*ì• í”Œ\s*\(êµ¬\)\s*ë‹¤ë²ˆì—­\s*ì„±ê²½\s*ì°¬ì†¡', 'ë°”ì´ë¸” ì• í”Œ', base_answer, flags=re.IGNORECASE)
                base_answer = re.sub(r'ë°”ì´ë¸”\s*ì• í”Œ\s*\(êµ¬\)\s*ë‹¤ë²ˆì—­ì„±ê²½ì°¬ì†¡', 'ë°”ì´ë¸” ì• í”Œ', base_answer, flags=re.IGNORECASE)
                base_answer = re.sub(r'\(êµ¬\)\s*ë‹¤ë²ˆì—­\s*ì„±ê²½\s*ì°¬ì†¡', 'ë°”ì´ë¸” ì• í”Œ', base_answer, flags=re.IGNORECASE)
                base_answer = re.sub(r'\(êµ¬\)\s*ë‹¤ë²ˆì—­ì„±ê²½ì°¬ì†¡', 'ë°”ì´ë¸” ì• í”Œ', base_answer, flags=re.IGNORECASE)
                base_answer = re.sub(r'ë‹¤ë²ˆì—­\s*ì„±ê²½\s*ì°¬ì†¡', 'ë°”ì´ë¸” ì• í”Œ', base_answer, flags=re.IGNORECASE)
                base_answer = re.sub(r'ë‹¤ë²ˆì—­ì„±ê²½ì°¬ì†¡', 'ë°”ì´ë¸” ì• í”Œ', base_answer, flags=re.IGNORECASE)
                
                # ğŸ”¥ ì™„ì „íˆ ê°•í™”ëœ ì¤‘ë³µ ëë§ºìŒë§ ì œê±° ì‹œìŠ¤í…œ
                # 1ë‹¨ê³„: ëª¨ë“  í˜•íƒœì˜ "í•­ìƒ ì„±ë„ë‹˜ê»˜..." íŒ¨í„´ ì œê±°
                base_answer = re.sub(r'í•­ìƒ\s*ì„±ë„ë‹˜ë“¤?ê»˜\s*ì¢‹ì€\s*(ì„œë¹„ìŠ¤|ì„±ê²½ì•±)ì„?\s*ì œê³µí•˜ê¸°\s*ìœ„í•´\s*ë…¸ë ¥í•˜ëŠ”\s*ë°”ì´ë¸”\s*ì• í”Œì´\s*ë˜ê² ìŠµë‹ˆë‹¤\.?\s*', 
                                   '', base_answer, flags=re.IGNORECASE)
                
                # 2ë‹¨ê³„: ê°ì‚¬í•©ë‹ˆë‹¤ íŒ¨í„´ ì™„ì „ ì œê±°
                base_answer = re.sub(r'ê°ì‚¬í•©ë‹ˆë‹¤\.?\s*(ì£¼ë‹˜\s*ì•ˆì—ì„œ\s*í‰ì•ˆí•˜ì„¸ìš”\.?)?\s*', 
                                   '', base_answer, flags=re.IGNORECASE)
                
                # 3ë‹¨ê³„: ë¶ˆì™„ì „í•œ ë¬¸ì¥ë“¤ ì œê±°
                base_answer = re.sub(r'ì˜¤ëŠ˜ë„\s*$', '', base_answer, flags=re.IGNORECASE)
                base_answer = re.sub(r'ì˜¤ëŠ˜ë„\s*\n', '\n', base_answer, flags=re.IGNORECASE)
                base_answer = re.sub(r'í•­ìƒ\s*$', '', base_answer, flags=re.IGNORECASE)
                
                # ğŸ”¥ 'í•­ìƒ' ë‹¨ë…ìœ¼ë¡œ ë‚¨ì€ ê²½ìš° ì œê±° (ì¤‘ë³µ ë¬¸ì œ í•´ê²°)
                base_answer = re.sub(r'\s*í•­ìƒ\s*$', '', base_answer, flags=re.IGNORECASE)
                base_answer = re.sub(r'\n\s*í•­ìƒ\s*\n', '\n', base_answer, flags=re.IGNORECASE)
                base_answer = re.sub(r'<p>\s*í•­ìƒ\s*</p>', '', base_answer, flags=re.IGNORECASE)
                base_answer = re.sub(r'<p><br></p>\s*<p>\s*í•­ìƒ\s*</p>', '', base_answer, flags=re.IGNORECASE)
                
                # ë³¸ë¬¸ì„ HTML ë‹¨ë½ í˜•ì‹ìœ¼ë¡œ í¬ë§·íŒ…
                formatted_body = self.format_answer_with_html_paragraphs(base_answer.strip(), 'ko')
                
                # í•œêµ­ì–´ ê³ ì • ì¸ì‚¬ë§ (HTML í˜•ì‹ìœ¼ë¡œ)
                final_answer = "<p>ì•ˆë…•í•˜ì„¸ìš”. GOODTV ë°”ì´ë¸” ì• í”Œì…ë‹ˆë‹¤.</p><p><br></p><p>ë°”ì´ë¸” ì• í”Œì„ ì´ìš©í•´ì£¼ì…”ì„œ ê°ì‚¬ë“œë¦½ë‹ˆë‹¤.</p><p><br></p>"
                
                # í¬ë§·íŒ…ëœ ë³¸ë¬¸ ì¶”ê°€ ì „ ìµœì¢… ì •ë¦¬
                # ğŸ”¥ HTML í¬ë§·íŒ… í›„ ì™„ì „í•œ ì •ë¦¬ ì‘ì—…
                # ì¤‘ë³µëœ ëë§ºìŒë§ HTML íƒœê·¸ ì œê±°
                formatted_body = re.sub(r'<p>\s*í•­ìƒ\s*ì„±ë„ë‹˜ë“¤?ê»˜\s*ì¢‹ì€\s*(ì„œë¹„ìŠ¤|ì„±ê²½ì•±)ì„?\s*ì œê³µí•˜ê¸°\s*ìœ„í•´\s*ë…¸ë ¥í•˜ëŠ”\s*ë°”ì´ë¸”\s*ì• í”Œì´\s*ë˜ê² ìŠµë‹ˆë‹¤\.?\s*</p>', '', formatted_body, flags=re.IGNORECASE)
                formatted_body = re.sub(r'<p>\s*ê°ì‚¬í•©ë‹ˆë‹¤\.?\s*(ì£¼ë‹˜\s*ì•ˆì—ì„œ\s*í‰ì•ˆí•˜ì„¸ìš”\.?)?\s*</p>', '', formatted_body, flags=re.IGNORECASE)
                
                # ë¶ˆì™„ì „í•œ ë¬¸ì¥ë“¤ ì œê±°
                formatted_body = re.sub(r'<p>\s*í•­ìƒ\s*</p>', '', formatted_body, flags=re.IGNORECASE)
                formatted_body = re.sub(r'<p>\s*ì˜¤ëŠ˜ë„\s*</p>', '', formatted_body, flags=re.IGNORECASE)
                formatted_body = re.sub(r'<p><br></p>\s*<p>\s*(í•­ìƒ|ì˜¤ëŠ˜ë„)\s*</p>', '', formatted_body, flags=re.IGNORECASE)
                formatted_body = re.sub(r'<p>\s*(í•­ìƒ|ì˜¤ëŠ˜ë„)\s*<br></p>', '', formatted_body, flags=re.IGNORECASE)
                
                # ì—°ì†ëœ ë¹ˆ íƒœê·¸ë“¤ ì •ë¦¬
                formatted_body = re.sub(r'(<p><br></p>\s*){3,}', '<p><br></p><p><br></p>', formatted_body)
                formatted_body = re.sub(r'(<p><br></p>\s*)+$', '', formatted_body)  # ëì˜ ë¹ˆ íƒœê·¸ë“¤ ì œê±°
                
                final_answer += formatted_body
                
                # ê³ ì •ëœ ëë§ºìŒë§ (HTML í˜•ì‹ìœ¼ë¡œ)
                final_answer += "<p><br></p><p>í•­ìƒ ì„±ë„ë‹˜ê»˜ ì¢‹ì€ ì„±ê²½ì•±ì„ ì œê³µí•˜ê¸° ìœ„í•´ ë…¸ë ¥í•˜ëŠ” ë°”ì´ë¸” ì• í”Œì´ ë˜ê² ìŠµë‹ˆë‹¤.</p><p><br></p><p>ê°ì‚¬í•©ë‹ˆë‹¤. ì£¼ë‹˜ ì•ˆì—ì„œ í‰ì•ˆí•˜ì„¸ìš”.</p>"
            
            logging.info(f"ìµœì¢… ë‹µë³€ ìƒì„± ì™„ë£Œ: {len(final_answer)}ì, ì ‘ê·¼ë°©ì‹: {approach}, ì–¸ì–´: {lang}")
            return final_answer
            
        except Exception as e:
            logging.error(f"ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜: {e}")
            if lang == 'en':
                return "<p>Sorry, we cannot generate an answer at this moment.</p><p><br></p><p>Please contact our customer service center.</p>"
            else:
                return "<p>ì•ˆë…•í•˜ì„¸ìš”, GOODTV ë°”ì´ë¸” ì• í”Œì…ë‹ˆë‹¤.</p><p><br></p><p>ë°”ì´ë¸” ì• í”Œì„ ì´ìš©í•´ ì£¼ì…”ì„œ ì§„ì‹¬ìœ¼ë¡œ ê°ì‚¬ë“œë¦½ë‹ˆë‹¤.</p><p><br></p><p>ë‚¨ê²¨ì£¼ì‹  ë¬¸ì˜ëŠ” í˜„ì¬ ë‹´ë‹¹ìê°€ ì§ì ‘ í™•ì¸í•˜ê³  ìˆìŠµë‹ˆë‹¤.</p><p><br></p><p>ì„±ë„ë‹˜ê»˜ ë„ì›€ì´ ë  ìˆ˜ ìˆë„ë¡ ë‚´ìš©ì„ ê¼¼ê¼¼íˆ ì‚´í´</p><p><br></p><p>ì •í™•í•˜ê³  êµ¬ì²´ì ì¸ ë‹µë³€ì„ ì¤€ë¹„í•˜ê² ìŠµë‹ˆë‹¤.</p><p><br></p><p>ë‹µë³€ì€ ìµœëŒ€ í•˜ë£¨ ì´ë‚´ì— ë“œë¦´ ì˜ˆì •ì´ì˜¤ë‹ˆ</p><p><br></p><p>ì¡°ê¸ˆë§Œ ê¸°ë‹¤ë ¤ ì£¼ì‹œë©´ ê°ì‚¬í•˜ê² ìŠµë‹ˆë‹¤.</p><p><br></p><p>í•­ìƒ ì£¼ë‹˜ ì•ˆì—ì„œ í‰ì•ˆí•˜ì„¸ìš”, ê°ì‚¬í•©ë‹ˆë‹¤.</p>"

    # â˜† ë©”ëª¨ë¦¬ ìµœì í™”ëœ ë©”ì¸ ì²˜ë¦¬ ë©”ì„œë“œ
    def process(self, seq: int, question: str, lang: str) -> dict:
        try:
            with memory_cleanup():
                # 1. ì „ì²˜ë¦¬
                processed_question = self.preprocess_text(question)

                # 2. ì˜¤íƒ€ ìˆ˜ì • ì¶”ê°€ (Pinecone ì €ì¥ ì‹œì™€ ë™ì¼í•˜ê²Œ!)
                if lang == 'ko' or lang == 'auto':
                    processed_question = self.fix_korean_typos_with_ai(processed_question)
                    logging.info(f"ì˜¤íƒ€ ìˆ˜ì • ì ìš©: {question[:50]} â†’ {processed_question[:50]}")
                
                if not processed_question:
                    return {"success": False, "error": "ì§ˆë¬¸ì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤."}
                
                # ì–¸ì–´ ìë™ ê°ì§€
                if not lang or lang == 'auto':
                    lang = self.detect_language(processed_question)
                    logging.info(f"ìë™ ê°ì§€ëœ ì–¸ì–´: {lang}")
                
                logging.info(f"ì²˜ë¦¬ ì‹œì‘ - SEQ: {seq}, ì–¸ì–´: {lang}, ì§ˆë¬¸: {processed_question[:50]}...")
                
                # 3. ìœ ì‚¬ ë‹µë³€ ê²€ìƒ‰ (ì´ì œ ì˜¤íƒ€ ìˆ˜ì •ëœ ì§ˆë¬¸ìœ¼ë¡œ ê²€ìƒ‰)
                similar_answers = self.search_similar_answers(processed_question, lang=lang)
                
                # AI ë‹µë³€ ìƒì„± (ì–¸ì–´ íŒŒë¼ë¯¸í„° ì „ë‹¬)
                ai_answer = self.generate_ai_answer(processed_question, similar_answers, lang)
                
                # íŠ¹ìˆ˜ë¬¸ì ì •ë¦¬
                ai_answer = ai_answer.replace('"', '"').replace('"', '"')
                ai_answer = ai_answer.replace(''', "'").replace(''', "'")
                
                result = {
                    "success": True,
                    "answer": ai_answer,
                    "similar_count": len(similar_answers),
                    "embedding_model": "text-embedding-3-small",
                    "generation_model": "gpt-3.5-turbo",
                    "detected_language": lang
                }
                
                logging.info(f"ì²˜ë¦¬ ì™„ë£Œ - SEQ: {seq}, ì–¸ì–´: {lang}, HTML ë‹µë³€ ìƒì„±ë¨")
                return result
                
        except Exception as e:
            logging.error(f"ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ - SEQ: {seq}, ì˜¤ë¥˜: {str(e)}")
            return {"success": False, "error": str(e)}

    # â˜† ë‹¨ìˆœí™”ëœ ì»¨í…ìŠ¤íŠ¸ í’ˆì§ˆ ë¶„ì„ ë©”ì„œë“œ (ê·œì¹™ ê¸°ë°˜)
    def analyze_context_quality_simple(self, similar_answers: list, query: str) -> dict:
        """ìœ ì‚¬ë„ ì ìˆ˜ ê¸°ë°˜ì˜ ë‹¨ìˆœí•˜ê³  ëª…í™•í•œ í’ˆì§ˆ ë¶„ì„"""
        
        if not similar_answers:
            return {
                'has_good_context': False,
                'best_score': 0.0,
                'recommended_approach': 'fallback',
                'quality_level': 'none',
                'top_scores': []
            }
        
        # ìƒìœ„ 5ê°œ ë‹µë³€ì˜ ì ìˆ˜ë§Œ í™•ì¸
        top_5_scores = [ans['score'] for ans in similar_answers[:5]]
        best_score = top_5_scores[0] if top_5_scores else 0.0
        
        # ì ìˆ˜ ë¶„í¬ ë¶„ì„
        high_quality_count = len([s for s in top_5_scores if s >= 0.8])
        medium_quality_count = len([s for s in top_5_scores if 0.6 <= s < 0.8])
        
        # ëª…í™•í•œ ê·œì¹™ ê¸°ë°˜ ì ‘ê·¼ ë°©ì‹ ê²°ì •
        if best_score >= 0.95:
            approach = 'direct_use'
            quality_level = 'excellent'
        elif best_score >= 0.85 and high_quality_count >= 2:
            approach = 'direct_use'
            quality_level = 'very_high'
        elif best_score >= 0.75:
            approach = 'gpt_with_strong_context'
            quality_level = 'high'
        elif best_score >= 0.6 and (high_quality_count + medium_quality_count) >= 2:
            approach = 'gpt_with_strong_context'
            quality_level = 'medium'
        elif best_score >= 0.45:
            approach = 'gpt_with_weak_context'
            quality_level = 'low'
        else:
            approach = 'fallback'
            quality_level = 'very_low'
        
        return {
            'has_good_context': quality_level in ['excellent', 'very_high', 'high', 'medium'],
            'best_score': best_score,
            'high_quality_count': high_quality_count,
            'medium_quality_count': medium_quality_count,
            'recommended_approach': approach,
            'quality_level': quality_level,
            'top_scores': top_5_scores,
            'context_summary': f"í’ˆì§ˆ: {quality_level}, ìµœê³ ì ìˆ˜: {best_score:.3f}, ê³ í’ˆì§ˆ: {high_quality_count}ê°œ"
        }

    # â˜† ë‹¨ìˆœí™”ëœ ê²€ìƒ‰ ê²°ê³¼ í•„í„°ë§ (ì„ê³„ê°’ ê¸°ë°˜)
    def search_similar_answers(self, query: str, top_k: int = 8, lang: str = 'ko') -> list:
        """ë‹¨ìˆœí™”ëœ ìœ ì‚¬ ë‹µë³€ ê²€ìƒ‰ - ëª…í™•í•œ ì ìˆ˜ ê¸°ë°˜ í•„í„°ë§"""
        try:
            with memory_cleanup():
                logging.info(f"=== ë‹¨ìˆœí™”ëœ ê²€ìƒ‰ ì‹œì‘ ===")
                logging.info(f"ê²€ìƒ‰ ì§ˆë¬¸: {query[:100]}")
                
                # ì˜¤íƒ€ ìˆ˜ì • (í•œêµ­ì–´ë§Œ)
                if lang == 'ko':
                    corrected_query = self.fix_korean_typos_with_ai(query)
                    query_to_embed = corrected_query
                else:
                    query_to_embed = query
                
                # ì„ë² ë”© ìƒì„±
                query_vector = self.create_embedding(query_to_embed)
                if query_vector is None:
                    logging.error("ì„ë² ë”© ìƒì„± ì‹¤íŒ¨")
                    return []
                
                # Pinecone ê²€ìƒ‰ (ë” ë§ì´ ê²€ìƒ‰í•´ì„œ ì¢‹ì€ ê²°ê³¼ í™•ë³´)
                results = index.query(
                    vector=query_vector,
                    top_k=top_k * 3,  # 3ë°° ë” ê²€ìƒ‰
                    include_metadata=True
                )
                
                # ì˜ì–´ ì§ˆë¬¸ì¸ ê²½ìš° í•œêµ­ì–´ ë²ˆì—­ìœ¼ë¡œ ì¶”ê°€ ê²€ìƒ‰
                if lang == 'en':
                    korean_query = self.translate_text(query_to_embed, 'en', 'ko')
                    korean_vector = self.create_embedding(korean_query)
                    if korean_vector:
                        korean_results = index.query(
                            vector=korean_vector,
                            top_k=top_k,
                            include_metadata=True
                        )
                        # ê²°ê³¼ ë³‘í•© ë° ì¤‘ë³µ ì œê±°
                        seen_ids = set()
                        merged_matches = []
                        for match in results['matches'] + korean_results['matches']:
                            if match['id'] not in seen_ids:
                                seen_ids.add(match['id'])
                                merged_matches.append(match)
                        results['matches'] = sorted(merged_matches, key=lambda x: x['score'], reverse=True)
                
                # ë‹¨ìˆœí•œ ì ìˆ˜ ê¸°ë°˜ í•„í„°ë§
                filtered_results = []
                for i, match in enumerate(results['matches'][:top_k*2]):  # ìƒìœ„ 2ë°°ë§Œ ê²€í† 
                    score = match['score']
                    question = match['metadata'].get('question', '')
                    answer = match['metadata'].get('answer', '')
                    category = match['metadata'].get('category', 'ì¼ë°˜')
                    
                    # ëª…í™•í•œ í¬í•¨ ê¸°ì¤€
                    should_include = False
                    
                    if score >= 0.4:  # ê¸°ë³¸ ì„ê³„ê°’
                        should_include = True
                    elif i < 5:  # ìƒìœ„ 5ê°œëŠ” ì ìˆ˜ê°€ ë‚®ì•„ë„ í¬í•¨
                        should_include = True
                    elif score >= 0.3 and len(filtered_results) < 3:  # ìµœì†Œ 3ê°œ ë³´ì¥
                        should_include = True
                    
                    if should_include and len(filtered_results) < top_k:
                        # ê¸°ë³¸ì ì¸ í…ìŠ¤íŠ¸ í’ˆì§ˆ ê²€ì¦ë§Œ
                        if len(answer.strip()) >= 10:  # ìµœì†Œ ê¸¸ì´ë§Œ í™•ì¸
                            filtered_results.append({
                                'score': score,
                                'question': question,
                                'answer': answer,
                                'category': category,
                                'rank': i + 1,
                                'lang': 'ko'
                            })
                            
                            logging.info(f"í¬í•¨: #{i+1} ì ìˆ˜={score:.3f} ì¹´í…Œê³ ë¦¬={category}")
                
                # ë©”ëª¨ë¦¬ ì •ë¦¬
                del results, query_vector
                
                logging.info(f"ê²€ìƒ‰ ì™„ë£Œ: {len(filtered_results)}ê°œ ë‹µë³€ (ì–¸ì–´: {lang})")
                return filtered_results
            
        except Exception as e:
            logging.error(f"ê²€ìƒ‰ ì‹¤íŒ¨: {str(e)}")
            return []

    # â˜† ë‹¨ìˆœí™”ëœ ì»¨í…ìŠ¤íŠ¸ ìƒì„± (ì ìˆ˜ ê¸°ë°˜ ìš°ì„ ìˆœìœ„)
    def create_enhanced_context_simple(self, similar_answers: list, max_answers: int = 6, target_lang: str = 'ko') -> str:
        """ì ìˆ˜ ê¸°ë°˜ì˜ ë‹¨ìˆœí•œ ì»¨í…ìŠ¤íŠ¸ ìƒì„±"""
        
        if not similar_answers:
            return ""
        
        context_parts = []
        used_count = 0
        
        # ì ìˆ˜ ìˆœìœ¼ë¡œ ì •ë ¬ë˜ì–´ ìˆìœ¼ë¯€ë¡œ ìˆœì„œëŒ€ë¡œ ì²˜ë¦¬
        for i, ans in enumerate(similar_answers[:max_answers]):
            if used_count >= max_answers:
                break
            
            score = ans['score']
            answer_text = ans['answer']
            
            # ê¸°ë³¸ ì •ë¦¬
            clean_answer = self.preprocess_text(answer_text)
            clean_answer = self.remove_greeting_and_closing(clean_answer, 'ko')
            
            # ì˜ì–´ ìš”ì²­ì‹œ ë²ˆì—­
            if target_lang == 'en' and ans.get('lang', 'ko') == 'ko':
                clean_answer = self.translate_text(clean_answer, 'ko', 'en')
            
            # ìµœì†Œ í’ˆì§ˆ ê²€ì¦
            if len(clean_answer.strip()) >= 20:
                # ì ìˆ˜ì— ë”°ë¥¸ ê¸¸ì´ ì¡°ì •
                max_length = 500 if score >= 0.8 else 350 if score >= 0.6 else 250
                
                context_parts.append(
                    f"[ì°¸ê³ ë‹µë³€ {used_count+1} - ìœ ì‚¬ë„: {score:.2f}]\n{clean_answer[:max_length]}"
                )
                used_count += 1
        
        logging.info(f"ì»¨í…ìŠ¤íŠ¸ ìƒì„±: {used_count}ê°œ ë‹µë³€ í¬í•¨ (ì–¸ì–´: {target_lang})")
        return "\n\n" + "="*50 + "\n\n".join(context_parts)

    # â˜† ë‹¨ìˆœí™”ëœ GPT ìƒì„± (ëª…í™•í•œ í”„ë¡¬í”„íŠ¸)
    def generate_with_simple_gpt(self, query: str, similar_answers: list, context_analysis: dict, lang: str = 'ko') -> str:
        """ë‹¨ìˆœí™”ëœ GPT ë‹µë³€ ìƒì„± - ë³µì¡í•œ ê²€ì¦ ì œê±°"""
        
        try:
            with memory_cleanup():
                approach = context_analysis['recommended_approach']
                quality_level = context_analysis['quality_level']
                
                # ì ‘ê·¼ ë°©ì‹ì´ GPT ìƒì„±ì´ ì•„ë‹ˆë©´ ë¹ˆ ë¬¸ìì—´ ë°˜í™˜
                if approach not in ['gpt_with_strong_context', 'gpt_with_weak_context']:
                    return ""
                
                context = self.create_enhanced_context_simple(similar_answers, target_lang=lang)
                if not context:
                    return ""
                
                # ë‹¨ìˆœí™”ëœ í”„ë¡¬í”„íŠ¸ ìƒì„±
                system_prompt, user_prompt = self.get_gpt_prompts(query, context, lang)
                
                # í’ˆì§ˆì— ë”°ë¥¸ ë‹¨ìˆœí•œ íŒŒë¼ë¯¸í„° ì„¤ì •
                if quality_level in ['high', 'medium']:
                    temperature = 0.6
                    max_tokens = 650
                else:  # low
                    temperature = 0.7
                    max_tokens = 600
                
                # GPT í˜¸ì¶œ
                response = self.openai_client.chat.completions.create(
                    model=GPT_MODEL,
                    messages=[
                        {"role": "system", "content": system_prompt},
                        {"role": "user", "content": user_prompt}
                    ],
                    max_tokens=max_tokens,
                    temperature=temperature,
                    top_p=0.85,
                    frequency_penalty=0.1,
                    presence_penalty=0.1
                )
                
                generated = response.choices[0].message.content.strip()
                del response
                
                # ê¸°ë³¸ì ì¸ ì •ë¦¬ë§Œ
                generated = self.clean_generated_text(generated)
                
                # ìµœì†Œ ê¸¸ì´ ê²€ì¦ë§Œ
                if len(generated.strip()) < 10:
                    logging.warning("ìƒì„±ëœ ë‹µë³€ì´ ë„ˆë¬´ ì§§ìŒ")
                    return ""
                
                logging.info(f"GPT ìƒì„± ì„±ê³µ ({approach}, í’ˆì§ˆ: {quality_level}): {len(generated)}ì")
                return generated
                
        except Exception as e:
            logging.error(f"GPT ìƒì„± ì‹¤íŒ¨: {e}")
            return ""

    # â˜† ë‹¨ìˆœí™”ëœ í´ë°± ë‹µë³€ ì„ íƒ (ì ìˆ˜ ê¸°ë°˜)
    def get_best_fallback_answer_simple(self, similar_answers: list, lang: str = 'ko') -> str:
        """ì ìˆ˜ ê¸°ë°˜ì˜ ë‹¨ìˆœí•œ ìµœì  ë‹µë³€ ì„ íƒ"""
        
        if not similar_answers:
            return ""
        
        # ìƒìœ„ 3ê°œ ì¤‘ì—ì„œ ì„ íƒ
        for i, ans in enumerate(similar_answers[:3]):
            score = ans['score']
            answer_text = ans['answer']
            
            # ì ìˆ˜ê°€ ë§¤ìš° ë†’ìœ¼ë©´ ì¦‰ì‹œ ë°˜í™˜
            if score >= 0.9:
                logging.info(f"ìµœê³  ì ìˆ˜({score:.3f}) ë‹µë³€ ì§ì ‘ ì‚¬ìš©")
                clean_answer = answer_text.strip()
                return clean_answer if clean_answer else ""
            
            # ê¸°ë³¸ ì „ì²˜ë¦¬
            processed = self.preprocess_text(answer_text)
            
            # ì˜ì–´ ë²ˆì—­
            if lang == 'en' and ans.get('lang', 'ko') == 'ko':
                processed = self.translate_text(processed, 'ko', 'en')
            
            # ê¸°ë³¸ í’ˆì§ˆ ê²€ì¦
            if len(processed.strip()) >= 20:
                # ì²« ë²ˆì§¸ ìœ íš¨í•œ ë‹µë³€ ì„ íƒ
                logging.info(f"í´ë°± ë‹µë³€ ì„ íƒ: #{i+1}, ì ìˆ˜={score:.3f}")
                return processed
        
        # ëª¨ë“  ë‹µë³€ì´ ë¶€ì ì ˆí•œ ê²½ìš° ì²« ë²ˆì§¸ ì›ë³¸ ë°˜í™˜
        if similar_answers:
            return similar_answers[0]['answer'].strip()
        
        return ""

    # â˜† í•µì‹¬ ê°œë… ì¶”ì¶œ ë©”ì„œë“œ
    def extract_key_concepts(self, text: str) -> list:
        """í…ìŠ¤íŠ¸ì—ì„œ í•µì‹¬ ê°œë…ì„ ì¶”ì¶œ"""        
        # 2ê¸€ì ì´ìƒì˜ í•œê¸€ ëª…ì‚¬ ì¶”ì¶œ
        korean_nouns = re.findall(r'[ê°€-í£]{2,}', text)
        
        # ì˜ì–´ ë‹¨ì–´ ì¶”ì¶œ
        english_words = re.findall(r'[a-zA-Z]{3,}', text)
        
        # ì¤‘ë³µ ì œê±° ë° ì •ë¦¬
        concepts = []
        for word in korean_nouns + english_words:
            word = word.lower().strip()
            if len(word) >= 2 and word not in ['ìˆë‚˜ìš”', 'í•´ì£¼ì„¸ìš”', 'ë„ì™€ì£¼ì„¸ìš”', 'ë¬¸ì˜', 'ì§ˆë¬¸']:
                concepts.append(word)
        
        return list(set(concepts))  # ì¤‘ë³µ ì œê±°

    # â˜† ì˜ë¯¸ë¡ ì  ë‹¤ì¸µ ê²€ìƒ‰ ë©”ì„œë“œ (ì˜ë„ ê¸°ë°˜ ê²€ìƒ‰ ê°•í™”)
    def search_similar_answers_enhanced(self, query: str, top_k: int = 8, lang: str = 'ko') -> list:
        """ì˜ë„ ê¸°ë°˜ ë‹¤ì¸µ ê²€ìƒ‰ìœ¼ë¡œ ì˜ë¯¸ë¡ ì ìœ¼ë¡œ ë™ë“±í•œ ì§ˆë¬¸ë“¤ì„ ì •í™•íˆ ë§¤ì¹­"""
        try:
            with memory_cleanup():
                logging.info(f"=== ì˜ë¯¸ë¡ ì  ë‹¤ì¸µ ê²€ìƒ‰ ì‹œì‘ ===")
                logging.info(f"ì›ë³¸ ì§ˆë¬¸: {query}")
                
                # 1. ê¸°ë³¸ ì „ì²˜ë¦¬
                if lang == 'ko':
                    corrected_query = self.fix_korean_typos_with_ai(query)
                    query_to_embed = corrected_query
                else:
                    query_to_embed = query
                
                # 2. â­ í•µì‹¬ ì˜ë„ ë¶„ì„ (ìƒˆë¡œ ì¶”ê°€)
                intent_analysis = self.analyze_question_intent(query_to_embed)
                core_intent = intent_analysis.get('core_intent', '')
                standardized_query = intent_analysis.get('standardized_query', query_to_embed)
                semantic_keywords = intent_analysis.get('semantic_keywords', [])
                
                logging.info(f"í•µì‹¬ ì˜ë„: {core_intent}")
                logging.info(f"í‘œì¤€í™”ëœ ì§ˆë¬¸: {standardized_query}")
                logging.info(f"ì˜ë¯¸ë¡ ì  í‚¤ì›Œë“œ: {semantic_keywords}")
                
                # 3. ê¸°ì¡´ í•µì‹¬ ê°œë… ì¶”ì¶œ (ë³´ì™„ìš©)
                key_concepts = self.extract_key_concepts(query_to_embed)
                
                all_results = []
                seen_ids = set()
                
                # 4. â­ ë‹¤ì¸µ ê²€ìƒ‰ ì¿¼ë¦¬ êµ¬ì„± (ì˜ë„ ê¸°ë°˜ ê°•í™”)
                search_layers = [
                    # Layer 1: ì›ë³¸ ì§ˆë¬¸ (ê°€ì¤‘ì¹˜ 1.0)
                    {'query': query_to_embed, 'weight': 1.0, 'type': 'original'},
                    
                    # Layer 2: í‘œì¤€í™”ëœ ì˜ë„ ê¸°ë°˜ ì§ˆë¬¸ (ê°€ì¤‘ì¹˜ 0.95) â­ í•µì‹¬ ì¶”ê°€
                    {'query': standardized_query, 'weight': 0.95, 'type': 'intent_based'},
                    
                    # Layer 3: í•µì‹¬ ì˜ë„ë§Œ (ê°€ì¤‘ì¹˜ 0.9) â­ í•µì‹¬ ì¶”ê°€
                    {'query': core_intent.replace('_', ' '), 'weight': 0.9, 'type': 'core_intent'},
                ]
                
                # Layer 4: ì˜ë¯¸ë¡ ì  í‚¤ì›Œë“œ ì¡°í•© (ê°€ì¤‘ì¹˜ 0.8)
                if semantic_keywords and len(semantic_keywords) >= 2:
                    semantic_query = ' '.join(semantic_keywords[:3])
                    search_layers.append({
                        'query': semantic_query, 'weight': 0.8, 'type': 'semantic_keywords'
                    })
                
                # Layer 5: ê¸°ì¡´ ê°œë… ê¸°ë°˜ ê²€ìƒ‰ (ë³´ì™„ìš©, ê°€ì¤‘ì¹˜ 0.7)
                if key_concepts:
                    if len(key_concepts) >= 2:
                        concept_query = ' '.join(key_concepts[:3])
                        search_layers.append({
                            'query': concept_query, 'weight': 0.7, 'type': 'concept_based'
                        })
                
                logging.info(f"ê²€ìƒ‰ ë ˆì´ì–´ ìˆ˜: {len(search_layers)}")
                
                # 5. ê° ë ˆì´ì–´ë¡œ ê²€ìƒ‰ ìˆ˜í–‰
                for i, layer in enumerate(search_layers):
                    search_query = layer['query']
                    weight = layer['weight']
                    layer_type = layer['type']
                    
                    if not search_query or len(search_query.strip()) < 2:
                        continue
                    
                    logging.info(f"ë ˆì´ì–´ {i+1} ({layer_type}): {search_query[:50]}...")
                    
                    query_vector = self.create_embedding(search_query)
                    if query_vector is None:
                        continue
                    
                    # ì²« ë²ˆì§¸ ë ˆì´ì–´ëŠ” ë” ë§ì´ ê²€ìƒ‰
                    search_top_k = top_k * 2 if i == 0 else top_k
                    
                    results = index.query(
                        vector=query_vector,
                        top_k=search_top_k,
                        include_metadata=True
                    )
                    
                    # ê²°ê³¼ë¥¼ ê°€ì¤‘ì¹˜ì™€ í•¨ê»˜ ìˆ˜ì§‘
                    for match in results['matches']:
                        match_id = match['id']
                        if match_id not in seen_ids:
                            seen_ids.add(match_id)
                            # ê°€ì¤‘ì¹˜ ì ìš©í•œ ì ìˆ˜ ê³„ì‚°
                            adjusted_score = match['score'] * weight
                            match['adjusted_score'] = adjusted_score
                            match['search_type'] = layer_type
                            match['layer_weight'] = weight
                            all_results.append(match)
                    
                    del query_vector, results
                
                # 6. ì˜ì–´ ì§ˆë¬¸ì¸ ê²½ìš° ë²ˆì—­ ê²€ìƒ‰
                if lang == 'en':
                    korean_query = self.translate_text(query_to_embed, 'en', 'ko')
                    korean_vector = self.create_embedding(korean_query)
                    if korean_vector:
                        korean_results = index.query(
                            vector=korean_vector,
                            top_k=top_k,
                            include_metadata=True
                        )
                        for match in korean_results['matches']:
                            if match['id'] not in seen_ids:
                                match['adjusted_score'] = match['score'] * 0.85
                                match['search_type'] = 'translated'
                                match['layer_weight'] = 0.85
                                all_results.append(match)
                        del korean_vector, korean_results
                
                # 7. ê²°ê³¼ ì •ë ¬ ë° ì˜ë¯¸ë¡ ì  ê´€ë ¨ì„± ê²€ì¦
                all_results.sort(key=lambda x: x['adjusted_score'], reverse=True)
                
                filtered_results = []
                for i, match in enumerate(all_results[:top_k*2]):
                    score = match['adjusted_score']
                    question = match['metadata'].get('question', '')
                    answer = match['metadata'].get('answer', '')
                    category = match['metadata'].get('category', 'ì¼ë°˜')
                    
                    # ê¸°ë³¸ ì„ê³„ê°’ ê²€ì‚¬
                    if score < 0.3 and i >= 5:  # ìƒìœ„ 5ê°œëŠ” ì ìˆ˜ê°€ ë‚®ì•„ë„ í¬í•¨
                        continue
                    
                    # â­ ì˜ë„ ê¸°ë°˜ ê´€ë ¨ì„± ê²€ì¦ (ìƒˆë¡œ ì¶”ê°€)
                    intent_relevance = self.calculate_intent_similarity(
                        intent_analysis, question, answer
                    )
                    
                    # ê¸°ì¡´ ê°œë… ì¼ì¹˜ë„ë„ í•¨ê»˜ ê³ ë ¤
                    concept_relevance = self.calculate_concept_relevance(
                        query_to_embed, key_concepts, question, answer
                    )
                    
                    # ìµœì¢… ì ìˆ˜ = ë²¡í„° ìœ ì‚¬ë„(60%) + ì˜ë„ ê´€ë ¨ì„±(25%) + ê°œë… ê´€ë ¨ì„±(15%)
                    final_score = (score * 0.6 + 
                                 intent_relevance * 0.25 + 
                                 concept_relevance * 0.15)
                    
                    if final_score >= 0.4 or i < 3:  # ìƒìœ„ 3ê°œëŠ” ë¬´ì¡°ê±´ í¬í•¨
                        filtered_results.append({
                            'score': final_score,
                            'vector_score': match['score'],
                            'intent_relevance': intent_relevance,
                            'concept_relevance': concept_relevance,
                            'question': question,
                            'answer': answer,
                            'category': category,
                            'rank': i + 1,
                            'search_type': match['search_type'],
                            'layer_weight': match.get('layer_weight', 1.0),
                            'lang': 'ko'
                        })
                        
                        logging.info(f"ì„ íƒ: #{i+1} ìµœì¢…ì ìˆ˜={final_score:.3f} "
                                   f"(ë²¡í„°={match['score']:.3f}, ì˜ë„={intent_relevance:.3f}, "
                                   f"ê°œë…={concept_relevance:.3f}) íƒ€ì…={match['search_type']}")
                        logging.info(f"ì§ˆë¬¸: {question[:50]}...")
                    
                    if len(filtered_results) >= top_k:
                        break
                
                logging.info(f"ì˜ë¯¸ë¡ ì  ë‹¤ì¸µ ê²€ìƒ‰ ì™„ë£Œ: {len(filtered_results)}ê°œ ë‹µë³€")
                return filtered_results
                
        except Exception as e:
            logging.error(f"ì˜ë¯¸ë¡ ì  ë‹¤ì¸µ ê²€ìƒ‰ ì‹¤íŒ¨: {str(e)}")
            return []

    # â˜† í•µì‹¬ ê°œë… ì¼ì¹˜ë„ ê³„ì‚° ë©”ì„œë“œ
    def calculate_concept_relevance(self, query: str, query_concepts: list, ref_question: str, ref_answer: str) -> float:
        """ì§ˆë¬¸ê³¼ ì°¸ì¡° ë‹µë³€ ê°„ì˜ í•µì‹¬ ê°œë… ì¼ì¹˜ë„ ê³„ì‚°"""
        
        if not query_concepts:
            return 0.5  # ê°œë…ì´ ì—†ìœ¼ë©´ ì¤‘ê°„ê°’
        
        # ì°¸ì¡° ì§ˆë¬¸ê³¼ ë‹µë³€ì—ì„œ ê°œë… ì¶”ì¶œ
        ref_concepts = self.extract_key_concepts(ref_question + ' ' + ref_answer)
        
        if not ref_concepts:
            return 0.3  # ì°¸ì¡°ì— ê°œë…ì´ ì—†ìœ¼ë©´ ë‚®ì€ ì ìˆ˜
        
        # ê°œë… ì¼ì¹˜ë„ ê³„ì‚°
        matched_concepts = 0
        total_weight = 0
        
        for query_concept in query_concepts:
            concept_weight = len(query_concept) / 10.0  # ê¸´ ë‹¨ì–´ì— ë†’ì€ ê°€ì¤‘ì¹˜
            total_weight += concept_weight
            
            # ì •í™•íˆ ì¼ì¹˜í•˜ëŠ” ê°œë… ì°¾ê¸°
            if query_concept in ref_concepts:
                matched_concepts += concept_weight
                continue
            
            # ë¶€ë¶„ ì¼ì¹˜ ê²€ì‚¬ (70% ì´ìƒ ì¼ì¹˜)
            for ref_concept in ref_concepts:
                if len(query_concept) >= 3 and len(ref_concept) >= 3:
                    # ê°„ë‹¨í•œ ë¬¸ìì—´ ìœ ì‚¬ë„ (ê³µí†µ ë¬¸ì ë¹„ìœ¨)
                    common_chars = set(query_concept) & set(ref_concept)
                    similarity = len(common_chars) / max(len(set(query_concept)), len(set(ref_concept)))
                    
                    if similarity >= 0.7:  # 70% ì´ìƒ ìœ ì‚¬í•˜ë©´ ë¶€ë¶„ ì ìˆ˜
                        matched_concepts += concept_weight * similarity
                        break
        
        # ì¼ì¹˜ë„ ë¹„ìœ¨ ê³„ì‚°
        relevance = matched_concepts / total_weight if total_weight > 0 else 0
        
        # 0-1 ë²”ìœ„ë¡œ ì •ê·œí™”
        return min(relevance, 1.0)

    # â˜† ì˜ë„ ê¸°ë°˜ ìœ ì‚¬ì„± ê³„ì‚° ë©”ì„œë“œ (ìƒˆë¡œ ì¶”ê°€)
    def calculate_intent_similarity(self, query_intent_analysis: dict, ref_question: str, ref_answer: str) -> float:
        """ì§ˆë¬¸ì˜ ì˜ë„ì™€ ì°¸ì¡° ë‹µë³€ ê°„ì˜ ì˜ë¯¸ë¡ ì  ìœ ì‚¬ì„± ê³„ì‚°"""
        
        try:
            # 1. ì§ˆë¬¸ ì˜ë„ ì •ë³´ ì¶”ì¶œ
            query_core_intent = query_intent_analysis.get('core_intent', '')
            query_primary_action = query_intent_analysis.get('primary_action', '')
            query_target_object = query_intent_analysis.get('target_object', '')
            query_semantic_keywords = query_intent_analysis.get('semantic_keywords', [])
            
            if not query_core_intent:
                return 0.5  # ì˜ë„ ì •ë³´ê°€ ì—†ìœ¼ë©´ ì¤‘ê°„ê°’
            
            # 2. ì°¸ì¡° ì§ˆë¬¸ê³¼ ë‹µë³€ì—ì„œ ì˜ë„ ë¶„ì„
            ref_text = ref_question + ' ' + ref_answer
            ref_intent_analysis = self.analyze_question_intent(ref_question)
            
            ref_core_intent = ref_intent_analysis.get('core_intent', '')
            ref_primary_action = ref_intent_analysis.get('primary_action', '')
            ref_target_object = ref_intent_analysis.get('target_object', '')
            ref_semantic_keywords = ref_intent_analysis.get('semantic_keywords', [])
            
            # 3. í•µì‹¬ ì˜ë„ ì¼ì¹˜ë„ ê³„ì‚° (ê°€ì¥ ì¤‘ìš”)
            intent_match_score = 0.0
            if query_core_intent == ref_core_intent:
                intent_match_score = 1.0
            elif query_core_intent and ref_core_intent:
                # ì˜ë„ ì´ë¦„ì˜ ìœ ì‚¬ì„± ê²€ì‚¬ (ë¶€ë¶„ ì¼ì¹˜)
                query_intent_words = set(query_core_intent.split('_'))
                ref_intent_words = set(ref_core_intent.split('_'))
                
                if query_intent_words & ref_intent_words:  # ê³µí†µ ë‹¨ì–´ê°€ ìˆìœ¼ë©´
                    overlap_ratio = len(query_intent_words & ref_intent_words) / len(query_intent_words | ref_intent_words)
                    intent_match_score = overlap_ratio * 0.8  # ì™„ì „ ì¼ì¹˜ë³´ë‹¤ëŠ” ë‚®ê²Œ
            
            # 4. í–‰ë™ ìœ í˜• ì¼ì¹˜ë„ ê³„ì‚°
            action_match_score = 0.0
            if query_primary_action == ref_primary_action:
                action_match_score = 1.0
            elif query_primary_action and ref_primary_action:
                # í–‰ë™ ìœ í˜• ìœ ì‚¬ì„± ê²€ì‚¬
                action_similarity_map = {
                    ('ë³´ê¸°', 'í™•ì¸'): 0.8,
                    ('ë³µì‚¬', 'ì €ì¥'): 0.7,
                    ('ë“£ê¸°', 'ì¬ìƒ'): 0.9,
                    ('ê²€ìƒ‰', 'ì°¾ê¸°'): 0.8,
                    ('ì„¤ì •', 'ë³€ê²½'): 0.7
                }
                
                action_key = (query_primary_action, ref_primary_action)
                reverse_key = (ref_primary_action, query_primary_action)
                
                if action_key in action_similarity_map:
                    action_match_score = action_similarity_map[action_key]
                elif reverse_key in action_similarity_map:
                    action_match_score = action_similarity_map[reverse_key]
            
            # 5. ëŒ€ìƒ ê°ì²´ ì¼ì¹˜ë„ ê³„ì‚°
            object_match_score = 0.0
            if query_target_object == ref_target_object:
                object_match_score = 1.0
            elif query_target_object and ref_target_object:
                # ê°ì²´ ìœ ì‚¬ì„± ê²€ì‚¬
                object_similarity_map = {
                    ('ë²ˆì—­ë³¸', 'ì„±ê²½'): 0.8,
                    ('í…ìŠ¤íŠ¸', 'ë‚´ìš©'): 0.7,
                    ('ìŒì„±', 'ì˜¤ë””ì˜¤'): 0.9,
                    ('í™”ë©´', 'ë””ìŠ¤í”Œë ˆì´'): 0.7
                }
                
                object_key = (query_target_object, ref_target_object)
                reverse_key = (ref_target_object, query_target_object)
                
                if object_key in object_similarity_map:
                    object_match_score = object_similarity_map[object_key]
                elif reverse_key in object_similarity_map:
                    object_match_score = object_similarity_map[reverse_key]
            
            # 6. ì˜ë¯¸ë¡ ì  í‚¤ì›Œë“œ ì¼ì¹˜ë„ ê³„ì‚°
            keyword_match_score = 0.0
            if query_semantic_keywords and ref_semantic_keywords:
                query_keyword_set = set(query_semantic_keywords)
                ref_keyword_set = set(ref_semantic_keywords)
                
                common_keywords = query_keyword_set & ref_keyword_set
                total_keywords = query_keyword_set | ref_keyword_set
                
                if total_keywords:
                    keyword_match_score = len(common_keywords) / len(total_keywords)
            
            # 7. ì „ì²´ ì ìˆ˜ ê³„ì‚° (ê°€ì¤‘ í‰ê· )
            total_score = (
                intent_match_score * 0.4 +      # í•µì‹¬ ì˜ë„ ì¼ì¹˜ (40%)
                action_match_score * 0.25 +     # í–‰ë™ ìœ í˜• ì¼ì¹˜ (25%)
                object_match_score * 0.2 +      # ëŒ€ìƒ ê°ì²´ ì¼ì¹˜ (20%)
                keyword_match_score * 0.15      # í‚¤ì›Œë“œ ì¼ì¹˜ (15%)
            )
            
            logging.debug(f"ì˜ë„ ìœ ì‚¬ì„± ë¶„ì„: ì˜ë„={intent_match_score:.2f}, "
                         f"í–‰ë™={action_match_score:.2f}, ê°ì²´={object_match_score:.2f}, "
                         f"í‚¤ì›Œë“œ={keyword_match_score:.2f}, ì „ì²´={total_score:.2f}")
            
            return min(total_score, 1.0)
            
        except Exception as e:
            logging.error(f"ì˜ë„ ìœ ì‚¬ì„± ê³„ì‚° ì‹¤íŒ¨: {e}")
            return 0.3  # ì˜¤ë¥˜ì‹œ ë‚®ì€ ê¸°ë³¸ê°’

    # â˜† ê¸°ì¡´ ë©”ì„œë“œë¥¼ í–¥ìƒëœ ë²„ì „ìœ¼ë¡œ êµì²´
    def search_similar_answers(self, query: str, top_k: int = 5, similarity_threshold: float = 0.7, lang: str = 'ko') -> list:
        """í–¥ìƒëœ ê²€ìƒ‰ ë©”ì„œë“œ ì‚¬ìš©"""
        return self.search_similar_answers_enhanced(query, top_k, lang)

    # â˜† í–¥ìƒëœ ì»¨í…ìŠ¤íŠ¸ í’ˆì§ˆ ë¶„ì„
    def analyze_context_quality(self, similar_answers: list, query: str) -> dict:
        """í–¥ìƒëœ ì»¨í…ìŠ¤íŠ¸ í’ˆì§ˆ ë¶„ì„ - ê°œë… ì¼ì¹˜ë„ ê³ ë ¤"""
        
        if not similar_answers:
            return {
                'has_good_context': False,
                'best_score': 0.0,
                'recommended_approach': 'fallback',
                'quality_level': 'none'
            }
        
        # ìµœê³  ì ìˆ˜ì™€ ê´€ë ¨ì„± ì ìˆ˜ í™•ì¸
        best_answer = similar_answers[0]
        best_score = best_answer['score']
        relevance_score = best_answer.get('relevance_score', 0.5)
        
        # ê³ í’ˆì§ˆ ë‹µë³€ ê°œìˆ˜ ê³„ì‚°
        high_quality_count = len([ans for ans in similar_answers if ans['score'] >= 0.7])
        good_relevance_count = len([ans for ans in similar_answers if ans.get('relevance_score', 0) >= 0.6])
        
        # ì ‘ê·¼ ë°©ì‹ ê²°ì • (ê°œë… ì¼ì¹˜ë„ ê³ ë ¤)
        if best_score >= 0.9 and relevance_score >= 0.7:
            approach = 'direct_use'
            quality_level = 'excellent'
        elif best_score >= 0.8 and relevance_score >= 0.6:
            approach = 'direct_use' 
            quality_level = 'very_high'
        elif best_score >= 0.7 and relevance_score >= 0.5:
            approach = 'gpt_with_strong_context'
            quality_level = 'high'
        elif best_score >= 0.6 and (high_quality_count + good_relevance_count) >= 2:
            approach = 'gpt_with_strong_context'
            quality_level = 'medium'
        elif best_score >= 0.4 and relevance_score >= 0.4:
            approach = 'gpt_with_weak_context'
            quality_level = 'low'
        else:
            approach = 'fallback'
            quality_level = 'very_low'
        
        return {
            'has_good_context': quality_level in ['excellent', 'very_high', 'high', 'medium'],
            'best_score': best_score,
            'relevance_score': relevance_score,
            'high_quality_count': high_quality_count,
            'good_relevance_count': good_relevance_count,
            'recommended_approach': approach,
            'quality_level': quality_level,
            'context_summary': f"í’ˆì§ˆ: {quality_level}, ì ìˆ˜: {best_score:.3f}, ê´€ë ¨ì„±: {relevance_score:.3f}"
        }

# ==================================================
# 8. Pinecone ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ë™ê¸°í™” í´ë˜ìŠ¤
# ==================================================
# MSSQL ìš´ì˜ ë°ì´í„°ë² ì´ìŠ¤ì™€ Pinecone ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ê°„ì˜ ë™ê¸°í™”ë¥¼ ë‹´ë‹¹í•˜ëŠ” í´ë˜ìŠ¤
# 
# ì£¼ìš” ê¸°ëŠ¥:
# 1. MSSQLì—ì„œ ìƒˆë¡œìš´ Q&A ë°ì´í„° ì¡°íšŒ
# 2. AIë¥¼ ì´ìš©í•œ í•œêµ­ì–´ ì˜¤íƒ€ ìˆ˜ì •
# 3. OpenAIë¡œ ì„ë² ë”© ë²¡í„° ìƒì„±
# 4. Pineconeì— ë²¡í„° ë°ì´í„° ì €ì¥/ìˆ˜ì •/ì‚­ì œ
# 
# ìš´ì˜ ì‹œë‚˜ë¦¬ì˜¤:
# - ìƒˆë¡œìš´ ê³ ê° ë¬¸ì˜ ë‹µë³€ì´ MSSQLì— ì €ì¥ë˜ë©´
# - ì´ í´ë˜ìŠ¤ë¥¼ í†µí•´ Pineconeì— ë™ê¸°í™”í•˜ì—¬
# - í–¥í›„ ìœ ì‚¬ ì§ˆë¬¸ ê²€ìƒ‰ì´ ê°€ëŠ¥í•˜ê²Œ í•¨
class PineconeSyncManager:
    
    # ë™ê¸°í™” ë§¤ë‹ˆì € ì´ˆê¸°í™”
    # ì™¸ë¶€ì—ì„œ ìƒì„±ëœ Pinecone ì¸ë±ìŠ¤ì™€ OpenAI í´ë¼ì´ì–¸íŠ¸ ì°¸ì¡°
    def __init__(self):
        self.index = index                    # Pinecone ë²¡í„° ì¸ë±ìŠ¤
        self.openai_client = openai_client    # OpenAI API í´ë¼ì´ì–¸íŠ¸
    
    # â˜† AIë¥¼ ì´ìš©í•œ í•œêµ­ì–´ ì˜¤íƒ€ ìˆ˜ì • ë©”ì„œë“œ
    def fix_korean_typos_with_ai(self, text: str) -> str:
        if not text or len(text.strip()) < 3:
            return text
        
        # ë„ˆë¬´ ê¸´ í…ìŠ¤íŠ¸ëŠ” ì²˜ë¦¬í•˜ì§€ ì•ŠìŒ (ë¹„ìš© ì ˆì•½)
        if len(text) > 500:
            logging.warning(f"í…ìŠ¤íŠ¸ê°€ ë„ˆë¬´ ê¸¸ì–´ ì˜¤íƒ€ ìˆ˜ì • ê±´ë„ˆëœ€: {len(text)}ì")
            return text
        
        try:
            with memory_cleanup():
                system_prompt = """ë‹¹ì‹ ì€ í•œêµ­ì–´ ë§ì¶¤ë²• ë° ì˜¤íƒ€ êµì • ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

ì§€ì¹¨:
1. ì…ë ¥ëœ í•œêµ­ì–´ í…ìŠ¤íŠ¸ì˜ ë§ì¶¤ë²•ê³¼ ì˜¤íƒ€ë§Œ ìˆ˜ì •í•˜ì„¸ìš”
2. ì›ë¬¸ì˜ ì˜ë¯¸ì™€ ì–´ì¡°ëŠ” ì ˆëŒ€ ë³€ê²½í•˜ì§€ ë§ˆì„¸ìš”
3. ë„ì–´ì“°ê¸°, ë§ì¶¤ë²•, ì¡°ì‚¬ ì‚¬ìš©ë²•ì„ ì •í™•íˆ êµì •í•˜ì„¸ìš”
4. ì•±/ì–´í”Œë¦¬ì¼€ì´ì…˜ ê´€ë ¨ ê¸°ìˆ  ìš©ì–´ëŠ” í‘œì¤€ ìš©ì–´ë¡œ í†µì¼í•˜ì„¸ìš”
5. ìˆ˜ì •ì´ í•„ìš”ì—†ë‹¤ë©´ ì›ë¬¸ ê·¸ëŒ€ë¡œ ë°˜í™˜í•˜ì„¸ìš”
6. ìˆ˜ì •ëœ í…ìŠ¤íŠ¸ë§Œ ë°˜í™˜í•˜ê³  ì¶”ê°€ ì„¤ëª…ì€ í•˜ì§€ ë§ˆì„¸ìš”

ì˜ˆì‹œ:
- "ì–´í”Œì´ ì•ˆë€ë‹¤" â†’ "ì•±ì´ ì•ˆ ë¼ìš”"
- "ë‹¤ìš´ë°›ê¸°ê°€ ì•ˆë˜ìš”" â†’ "ë‹¤ìš´ë¡œë“œê°€ ì•ˆ ë¼ìš”"
- "ì‚­ì¬í•˜ê³ ì‹¶ì–´ìš”" â†’ "ì‚­ì œí•˜ê³  ì‹¶ì–´ìš”"
- "ì—…ë°ì´ë“œí•´ì£¼ì„¸ìš”" â†’ "ì—…ë°ì´íŠ¸í•´ì£¼ì„¸ìš”"
"""

                user_prompt = f"ë‹¤ìŒ í…ìŠ¤íŠ¸ì˜ ë§ì¶¤ë²•ê³¼ ì˜¤íƒ€ë¥¼ ìˆ˜ì •í•´ì£¼ì„¸ìš”:\n\n{text}"

                response = self.openai_client.chat.completions.create(
                    model='gpt-3.5-turbo',
                    messages=[
                        {"role": "system", "content": system_prompt},
                        {"role": "user", "content": user_prompt}
                    ],
                    max_tokens=600,
                    temperature=0.1,  # ë§¤ìš° ë³´ìˆ˜ì ìœ¼ë¡œ ì„¤ì •
                    top_p=0.8,
                    frequency_penalty=0.0,
                    presence_penalty=0.0
                )
                
                corrected_text = response.choices[0].message.content.strip()
                del response # ë©”ëª¨ë¦¬ í•´ì œ
                
                # ê²°ê³¼ ê²€ì¦
                if not corrected_text or len(corrected_text) == 0:
                    logging.warning("AI ì˜¤íƒ€ ìˆ˜ì • ê²°ê³¼ê°€ ë¹„ì–´ìˆìŒ, ì›ë¬¸ ë°˜í™˜")
                    return text
                
                # ë„ˆë¬´ ë§ì´ ë³€ê²½ëœ ê²½ìš° ì˜ì‹¬ìŠ¤ëŸ¬ìš°ë¯€ë¡œ ì›ë¬¸ ë°˜í™˜
                if len(corrected_text) > len(text) * 2:
                    logging.warning("AI ì˜¤íƒ€ ìˆ˜ì • ê²°ê³¼ê°€ ì›ë¬¸ë³´ë‹¤ ë„ˆë¬´ ê¸¸ì–´ì§, ì›ë¬¸ ë°˜í™˜")
                    return text
                
                # ìˆ˜ì • ë‚´ìš©ì´ ìˆìœ¼ë©´ ë¡œê·¸ ê¸°ë¡
                if corrected_text != text:
                    logging.info(f"AI ì˜¤íƒ€ ìˆ˜ì •: '{text[:50]}...' â†’ '{corrected_text[:50]}...'")
                
                return corrected_text
                
        except Exception as e:
            logging.error(f"AI ì˜¤íƒ€ ìˆ˜ì • ì‹¤íŒ¨: {e}")
            # AI ì‹¤íŒ¨ ì‹œ ì›ë¬¸ ê·¸ëŒ€ë¡œ ë°˜í™˜
            return text
        
    # â˜† í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ ë©”ì„œë“œ
    def preprocess_text(self, text: str, for_metadata: bool = False) -> str:
        if not text or text == 'None':
            return ""
        
        text = str(text)
        text = html.unescape(text)
        
        # HTML íƒœê·¸ ì œê±°
        text = re.sub(r'<br\s*/?>', '\n', text, flags=re.IGNORECASE)
        text = re.sub(r'</p>', '\n', text, flags=re.IGNORECASE)
        text = re.sub(r'<p[^>]*>', '', text, flags=re.IGNORECASE)
        text = re.sub(r'<[^>]+>', '', text)
        
        # ìœ ë‹ˆì½”ë“œ ì •ê·œí™”
        text = unicodedata.normalize('NFC', text)
        
        # ê³µë°± ì •ë¦¬
        if for_metadata:
            text = re.sub(r'\n{3,}', '\n\n', text)
            text = re.sub(r'[ \t]+', ' ', text)
        else:
            text = re.sub(r'\s+', ' ', text)
        
        text = text.strip()
        
        # ê¸¸ì´ ì œí•œ
        max_length = 1000 if for_metadata else MAX_TEXT_LENGTH
        if len(text) > max_length:
            text = text[:max_length-3] + "..."
        
        return text
    
    # â˜† OpenAIë¡œ ì„ë² ë”© ìƒì„±í•˜ëŠ” ë©”ì„œë“œ
    def create_embedding(self, text: str) -> Optional[list]:
        try:
            if not text or not text.strip():
                return None
            
            with memory_cleanup():
                response = openai_client.embeddings.create(
                    model=MODEL_NAME,
                    input=text
                )
                return response.data[0].embedding
            
        except Exception as e:
            logging.error(f"ì„ë² ë”© ìƒì„± ì‹¤íŒ¨: {e}")
            return None
    
    # â˜† ì¹´í…Œê³ ë¦¬ ì¸ë±ìŠ¤ë¥¼ ì´ë¦„ìœ¼ë¡œ ë³€í™˜í•˜ëŠ” ë©”ì„œë“œ
    def get_category_name(self, cate_idx: str) -> str:
        return CATEGORY_MAPPING.get(str(cate_idx), 'ì‚¬ìš© ë¬¸ì˜(ê¸°íƒ€)')
    
    # â˜† MSSQLì—ì„œ ë°ì´í„° ì¡°íšŒí•˜ëŠ” ë©”ì„œë“œ
    # íŒŒë¼ë¯¸í„°í™”ëœ ì¿¼ë¦¬ë¡œ SQL ì¸ì ì…˜ ë°©ì§€, ? í”Œë ˆì´ìŠ¤í™€ë”ë¥¼ ì‚¬ìš©í•˜ì—¬ ì•ˆì „í•˜ê²Œ ê°’ì„ ë°”ì¸ë”©
    def get_mssql_data(self, seq: int) -> Optional[Dict]:
        try:
            with memory_cleanup():
                conn = pyodbc.connect(connection_string)
                cursor = conn.cursor()
                
                query = """
                SELECT seq, contents, reply_contents, cate_idx, name, 
                       CONVERT(varchar, regdate, 120) as regdate
                FROM mobile.dbo.bible_inquiry
                WHERE seq = ? AND answer_YN = 'Y'
                """
                
                cursor.execute(query, seq)
                row = cursor.fetchone()
                
                if row:
                    data = {
                        'seq': row[0],
                        'contents': row[1],
                        'reply_contents': row[2],
                        'cate_idx': row[3],
                        'name': row[4],
                        'regdate': row[5]
                    }
                    return data
                
                cursor.close()
                conn.close()
                return None
            
        except Exception as e:
            logging.error(f"MSSQL ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return None
    
    # â˜† MSSQL ë°ì´í„°ë¥¼ Pineconeì— ë™ê¸°í™”í•˜ëŠ” ë©”ì„œë“œ
    def sync_to_pinecone(self, seq: int, mode: str = 'upsert') -> Dict[str, Any]:
        try:
            with memory_cleanup():
                # ì‚­ì œ ëª¨ë“œ
                if mode == 'delete':
                    vector_id = f"qa_bible_{seq}"
                    self.index.delete(ids=[vector_id])
                    logging.info(f"Pineconeì—ì„œ ì‚­ì œ ì™„ë£Œ: {vector_id}")
                    return {"success": True, "message": "ì‚­ì œ ì™„ë£Œ", "seq": seq}
                
                # MSSQLì—ì„œ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
                data = self.get_mssql_data(seq)
                if not data:
                    return {"success": False, "error": "ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤"}
                
                # í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ (ì§ˆë¬¸ì— AI ì˜¤íƒ€ ìˆ˜ì • ì ìš©)
                raw_question = self.preprocess_text(data['contents'])
                question = self.fix_korean_typos_with_ai(raw_question)
                answer = self.preprocess_text(data['reply_contents'])
                
                # ì„ë² ë”© ìƒì„± (ì§ˆë¬¸ ê¸°ë°˜)
                embedding = self.create_embedding(question)
                if not embedding:
                    return {"success": False, "error": "ì„ë² ë”© ìƒì„± ì‹¤íŒ¨"}
                
                # ì¹´í…Œê³ ë¦¬ ì´ë¦„ ê°€ì ¸ì˜¤ê¸°
                category = self.get_category_name(data['cate_idx'])
                
                # ë©”íƒ€ë°ì´í„° êµ¬ì„± (ì§ˆë¬¸ì€ ì˜¤íƒ€ ìˆ˜ì •ëœ ë²„ì „ ì‚¬ìš©)
                metadata = {
                    "seq": int(data['seq']),
                    "question": question,
                    "answer": self.preprocess_text(data['reply_contents'], for_metadata=True),
                    "category": category,
                    "name": data['name'] if data['name'] else "ìµëª…",
                    "regdate": data['regdate'],
                    "source": "bible_inquiry_mssql",
                    "updated_at": datetime.now().strftime("%Y-%m-%d %H:%M:%S")
                }
                
                # Pineconeì— upsert
                vector_id = f"qa_bible_{seq}"
                
                # ê¸°ì¡´ ë²¡í„° í™•ì¸
                existing = self.index.fetch(ids=[vector_id])
                is_update = vector_id in existing['vectors']
                
                # ë²¡í„° ë°ì´í„° êµ¬ì„±
                vector_data = {
                    "id": vector_id,
                    "values": embedding,
                    "metadata": metadata
                }
                
                # Pineconeì— ì €ì¥
                self.index.upsert(vectors=[vector_data])
                
                action = "ìˆ˜ì •" if is_update else "ìƒì„±"
                logging.info(f"Pinecone {action} ì™„ë£Œ: {vector_id}")
                
                return {
                    "success": True,
                    "message": f"Pinecone {action} ì™„ë£Œ",
                    "seq": seq,
                    "vector_id": vector_id,
                    "is_update": is_update
                }
            
        except Exception as e:
            logging.error(f"Pinecone ë™ê¸°í™” ì‹¤íŒ¨: {str(e)}")
            return {"success": False, "error": str(e)}

# ==================================================
# 9. ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤ ìƒì„± (ì „ì—­ ê°ì²´)
# ==================================================
# ì• í”Œë¦¬ì¼€ì´ì…˜ ì „ì²´ì—ì„œ ì‚¬ìš©í•  ë‹¨ì¼ ì¸ìŠ¤í„´ìŠ¤ë“¤
# ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±ê³¼ ìƒíƒœ ì¼ê´€ì„±ì„ ìœ„í•´ ì‹±ê¸€í†¤ íŒ¨í„´ ì ìš©
generator = AIAnswerGenerator()      # AI ë‹µë³€ ìƒì„±ê¸°
sync_manager = PineconeSyncManager() # Pinecone ë™ê¸°í™” ë§¤ë‹ˆì €

# ==================================================
# 10. Flask RESTful API ì—”ë“œí¬ì¸íŠ¸ ì •ì˜
# ==================================================
# â˜… AI ë‹µë³€ ìƒì„± API ì—”ë“œí¬ì¸íŠ¸ (ë©”ì¸ ê¸°ëŠ¥)
# ASP Classicì—ì„œ í˜¸ì¶œí•˜ëŠ” ì£¼ìš” APIë¡œ, ê³ ê° ì§ˆë¬¸ì— ëŒ€í•œ AI ë‹µë³€ì„ ìƒì„±
# ì²˜ë¦¬ ê³¼ì •:
# 1. ì§ˆë¬¸ ì „ì²˜ë¦¬ ë° ê²€ì¦
# 2. Pineconeì—ì„œ ìœ ì‚¬ ë‹µë³€ ê²€ìƒ‰
# 3. ì»¨í…ìŠ¤íŠ¸ í’ˆì§ˆ ë¶„ì„
# 4. GPTë¥¼ ì´ìš©í•œ ë§ì¶¤ ë‹µë³€ ìƒì„±
# 5. ìµœì¢… í¬ë§·íŒ… ë° ë°˜í™˜
@app.route('/generate_answer', methods=['POST'])
def generate_answer():
    try:
        with memory_cleanup():
            data = request.get_json()
            seq = data.get('seq', 0)
            question = data.get('question', '')
            lang = data.get('lang', 'auto')  # ê¸°ë³¸ê°’ì„ 'auto'ë¡œ ë³€ê²½ (ìë™ ê°ì§€)
            
            if not question:
                return jsonify({"success": False, "error": "ì§ˆë¬¸ì´ í•„ìš”í•©ë‹ˆë‹¤."}), 400
            
            result = generator.process(seq, question, lang)
            
            response = jsonify(result)
            response.headers['Content-Type'] = 'application/json; charset=utf-8'

            # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ëª¨ë‹ˆí„°ë§
            snapshot = tracemalloc.take_snapshot() # ê° ìš”ì²­ í›„ ë©”ëª¨ë¦¬ ìŠ¤ëƒ…ìƒ· ì´¬ì˜
            top_stats = snapshot.statistics('lineno') # ê° ìš”ì²­ í›„ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ í†µê³„
            memory_usage = sum(stat.size for stat in top_stats) / 1024 / 1024  # MBë¡œ ë°˜í™˜
            logging.info(f"í˜„ì¬ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {memory_usage:.2f}MB")
            
            if memory_usage > 500: # 500MB ì´ˆê³¼ì‹œ ê²½ê³  ë° ê°€ë¹„ì§€ ì»¬ë ‰ì…˜ ê°•ì œ ì‹¤í–‰
                logging.warning(f"ë†’ì€ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ê°ì§€: {memory_usage:.2f}MB")
                gc.collect()

            return response
        
    except Exception as e:
        logging.error(f"API í˜¸ì¶œ ì˜¤ë¥˜: {str(e)}")
        return jsonify({"success": False, "error": str(e)}), 500

# â˜… MSSQL ë°ì´í„°ë¥¼ Pineconeì— ë™ê¸°í™”í•˜ëŠ” API ì—”ë“œí¬ì¸íŠ¸
# ìš´ì˜ ì‹œìŠ¤í…œì—ì„œ ìƒˆë¡œìš´ Q&A ë°ì´í„°ê°€ ìƒì„±ë˜ê±°ë‚˜ ìˆ˜ì •ë  ë•Œ í˜¸ì¶œ
# ì²˜ë¦¬ ê³¼ì •:
# 1. MSSQLì—ì„œ í•´ë‹¹ seq ë°ì´í„° ì¡°íšŒ
# 2. AIë¡œ ì§ˆë¬¸ ì˜¤íƒ€ ìˆ˜ì •
# 3. OpenAIë¡œ ì„ë² ë”© ë²¡í„° ìƒì„±
# 4. Pineconeì— ë²¡í„° ì €ì¥/ìˆ˜ì •/ì‚­ì œ
@app.route('/sync_to_pinecone', methods=['POST'])
def sync_to_pinecone():
    try:
        data = request.get_json()
        seq = data.get('seq')
        mode = data.get('mode', 'upsert')

        logging.info(f"ë™ê¸°í™” ìš”ì²­ ìˆ˜ì‹ : seq={seq}, mode={mode}")
        
        if not seq:
            logging.warning("seq ëˆ„ë½")
            return jsonify({"success": False, "error": "seqê°€ í•„ìš”í•©ë‹ˆë‹¤"}), 400
        
        if not isinstance(seq, int):
            seq = int(seq)
        
        result = sync_manager.sync_to_pinecone(seq, mode)

        logging.info(f"ë™ê¸°í™” ê²°ê³¼: {result}")
        
        status_code = 200 if result["success"] else 500
        return jsonify(result), status_code
        
    except ValueError as e:
        logging.error(f"ì˜ëª»ëœ seq ê°’: {str(e)}")
        return jsonify({"success": False, "error": f"ì˜ëª»ëœ seq ê°’: {str(e)}"}), 400
    except Exception as e:
        logging.error(f"Pinecone ë™ê¸°í™” API ì˜¤ë¥˜: {str(e)}")
        return jsonify({"success": False, "error": str(e)}), 500

# â˜… ì‹œìŠ¤í…œ ìƒíƒœ í™•ì¸ì„ ìœ„í•œ í—¬ìŠ¤ì²´í¬ API ì—”ë“œí¬ì¸íŠ¸
# ë¡œë“œë°¸ëŸ°ì„œë‚˜ ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œì—ì„œ í˜¸ì¶œí•˜ì—¬ ì„œë²„ ìƒíƒœ í™•ì¸
@app.route('/health', methods=['GET'])
def health_check():
    try:
        stats = index.describe_index_stats()
        
        return jsonify({
            "status": "healthy",
            "pinecone_vectors": stats.get('total_vector_count', 0),
            "timestamp": datetime.now().isoformat(),
            "services": {
                "ai_answer": "active",
                "pinecone_sync": "active",
                "multilingual_support": "active"  # ë‹¤êµ­ì–´ ì§€ì› ìƒíƒœ ì¶”ê°€
            }
        }), 200
    except Exception as e:
        return jsonify({
            "status": "unhealthy",
            "error": str(e)
        }), 500

# ==================================================
# 11. ë©”ì¸ ì‹¤í–‰ ë¶€ë¶„
# ==================================================
# Flask ì›¹ ì„œë²„ ì‹œì‘ì 
# 
# ì´ ë¶€ë¶„ì€ ì´ íŒŒì¼(ìŠ¤í¬ë¦½íŠ¸)ê°€ ì§ì ‘ ì‹¤í–‰ë  ë•Œë§Œ ì‹¤í–‰ë˜ëŠ” ì ˆì°¨ì  ì½”ë“œ
# ë‹¤ë¥¸ ëª¨ë“ˆì—ì„œ importí•  ë•ŒëŠ” ì‹¤í–‰ë˜ì§€ ì•ŠìŒ
# 
# ì„¤ì •:
# - í¬íŠ¸: í™˜ê²½ë³€ìˆ˜ FLASK_PORT ë˜ëŠ” ê¸°ë³¸ê°’ 8000
# - í˜¸ìŠ¤íŠ¸: 0.0.0.0 (ëª¨ë“  IPì—ì„œ ì ‘ê·¼ ê°€ëŠ¥)
# - ë””ë²„ê·¸: False (ìš´ì˜ ëª¨ë“œ)
# - ìŠ¤ë ˆë“œ: True (ë©€í‹°ìŠ¤ë ˆë”© ì§€ì›)
if __name__ == "__main__":
    
    # í™˜ê²½ë³€ìˆ˜ì—ì„œ í¬íŠ¸ ì„¤ì • ë¡œë“œ (ê¸°ë³¸ê°’: 8000)
    port = int(os.getenv('FLASK_PORT', 8000))
    
    # ì‹œì‘ ë©”ì‹œì§€ ì¶œë ¥
    print("="*60)
    print("ğŸš€ GOODTV ë°”ì´ë¸” ì• í”Œ AI ë‹µë³€ ìƒì„± ì„œë²„ ì‹œì‘")
    print("="*60)
    print(f"ğŸ“¡ ì„œë²„ í¬íŠ¸: {port}")
    print(f"ğŸ¤– AI ëª¨ë¸: {GPT_MODEL} (Enhanced Context Mode)")
    print(f"ğŸ” ì„ë² ë”© ëª¨ë¸: {MODEL_NAME}")
    print(f"ğŸ—ƒï¸  ë²¡í„° DB: Pinecone ({INDEX_NAME})")
    print(f"ğŸŒ ë‹¤êµ­ì–´ ì§€ì›: í•œêµ­ì–´(ko), ì˜ì–´(en)")
    print("ğŸ”§ ì œê³µ ì„œë¹„ìŠ¤:")
    print("   â”œâ”€â”€ AI ë‹µë³€ ìƒì„± (/generate_answer)")
    print("   â”œâ”€â”€ Pinecone ë™ê¸°í™” (/sync_to_pinecone)")
    print("   â””â”€â”€ í—¬ìŠ¤ì²´í¬ (/health)")
    print("="*60)
    
    # Flask ì›¹ ì„œë²„ ì‹œì‘
    # host='0.0.0.0': ëª¨ë“  ë„¤íŠ¸ì›Œí¬ ì¸í„°í˜ì´ìŠ¤ì—ì„œ ì ‘ê·¼ í—ˆìš©
    # debug=False: ìš´ì˜ ëª¨ë“œ (ë³´ì•ˆìƒ ì¤‘ìš”)
    # threaded=True: ë©€í‹°ìŠ¤ë ˆë”© í™œì„±í™”, ë™ì‹œ ìš”ì²­ ì²˜ë¦¬ ê°€ëŠ¥
    app.run(host='0.0.0.0', port=port, debug=False, threaded=True)