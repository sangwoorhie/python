#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
ìµœì í™”ëœ AI ë‹µë³€ ìƒì„± ë©”ì¸ í´ë˜ìŠ¤
ìºì‹±, ë°°ì¹˜ ì²˜ë¦¬, ì§€ëŠ¥í˜• API ê´€ë¦¬ë¥¼ í†µí•©í•œ ê³ ì„±ëŠ¥ AI ì‹œìŠ¤í…œ
"""

import re
import logging
import time
from memory_profiler import profile
from typing import Dict, List, Optional
import numpy as np
from langdetect import detect, LangDetectException
import json

# ê¸°ì¡´ ëª¨ë“ˆë“¤
from src.utils.text_preprocessor import TextPreprocessor
from src.utils.memory_manager import memory_cleanup
from src.models.answer_generator import AnswerGenerator
from src.services.quality_validator import QualityValidator
from src.services.sync_service import SyncService
from src.utils.unified_text_analyzer import UnifiedTextAnalyzer

# ìµœì í™” ëª¨ë“ˆë“¤
from src.utils.cache_manager import CacheManager
from src.utils.batch_processor import BatchProcessor
from src.utils.intelligent_api_manager import (
    IntelligentAPIManager, APICallRequest, APICallStrategy
)
from src.services.optimized_search_service import OptimizedSearchService


class OptimizedAIAnswerGenerator:
    """ìµœì í™”ëœ AI ë‹µë³€ ìƒì„± í´ë˜ìŠ¤ - ê¸°ì¡´ ì¸í„°í˜ì´ìŠ¤ ì™„ì „ í˜¸í™˜"""

    def __init__(self, pinecone_index, openai_client, connection_string=None, 
                 category_mapping=None, redis_config=None):
        # 1ë‹¨ê³„. ê¸°ë³¸ ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™”
        self.index = pinecone_index
        self.openai_client = openai_client
        
        # 2ë‹¨ê³„. ìœ í‹¸ë¦¬í‹° ì»´í¬ë„ŒíŠ¸ ìƒì„±
        self.text_processor = TextPreprocessor()
        
        # 3ë‹¨ê³„. ìµœì í™” ì‹œìŠ¤í…œ ì´ˆê¸°í™”
        self._initialize_optimization_system(redis_config) # â† REDIS_CONFIG ì‚¬ìš©
        
        # 4ë‹¨ê³„: AI ëª¨ë¸ ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™”
        self.unified_analyzer = UnifiedTextAnalyzer(openai_client)
        self.answer_generator = AnswerGenerator(openai_client)
        
        # 5ë‹¨ê³„: ì„œë¹„ìŠ¤ ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™” (ìµœì í™” ì ìš©)
        self.search_service = OptimizedSearchService(pinecone_index, self.api_manager)
        self.quality_validator = QualityValidator(openai_client)
        
        # 6ë‹¨ê³„: ë™ê¸°í™” ì„œë¹„ìŠ¤ ì´ˆê¸°í™” (ì¡°ê±´ë¶€)
        if connection_string and category_mapping:
            self.sync_service = SyncService(
                pinecone_index, 
                openai_client, 
                connection_string, 
                category_mapping
                )
        
        # 7ë‹¨ê³„: ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ì´ˆê¸°í™”
        self.performance_stats = {
            'total_requests': 0, # ì´ ìš”ì²­ ìˆ˜
            'cache_hit_rate': 0.0, # ìºì‹œ íˆíŠ¸ìœ¨
            'avg_processing_time': 0.0, # í‰ê·  ì²˜ë¦¬ ì‹œê°„
            'api_calls_saved': 0 # ì ˆì•½ëœ API í˜¸ì¶œ ìˆ˜
        }
        
        logging.info("ìµœì í™”ëœ AI ë‹µë³€ ìƒì„±ê¸° ì´ˆê¸°í™” ì™„ë£Œ")

    def _initialize_optimization_system(self, redis_config: Optional[Dict]):
        """ìµœì í™” ì‹œìŠ¤í…œ ì´ˆê¸°í™”"""
        # Redis ì„¤ì •
        if redis_config:
            self.cache_manager = CacheManager(
                redis_host=redis_config.get('host', 'localhost'),
                redis_port=redis_config.get('port', 6379),
                redis_db=redis_config.get('db', 0),
                redis_password=redis_config.get('password')
            )
        else:
            # ê¸°ë³¸ ì„¤ì • (ë¡œì»¬ Redis ë˜ëŠ” ë©”ëª¨ë¦¬ ìºì‹œ)
            self.cache_manager = CacheManager()
        
        # ë°°ì¹˜ í”„ë¡œì„¸ì„œ ì´ˆê¸°í™”
        self.batch_processor = BatchProcessor(
            max_workers=5,
            batch_size=10,
            batch_timeout=2.0
        )
        
        # ì§€ëŠ¥í˜• API ê´€ë¦¬ì ì´ˆê¸°í™”
        self.api_manager = IntelligentAPIManager(
            cache_manager=self.cache_manager,
            batch_processor=self.batch_processor,
            openai_client=self.openai_client
        )
        
        # ë°°ì¹˜ í”„ë¡œì„¸ì„œ ì‹œì‘
        self.batch_processor.start()
        
        logging.info("ìµœì í™” ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ")

    # ================================
    # ê¸°ì¡´ ì¸í„°í˜ì´ìŠ¤ í˜¸í™˜ì„± ë©”ì„œë“œë“¤ (ìµœì í™” ì ìš©)
    # ================================

    def detect_language(self, text: str) -> str:
        """ì–¸ì–´ ê°ì§€ (langdetect ê¸°ë°˜ - ì •í™•í•œ ì–¸ì–´ íŒ¨í„´ ë¶„ì„)"""
        try:
            # ===== 1ë‹¨ê³„: langdetect ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì‚¬ìš©í•œ ìë™ ì–¸ì–´ ê°ì§€ =====
            detected = detect(text)
            
            # ===== 2ë‹¨ê³„: ì§€ì› ì–¸ì–´ ê²€ì¦ (í•œêµ­ì–´/ì˜ì–´ë§Œ ì§€ì›) =====
            if detected == 'ko':
                return 'ko'                                   # í•œêµ­ì–´ë¡œ ê°ì§€ë¨
            elif detected == 'en':
                return 'en'                                   # ì˜ì–´ë¡œ ê°ì§€ë¨
            else:
                # ê¸°íƒ€ ì–¸ì–´ëŠ” ê¸°ë³¸ê°’(í•œêµ­ì–´)ìœ¼ë¡œ ì²˜ë¦¬
                return 'ko'
                
        except LangDetectException as e:
            logging.warning(f"langdetect ì–¸ì–´ ê°ì§€ ì‹¤íŒ¨: {e}, í´ë°± ë¡œì§ ì‚¬ìš©")
            
            # ===== 3ë‹¨ê³„: ê°ì§€ ì‹¤íŒ¨ì‹œ ê°œì„ ëœ ë¬¸ì ë¹„ìœ¨ ê¸°ë°˜ í´ë°± ë¡œì§ =====
            # ê¸°ë³¸ ë¬¸ì ì¹´ìš´íŠ¸
            korean_chars = len(re.findall(r'[ê°€-í£]', text))
            english_chars = len(re.findall(r'[a-zA-Z]', text))
            
            # í•œêµ­ì–´ ë¬¸ë²• íŒ¨í„´ ê°€ì¤‘ì¹˜ (ì¡°ì‚¬, ì–´ë¯¸ ë“±)
            korean_particles = len(re.findall(r'[ì„ë¥¼ì´ê°€ì—ì„œë¡œê³¼ì™€ì˜ë„ë§Œê¹Œì§€ë¶€í„°ê»˜ì„œì—ê²Œí•œí…Œ]', text))
            korean_endings = len(re.findall(r'ìŠµë‹ˆë‹¤|ì„¸ìš”|ì–´ìš”|ê² ì–´ìš”|ì•˜ì–´ìš”|ì—ˆì–´ìš”|í•˜ê²Œ|ì£¼ì„¸ìš”', text))
            
            # ê°€ì¤‘ì¹˜ ì ìš©í•œ ì ìˆ˜ ê³„ì‚°
            korean_score = korean_chars + (korean_particles * 2) + (korean_endings * 3)
            english_score = english_chars
            
            # ë¬¸ì ìˆ˜ ë¹„êµë¡œ ì–¸ì–´ íŒë‹¨ (ê°œì„ ëœ ë²„ì „)
            if korean_score > english_score:
                return 'ko'                                   # í•œêµ­ì–´ë¡œ íŒë‹¨
            else:
                return 'en'                                   # ì˜ì–´ë¡œ íŒë‹¨

    def preprocess_text(self, text: str) -> str:
        """í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ (ê¸°ì¡´ í˜¸í™˜)"""
        return self.text_processor.preprocess_text(text)

    def create_embedding(self, text: str):
        """ì„ë² ë”© ìƒì„± (ìºì‹± ì ìš©)"""
        request = APICallRequest(
            operation='embedding',
            data={'text': text},
            priority=3,
            strategy=APICallStrategy.CACHE_FIRST
        )
        
        response = self.api_manager.process_request(request)
        return response.data if response.success else None

    # analyze_question_intent ë©”ì„œë“œ ì œê±°ë¨ - unified_analyzer.analyze_and_correct()ë¡œ í†µí•©
    # ì˜ì–´ ì§ˆë¬¸ì˜ ê²½ìš°ì—ë§Œ ê°œë³„ í˜¸ì¶œì´ í•„ìš”í•˜ë¯€ë¡œ ê¸°ë³¸ê°’ ë°˜í™˜ ë©”ì„œë“œë¡œ ëŒ€ì²´
    def _get_default_intent_analysis(self, query: str) -> dict:
        """ê¸°ë³¸ ì˜ë„ ë¶„ì„ ê²°ê³¼ ë°˜í™˜ (ì˜ì–´ ì§ˆë¬¸ìš©)"""
        return {
            "core_intent": "general_inquiry",
            "intent_category": "ì¼ë°˜ë¬¸ì˜",
            "primary_action": "ê¸°íƒ€",
            "target_object": "ê¸°íƒ€",
            "constraint_conditions": [],
            "standardized_query": query,
            "semantic_keywords": [query[:20]],
            # ê¸°ì¡´ í˜¸í™˜ì„± í•„ë“œ
            "intent_type": "ì¼ë°˜ë¬¸ì˜",
            "main_topic": "ê¸°íƒ€",
            "specific_request": query[:100],
            "keywords": [query[:20]],
            "urgency": "medium",
            "action_type": "ê¸°íƒ€"
        }

    def search_similar_answers(self, query: str, top_k: int = 5, similarity_threshold: float = 0.7, lang: str = 'ko') -> list:
        """ìœ ì‚¬ ë‹µë³€ ê²€ìƒ‰ (ìµœì í™” ì ìš©)"""
        return self.search_service.search_similar_answers_optimized(query, top_k, lang)
    
    def search_similar_answers_with_cached_intent(self, query: str, cached_intent: Dict, top_k: int = 5, lang: str = 'ko') -> list:
        """ìºì‹œëœ ì˜ë„ ë¶„ì„ì„ í™œìš©í•œ ìœ ì‚¬ ë‹µë³€ ê²€ìƒ‰ (API í˜¸ì¶œ ì ˆì•½)"""
        return self.search_service.search_similar_answers_with_cached_intent(query, cached_intent, top_k, lang)

    def analyze_context_quality(self, similar_answers: list, query: str) -> dict:
        """ì»¨í…ìŠ¤íŠ¸ í’ˆì§ˆ ë¶„ì„ (ê¸°ì¡´ í˜¸í™˜)"""
        return self.search_service.analyze_context_quality(similar_answers, query)

    def get_best_fallback_answer(self, similar_answers: list, lang: str = 'ko') -> str:
        """ìµœì  í´ë°± ë‹µë³€ ì„ íƒ (ìµœì í™” ì ìš©)"""
        return self.search_service.get_best_fallback_answer(similar_answers, lang)

    def generate_with_enhanced_gpt(self, query: str, similar_answers: list, context_analysis: dict, lang: str = 'ko') -> str:
        """í–¥ìƒëœ GPT ìƒì„± (ê¸°ì¡´ í˜¸í™˜)"""
        return self.answer_generator.generate_with_enhanced_gpt(query, similar_answers, context_analysis, lang)

    def is_valid_text(self, text: str, lang: str = 'ko') -> bool:
        """í…ìŠ¤íŠ¸ ìœ íš¨ì„± ê²€ì¦ (ê¸°ì¡´ í˜¸í™˜)"""
        return self.quality_validator.is_valid_text(text, lang)

    def check_answer_completeness(self, answer: str, query: str, lang: str = 'ko') -> float:
        """ë‹µë³€ ì™„ì„±ë„ ê²€ì¦ (ê¸°ì¡´ í˜¸í™˜)"""
        return self.quality_validator.check_answer_completeness(answer, query, lang)

    def detect_empty_promises(self, answer: str, lang: str = 'ko') -> float:
        """ë¹ˆ ì•½ì† íŒ¨í„´ ê°ì§€ (ê¸°ì¡´ í˜¸í™˜)"""
        return self.quality_validator.detect_empty_promises(answer, lang)

    def detect_hallucination_and_inconsistency(self, answer: str, query: str, lang: str = 'ko') -> dict:
        """í• ë£¨ì‹œë„¤ì´ì…˜ ë° ì¼ê´€ì„± ê²€ì¦ (ê¸°ì¡´ í˜¸í™˜)"""
        return self.quality_validator.detect_hallucination_and_inconsistency(answer, query, lang)

    def validate_answer_relevance_ai(self, answer: str, query: str, question_analysis: dict) -> bool:
        """AI ê¸°ë°˜ ë‹µë³€ ê´€ë ¨ì„± ê²€ì¦ (ê¸°ì¡´ í˜¸í™˜)"""
        return self.quality_validator.validate_answer_relevance_ai(answer, query, question_analysis)

    # fix_korean_typos_with_ai ë©”ì„œë“œ ì œê±°ë¨ - unified_analyzer.analyze_and_correct()ë¡œ í†µí•©

    def translate_text(self, text: str, source_lang: str, target_lang: str) -> str:
        """ë²ˆì—­ (ìºì‹± ì ìš©)"""
        request = APICallRequest(
            operation='translation',
            data={'text': text, 'source_lang': source_lang, 'target_lang': target_lang},
            priority=4,
            strategy=APICallStrategy.CACHE_FIRST
        )
        
        response = self.api_manager.process_request(request)
        return response.data if response.success else text

    def remove_greeting_and_closing(self, text: str, lang: str = 'ko') -> str:
        """ì¸ì‚¬ë§/ëë§ºìŒë§ ì œê±° (ê¸°ì¡´ í˜¸í™˜)"""
        return self.answer_generator.remove_greeting_and_closing(text, lang)

    def format_answer_with_html_paragraphs(self, text: str, lang: str = 'ko') -> str:
        """HTML ë‹¨ë½ í¬ë§·íŒ… (ê¸°ì¡´ í˜¸í™˜)"""
        return self._format_answer_with_html_paragraphs(text, lang)

    def _format_answer_with_html_paragraphs(self, text: str, lang: str = 'ko') -> str:
        """ë‹µë³€ í…ìŠ¤íŠ¸ë¥¼ HTML ë‹¨ë½ í˜•ì‹ìœ¼ë¡œ í¬ë§·íŒ…í•˜ëŠ” ë©”ì„œë“œ"""
        if not text:
            return ""

        text = self.text_processor.remove_old_app_name(text)

        # ë¬¸ì¥ì„ ë§ˆì¹¨í‘œ, ëŠë‚Œí‘œ, ë¬¼ìŒí‘œë¡œ ë¶„ë¦¬
        sentences = re.split(r'(?<=[.!?])\s+', text)

        paragraphs = []
        current_paragraph = []

        # ë‹¨ë½ ë¶„ë¦¬ íŠ¸ë¦¬ê±° í‚¤ì›Œë“œë“¤
        if lang == 'ko':
            paragraph_triggers = [
                'ì•ˆë…•í•˜ì„¸ìš”', 'ê°ì‚¬í•©ë‹ˆë‹¤', 'ê°ì‚¬ë“œë¦½ë‹ˆë‹¤', 'ë°”ì´ë¸” ì• í”Œì„',
                'ë”°ë¼ì„œ', 'ê·¸ëŸ¬ë¯€ë¡œ', 'ë˜í•œ', 'ê·¸ë¦¬ê³ ', 'ë˜ëŠ”', 'í•˜ì§€ë§Œ', 'ê·¸ëŸ°ë°',
                'í˜„ì¬', 'ì§€ê¸ˆ', 'ë§Œì•½', 'í˜¹ì‹œ', 'ì„±ë„ë‹˜', 'ê³ ê°ë‹˜',
                'ê¸°ëŠ¥', 'ìŠ¤í”¼ì»¤', 'ë²„íŠ¼', 'ë©”ë‰´', 'í™”ë©´', 'ì„¤ì •'
            ]
        else:  # ì˜ì–´
            paragraph_triggers = [
                'Hello', 'Thank', 'Therefore', 'However', 'Additionally',
                'Currently', 'If', 'Please', 'Feature', 'Function'
            ]

        for i, sentence in enumerate(sentences):
            sentence = sentence.strip()
            if not sentence:
                continue

            # ì²« ë²ˆì§¸ ë¬¸ì¥ì€ í•­ìƒ ë³„ë„ ë‹¨ë½
            if i == 0:
                if current_paragraph:
                    paragraphs.append(' '.join(current_paragraph))
                    current_paragraph = []
                paragraphs.append(sentence)
                continue

            should_break = False

            # íŠ¸ë¦¬ê±° í‚¤ì›Œë“œë¡œ ì‹œì‘í•˜ëŠ” ë¬¸ì¥ì€ ìƒˆ ë‹¨ë½
            for trigger in paragraph_triggers:
                if sentence.startswith(trigger):
                    should_break = True
                    break

            # í˜„ì¬ ë‹¨ë½ì— 2ê°œ ì´ìƒ ë¬¸ì¥ì´ ìˆìœ¼ë©´ ìƒˆ ë‹¨ë½
            if current_paragraph and len(current_paragraph) >= 2:
                should_break = True

            # ìƒˆ ë‹¨ë½ ë¶„ë¦¬
            if should_break and current_paragraph:
                paragraphs.append(' '.join(current_paragraph))
                current_paragraph = [sentence]
            else:
                current_paragraph.append(sentence)

        # ë§ˆì§€ë§‰ ë‹¨ë½ ì¶”ê°€
        if current_paragraph:
            paragraphs.append(' '.join(current_paragraph))

        # HTML ë‹¨ë½ìœ¼ë¡œ ë³€í™˜
        html_paragraphs = []
        for i, paragraph in enumerate(paragraphs):
            html_paragraphs.append(f"<p>{paragraph}</p>")

            # ë‹¨ë½ ì‚¬ì´ì— ë¹ˆ ì¤„ ì¶”ê°€ (ë§ˆì§€ë§‰ ë‹¨ë½ ì œì™¸)
            if i < len(paragraphs) - 1:
                html_paragraphs.append("<p><br></p>")

        return ''.join(html_paragraphs)

    # ================================
    # ë©”ì¸ ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§ (ìµœì í™” ì ìš©)
    # ================================

    @profile
    def generate_ai_answer(self, query: str, similar_answers: list, lang: str) -> str:
        """ìµœì í™”ëœ AI ë‹µë³€ ìƒì„±"""
        
        # 1. ì–¸ì–´ ê°ì§€ (ë¹ ë¥¸ ë£° ê¸°ë°˜)
        if not lang or lang == 'auto':
            detected_lang = self.detect_language(query)
            lang = detected_lang
            logging.info(f"ê°ì§€ëœ ì–¸ì–´: {lang}")

        # 2. ìœ ì‚¬ ë‹µë³€ì´ ì—†ëŠ” ê²½ìš°
        if not similar_answers:
            logging.error("ìœ ì‚¬ ë‹µë³€ì´ ì „í˜€ ì—†ìŒ")
            if lang == 'en':
                default_msg = "<p>We need more detailed information to provide an accurate answer to your inquiry.</p><p><br></p><p>Please contact our customer service center for prompt assistance.</p>"
            else:
                default_msg = "<p>ì•ˆë…•í•˜ì„¸ìš”, GOODTV ë°”ì´ë¸” ì• í”Œì…ë‹ˆë‹¤.</p><p><br></p><p>ë°”ì´ë¸” ì• í”Œì„ ì´ìš©í•´ ì£¼ì…”ì„œ ì§„ì‹¬ìœ¼ë¡œ ê°ì‚¬ë“œë¦½ë‹ˆë‹¤.</p><p><br></p><p>ë‚¨ê²¨ì£¼ì‹  ë¬¸ì˜ëŠ” í˜„ì¬ ë‹´ë‹¹ìê°€ ì§ì ‘ í™•ì¸í•˜ê³  ìˆìŠµë‹ˆë‹¤.</p><p><br></p><p>ì„±ë„ë‹˜ê»˜ ë„ì›€ì´ ë  ìˆ˜ ìˆë„ë¡ ë‚´ìš©ì„ ê¼¼ê¼¼íˆ ì‚´í´ë³´ê³  ì •í™•í•˜ê³  êµ¬ì²´ì ì¸ ë‹µë³€ì„ ì¤€ë¹„í•˜ê² ìŠµë‹ˆë‹¤.</p><p><br></p><p>ë‹µë³€ì€ ìµœëŒ€ í•˜ë£¨ ì´ë‚´ì— ë“œë¦´ ì˜ˆì •ì´ì˜¤ë‹ˆ ì¡°ê¸ˆë§Œ ê¸°ë‹¤ë ¤ ì£¼ì‹œë©´ ê°ì‚¬í•˜ê² ìŠµë‹ˆë‹¤.</p><p><br></p><p>í•­ìƒ ì£¼ë‹˜ ì•ˆì—ì„œ í‰ì•ˆí•˜ì„¸ìš”, ê°ì‚¬í•©ë‹ˆë‹¤.</p>"
            return default_msg

        # 3. ì»¨í…ìŠ¤íŠ¸ ë¶„ì„
        context_analysis = self.analyze_context_quality(similar_answers, query)

        try:
            approach = context_analysis['recommended_approach']
            logging.info(f"ì„ íƒëœ ì ‘ê·¼ ë°©ì‹: {approach}, ì–¸ì–´: {lang}")

            base_answer = ""

            if approach == 'direct_use':
                base_answer = self.get_best_fallback_answer(similar_answers[:3], lang)

            elif approach in ['gpt_with_strong_context', 'gpt_with_weak_context']:
                base_answer = self.generate_with_enhanced_gpt(query, similar_answers, context_analysis, lang)

                if not base_answer or not self.is_valid_text(base_answer, lang):
                    logging.warning("GPT ìƒì„± ì‹¤íŒ¨, í´ë°± ë‹µë³€ ì‚¬ìš©")
                    base_answer = self.get_best_fallback_answer(similar_answers, lang)

            else:
                base_answer = self.get_best_fallback_answer(similar_answers, lang)

            # ìµœì¢… ê²€ì¦ ë° í’ˆì§ˆ í–¥ìƒ
            if not base_answer:
                if lang == 'en':
                    return "<p>We need more detailed information to provide an accurate answer to your inquiry.</p><p><br></p><p>Please contact our customer service center for prompt assistance.</p>"
                else:
                    return "<p>ì•ˆë…•í•˜ì„¸ìš”, GOODTV ë°”ì´ë¸” ì• í”Œì…ë‹ˆë‹¤.</p><p><br></p><p>ë°”ì´ë¸” ì• í”Œì„ ì´ìš©í•´ ì£¼ì…”ì„œ ì§„ì‹¬ìœ¼ë¡œ ê°ì‚¬ë“œë¦½ë‹ˆë‹¤.</p><p><br></p><p>ë‚¨ê²¨ì£¼ì‹  ë¬¸ì˜ëŠ” í˜„ì¬ ë‹´ë‹¹ìê°€ ì§ì ‘ í™•ì¸í•˜ê³  ìˆìŠµë‹ˆë‹¤.</p><p><br></p><p>ì„±ë„ë‹˜ê»˜ ë„ì›€ì´ ë  ìˆ˜ ìˆë„ë¡ ë‚´ìš©ì„ ê¼¼ê¼¼íˆ ì‚´í´ë³´ê³  ì •í™•í•˜ê³  êµ¬ì²´ì ì¸ ë‹µë³€ì„ ì¤€ë¹„í•˜ê² ìŠµë‹ˆë‹¤.</p><p><br></p><p>ë‹µë³€ì€ ìµœëŒ€ í•˜ë£¨ ì´ë‚´ì— ë“œë¦´ ì˜ˆì •ì´ì˜¤ë‹ˆ ì¡°ê¸ˆë§Œ ê¸°ë‹¤ë ¤ ì£¼ì‹œë©´ ê°ì‚¬í•˜ê² ìŠµë‹ˆë‹¤.</p><p><br></p><p>í•­ìƒ ì£¼ë‹˜ ì•ˆì—ì„œ í‰ì•ˆí•˜ì„¸ìš”, ê°ì‚¬í•©ë‹ˆë‹¤.</p>"

            # ìê°€ í‰ê°€ ë¡œì§ ì¶”ê°€
            if base_answer:
                coherence_score = self._evaluate_semantic_coherence(query, base_answer)
                
                # ì„ê³„ê°’ì„ 0.3ìœ¼ë¡œ ë‚®ì¶”ê³ , ì¬ìƒì„± ì‹¤íŒ¨ì‹œ ì›ë³¸ ìœ ì§€
                if coherence_score < 0.3:  # 0.5 â†’ 0.3ìœ¼ë¡œ ì™„í™”
                    logging.info(f"ë‚®ì€ ì¼ê´€ì„± ì ìˆ˜ ({coherence_score:.2f}), ë‹µë³€ ì¬ìƒì„± ì‹œë„")
                    
                    # ì›ë³¸ ë‹µë³€ ë°±ì—…
                    original_answer = base_answer
                    
                    # ê´€ë ¨ì„± ë‚®ì€ ë‹µë³€ í•„í„°ë§ í›„ ì¬ìƒì„±
                    filtered_answers = self._filter_by_coherence(query, similar_answers)
                    if filtered_answers and len(filtered_answers) >= 2:
                        new_answer = self.generate_with_enhanced_gpt(
                            query, filtered_answers[:3], context_analysis, lang
                        )
                        
                        # ì¬ìƒì„±ëœ ë‹µë³€ì´ ìœ íš¨í•œ ê²½ìš°ì—ë§Œ ì‚¬ìš©
                        if new_answer and len(new_answer) > 50:  # ìµœì†Œ ê¸¸ì´ ì²´í¬
                            base_answer = new_answer
                            logging.info(f"ì¬ìƒì„± ì„±ê³µ, í•„í„°ë§ëœ ë‹µë³€ {len(filtered_answers)}ê°œ ì‚¬ìš©")
                        else:
                            # ì¬ìƒì„± ì‹¤íŒ¨ì‹œ ì›ë³¸ ìœ ì§€
                            base_answer = original_answer
                            logging.warning("ì¬ìƒì„± ì‹¤íŒ¨, ì›ë³¸ ë‹µë³€ ìœ ì§€")
                    else:
                        logging.warning(f"í•„í„°ë§ëœ ë‹µë³€ ë¶€ì¡± ({len(filtered_answers)}ê°œ), ì›ë³¸ ìœ ì§€")

            # ê°•í™”ëœ ë‹µë³€ ì™„ì„±ë„ ê²€ì¦ ë° ì¬ìƒì„± ë¡œì§
            base_completeness = self.check_answer_completeness(base_answer, query, lang)
            empty_promise_score = self.detect_empty_promises(base_answer, lang)
            final_hallucination_check = self.detect_hallucination_and_inconsistency(base_answer, query, lang)
            final_hallucination_score = final_hallucination_check['overall_score']

            # í• ë£¨ì‹œë„¤ì´ì…˜ì´ ì¹˜ëª…ì ì´ë©´ ì¦‰ì‹œ í´ë°±ìœ¼ë¡œ ë³€ê²½
            if final_hallucination_score < 0.3:
                logging.error("ìµœì¢… ë‹µë³€ì—ì„œ ì¹˜ëª…ì  í• ë£¨ì‹œë„¤ì´ì…˜ ê°ì§€! í´ë°± ë‹µë³€ìœ¼ë¡œ ê°•ì œ ë³€ê²½")
                base_answer = self.get_best_fallback_answer(similar_answers, lang)

            # ì¬ìƒì„± ì¡°ê±´ ê²€ì‚¬
            needs_regeneration = (
                base_completeness < 0.6 or
                empty_promise_score < 0.3 or
                final_hallucination_score < 0.5
            )

            if needs_regeneration and approach in ['gpt_with_strong_context', 'gpt_with_weak_context']:
                # ì¬ìƒì„± ì‹œë„ (ìµœëŒ€ 2íšŒ)
                for attempt in range(2):
                    retry_analysis = context_analysis.copy()
                    retry_analysis['recommended_approach'] = 'gpt_with_strong_context'

                    retry_answer = self.generate_with_enhanced_gpt(query, similar_answers, retry_analysis, lang)
                    if retry_answer:
                        retry_completeness = self.check_answer_completeness(retry_answer, query, lang)
                        retry_empty_promise = self.detect_empty_promises(retry_answer, lang)
                        retry_hallucination_check = self.detect_hallucination_and_inconsistency(retry_answer, query, lang)
                        retry_hallucination_score = retry_hallucination_check['overall_score']

                        # ì¬ìƒì„± ë‹µë³€ì— ì¹˜ëª…ì  í• ë£¨ì‹œë„¤ì´ì…˜ì´ ìˆìœ¼ë©´ ì‚¬ìš©í•˜ì§€ ì•ŠìŒ
                        if retry_hallucination_score < 0.3:
                            continue

                        # ì¬ìƒì„± ë‹µë³€ì´ ë” ë‚˜ì€ì§€ í™•ì¸
                        is_better = (
                            retry_completeness > base_completeness and
                            retry_empty_promise > empty_promise_score and
                            retry_hallucination_score > final_hallucination_score
                        )

                        if is_better:
                            base_answer = retry_answer
                            break

            # ì–¸ì–´ë³„ í¬ë§·íŒ…
            if lang == 'en':
                # ì˜ì–´ ë‹µë³€ í¬ë§·íŒ…
                base_answer = self.text_processor.remove_old_app_name(base_answer)

                # ê¸°ì¡´ ì¸ì‚¬ë§/ëë§ºìŒë§ ì œê±°
                base_answer = re.sub(r'^Hello[^.]*\.\s*', '', base_answer, flags=re.IGNORECASE)
                base_answer = re.sub(r'^This is GOODTV Bible App[^.]*\.\s*', '', base_answer, flags=re.IGNORECASE)
                base_answer = re.sub(r'\s*Thank you[^.]*\.?\s*$', '', base_answer, flags=re.IGNORECASE)
                base_answer = re.sub(r'\s*Best regards[^.]*\.?\s*$', '', base_answer, flags=re.IGNORECASE)
                base_answer = re.sub(r'\s*God bless[^.]*\.?\s*$', '', base_answer, flags=re.IGNORECASE)

                formatted_body = self.format_answer_with_html_paragraphs(base_answer.strip(), 'en')

                final_answer = "<p>Hello, this is GOODTV Bible Apple App customer service team.</p><p><br></p><p>Thank you very much for using our app and for taking the time to contact us.</p><p><br></p>"
                final_answer += formatted_body
                final_answer += "<p><br></p><p>Thank you once again for sharing your thoughts with us!</p><p><br></p><p>May God's peace and grace always be with you.</p>"

            else:  # í•œêµ­ì–´
                # í•œêµ­ì–´ ë‹µë³€ ìµœì¢… í¬ë§·íŒ…
                base_answer = self.text_processor.remove_old_app_name(base_answer)
                base_answer = re.sub(r'ê³ ê°ë‹˜', 'ì„±ë„ë‹˜', base_answer)

                # ê¸°ì¡´ ì¸ì‚¬ë§/ëë§ºìŒë§ ì œê±°
                base_answer = re.sub(r'^ì•ˆë…•í•˜ì„¸ìš”[^.]*\.\s*', '', base_answer, flags=re.IGNORECASE)
                base_answer = re.sub(r'^GOODTV\s+ë°”ì´ë¸”\s*ì• í”Œ[^.]*\.\s*', '', base_answer, flags=re.IGNORECASE)
                base_answer = re.sub(r'^ë°”ì´ë¸”\s*ì• í”Œ[^.]*\.\s*', '', base_answer, flags=re.IGNORECASE)
                base_answer = re.sub(r'ê³ ê°ì„¼í„°[^.]*\.\s*', '', base_answer, flags=re.IGNORECASE)

                # ëë§ºìŒë§ ì œê±°
                base_answer = re.sub(r'\s*ê°ì‚¬í•©ë‹ˆë‹¤[^.]*\.?\s*$', '', base_answer, flags=re.IGNORECASE)
                base_answer = re.sub(r'\s*í‰ì•ˆí•˜ì„¸ìš”[^.]*\.?\s*$', '', base_answer, flags=re.IGNORECASE)
                base_answer = re.sub(r'\s*ì£¼ë‹˜\s*ì•ˆì—ì„œ[^.]*\.?\s*$', '', base_answer, flags=re.IGNORECASE)

                # êµ¬ ì•± ì´ë¦„ì„ ë°”ì´ë¸” ì• í”Œë¡œ ì™„ì „ êµì²´
                base_answer = re.sub(r'ë°”ì´ë¸”\s*ì• í”Œ\s*\(êµ¬\)\s*ë‹¤ë²ˆì—­\s*ì„±ê²½\s*ì°¬ì†¡', 'ë°”ì´ë¸” ì• í”Œ', base_answer, flags=re.IGNORECASE)
                base_answer = re.sub(r'ë‹¤ë²ˆì—­\s*ì„±ê²½\s*ì°¬ì†¡', 'ë°”ì´ë¸” ì• í”Œ', base_answer, flags=re.IGNORECASE)

                # ì¤‘ë³µ ëë§ºìŒë§ ì œê±°
                base_answer = re.sub(r'í•­ìƒ\s*ì„±ë„ë‹˜ë“¤?ê»˜\s*ì¢‹ì€\s*(ì„œë¹„ìŠ¤|ì„±ê²½ì•±)ì„?\s*ì œê³µí•˜ê¸°\s*ìœ„í•´\s*ë…¸ë ¥í•˜ëŠ”\s*ë°”ì´ë¸”\s*ì• í”Œì´\s*ë˜ê² ìŠµë‹ˆë‹¤\.?\s*', '', base_answer, flags=re.IGNORECASE)
                base_answer = re.sub(r'\s*í•­ìƒ\s*$', '', base_answer, flags=re.IGNORECASE)

                # ë³¸ë¬¸ì„ HTML ë‹¨ë½ í˜•ì‹ìœ¼ë¡œ í¬ë§·íŒ…
                formatted_body = self.format_answer_with_html_paragraphs(base_answer.strip(), 'ko')

                # í•œêµ­ì–´ ê³ ì • ì¸ì‚¬ë§ ë° ëë§ºìŒë§
                final_answer = "<p>ì•ˆë…•í•˜ì„¸ìš”. GOODTV ë°”ì´ë¸” ì• í”Œì…ë‹ˆë‹¤.</p><p><br></p><p>ë°”ì´ë¸” ì• í”Œì„ ì´ìš©í•´ì£¼ì…”ì„œ ê°ì‚¬ë“œë¦½ë‹ˆë‹¤.</p><p><br></p>"

                # HTML í¬ë§·íŒ… í›„ ì™„ì „í•œ ì •ë¦¬ ì‘ì—…
                formatted_body = re.sub(r'<p>\s*í•­ìƒ\s*ì„±ë„ë‹˜ë“¤?ê»˜\s*ì¢‹ì€\s*(ì„œë¹„ìŠ¤|ì„±ê²½ì•±)ì„?\s*ì œê³µí•˜ê¸°\s*ìœ„í•´\s*ë…¸ë ¥í•˜ëŠ”\s*ë°”ì´ë¸”\s*ì• í”Œì´\s*ë˜ê² ìŠµë‹ˆë‹¤\.?\s*</p>', '', formatted_body, flags=re.IGNORECASE)
                formatted_body = re.sub(r'<p>\s*ê°ì‚¬í•©ë‹ˆë‹¤\.?\s*(ì£¼ë‹˜\s*ì•ˆì—ì„œ\s*í‰ì•ˆí•˜ì„¸ìš”\.?)?\s*</p>', '', formatted_body, flags=re.IGNORECASE)
                formatted_body = re.sub(r'<p>\s*í•­ìƒ\s*</p>', '', formatted_body, flags=re.IGNORECASE)
                formatted_body = re.sub(r'(<p><br></p>\s*){3,}', '<p><br></p><p><br></p>', formatted_body)
                formatted_body = re.sub(r'(<p><br></p>\s*)+$', '', formatted_body)

                final_answer += formatted_body
                final_answer += "<p><br></p><p>í•­ìƒ ì„±ë„ë‹˜ê»˜ ì¢‹ì€ ì„±ê²½ì•±ì„ ì œê³µí•˜ê¸° ìœ„í•´ ë…¸ë ¥í•˜ëŠ” ë°”ì´ë¸” ì• í”Œì´ ë˜ê² ìŠµë‹ˆë‹¤.</p><p><br></p><p>ê°ì‚¬í•©ë‹ˆë‹¤. ì£¼ë‹˜ ì•ˆì—ì„œ í‰ì•ˆí•˜ì„¸ìš”.</p>"

            logging.info(f"ìµœì¢… ë‹µë³€ ìƒì„± ì™„ë£Œ: {len(final_answer)}ì, ì ‘ê·¼ë°©ì‹: {approach}, ì–¸ì–´: {lang}")
            return final_answer

        except Exception as e:
            logging.error(f"ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜: {e}")
            if lang == 'en':
                return "<p>Sorry, we cannot generate an answer at this moment.</p><p><br></p><p>Please contact our customer service center.</p>"
            else:
                return "<p>ì•ˆë…•í•˜ì„¸ìš”, GOODTV ë°”ì´ë¸” ì• í”Œì…ë‹ˆë‹¤.</p><p><br></p><p>ë°”ì´ë¸” ì• í”Œì„ ì´ìš©í•´ ì£¼ì…”ì„œ ì§„ì‹¬ìœ¼ë¡œ ê°ì‚¬ë“œë¦½ë‹ˆë‹¤.</p><p><br></p><p>ë‚¨ê²¨ì£¼ì‹  ë¬¸ì˜ëŠ” í˜„ì¬ ë‹´ë‹¹ìê°€ ì§ì ‘ í™•ì¸í•˜ê³  ìˆìŠµë‹ˆë‹¤.</p><p><br></p><p>ì„±ë„ë‹˜ê»˜ ë„ì›€ì´ ë  ìˆ˜ ìˆë„ë¡ ë‚´ìš©ì„ ê¼¼ê¼¼íˆ ì‚´í´ë³´ê³  ì •í™•í•˜ê³  êµ¬ì²´ì ì¸ ë‹µë³€ì„ ì¤€ë¹„í•˜ê² ìŠµë‹ˆë‹¤.</p><p><br></p><p>ë‹µë³€ì€ ìµœëŒ€ í•˜ë£¨ ì´ë‚´ì— ë“œë¦´ ì˜ˆì •ì´ì˜¤ë‹ˆ ì¡°ê¸ˆë§Œ ê¸°ë‹¤ë ¤ ì£¼ì‹œë©´ ê°ì‚¬í•˜ê² ìŠµë‹ˆë‹¤.</p><p><br></p><p>í•­ìƒ ì£¼ë‹˜ ì•ˆì—ì„œ í‰ì•ˆí•˜ì„¸ìš”, ê°ì‚¬í•©ë‹ˆë‹¤.</p>"

    # â˜† 2. ê¸°ë³¸ ì „ì²˜ë¦¬ ë©”ì„œë“œ
    def process(self, seq: int, question: str, lang: str) -> dict:
        """ìµœì í™”ëœ ë©”ì¸ ì²˜ë¦¬ ë©”ì„œë“œ"""
        start_time = time.time() # í˜„ì¬ ì‹œê°„ì„ íƒ€ì„ìŠ¤íƒ¬í”„ë¡œ ê¸°ë¡í•˜ëŠ” ì½”ë“œ
        
        try:
            with memory_cleanup():
                # ì„±ëŠ¥ í†µê³„ ì—…ë°ì´íŠ¸
                self.performance_stats['total_requests'] += 1
                
                # 1ë‹¨ê³„. ì „ì²˜ë¦¬
                processed_question = self.preprocess_text(question)

                # 2ë‹¨ê³„. í†µí•© ë¶„ì„ (ì˜¤íƒ€ ìˆ˜ì • + ì˜ë„ ë¶„ì„) - API í˜¸ì¶œ 1íšŒë¡œ í†µí•©
                if lang == 'ko' or lang == 'auto':
                    corrected_text, intent_analysis = self.unified_analyzer.analyze_and_correct(processed_question)
                    processed_question = corrected_text
                    
                    print("="*80)
                    print("ğŸ” [í†µí•© ë¶„ì„ ê²°ê³¼]")
                    print(f"ì›ë³¸ ì§ˆë¬¸: {processed_question}")
                    print(f"ìˆ˜ì •ëœ ì§ˆë¬¸: {corrected_text}")
                    print(f"ì˜ë„ ë¶„ì„ JSON: {json.dumps(intent_analysis, ensure_ascii=False, indent=2)}")
                    print("="*80)
                    
                    # ë¡œê·¸ íŒŒì¼ì—ë„ ê¸°ë¡
                    logging.info(f"í†µí•© ë¶„ì„ - ì›ë³¸: {processed_question}")
                    logging.info(f"í†µí•© ë¶„ì„ - ìˆ˜ì •: {corrected_text}")
                    logging.info(f"í†µí•© ë¶„ì„ - ì˜ë„: {json.dumps(intent_analysis, ensure_ascii=False)}")

                    # ì˜ë„ ë¶„ì„ ê²°ê³¼ë¥¼ ì„ì‹œ ì €ì¥ (ê²€ìƒ‰ ë‹¨ê³„ì—ì„œ ì¬ì‚¬ìš©)
                    self._cached_intent_analysis = intent_analysis
                    if processed_question != question:
                        logging.info(f"í†µí•© ë¶„ì„ - ì˜¤íƒ€ ìˆ˜ì •: {question[:50]} â†’ {processed_question[:50]}")
                        logging.info(f"í†µí•© ë¶„ì„ - ì˜ë„: {intent_analysis.get('core_intent', 'N/A')}")
                else:
                    # ì˜ì–´ì¸ ê²½ìš° ê¸°ë³¸ ì˜ë„ ë¶„ì„ ì‚¬ìš© (GPT í˜¸ì¶œ ìƒëµ)
                    self._cached_intent_analysis = self._get_default_intent_analysis(processed_question)

                if not processed_question:
                    return {"success": False, "error": "ì§ˆë¬¸ì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤."}

                # ì–¸ì–´ ìë™ ê°ì§€
                if not lang or lang == 'auto':
                    lang = self.detect_language(processed_question)
                    logging.info(f"ìë™ ê°ì§€ëœ ì–¸ì–´: {lang}")

                logging.info(f"ì²˜ë¦¬ ì‹œì‘ - SEQ: {seq}, ì–¸ì–´: {lang}, ì§ˆë¬¸: {processed_question[:50]}...")

                # 3ë‹¨ê³„. ìœ ì‚¬ ë‹µë³€ ê²€ìƒ‰ (ìºì‹œëœ ì˜ë„ ë¶„ì„ í™œìš©)
                similar_answers = self.search_similar_answers_with_cached_intent(
                    processed_question, self._cached_intent_analysis, lang=lang)

                # AI ë‹µë³€ ìƒì„±
                ai_answer = self.generate_ai_answer(processed_question, similar_answers, lang)

                # íŠ¹ìˆ˜ë¬¸ì ì •ë¦¬
                ai_answer = ai_answer.replace('"', '"').replace('"', '"')
                ai_answer = ai_answer.replace(''', "'").replace(''', "'")

                # ì„±ëŠ¥ í†µê³„ ì—…ë°ì´íŠ¸
                processing_time = time.time() - start_time
                self._update_performance_stats(processing_time)

                result = {
                    "success": True,
                    "answer": ai_answer,
                    "similar_count": len(similar_answers),
                    "embedding_model": "text-embedding-3-small",
                    "generation_model": "gpt-5-mini",
                    "detected_language": lang,
                    "processing_time": processing_time,
                    "optimization_stats": self.get_optimization_summary()
                }

                logging.info(f"ì²˜ë¦¬ ì™„ë£Œ - SEQ: {seq}, ì–¸ì–´: {lang}, ì‹œê°„: {processing_time:.2f}s")
                return result

        except Exception as e:
            logging.error(f"ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ - SEQ: {seq}, ì˜¤ë¥˜: {str(e)}")
            return {"success": False, "error": str(e)}

    def _evaluate_semantic_coherence(self, query: str, answer: str) -> float:
        """ì˜ë¯¸ì  ì¼ê´€ì„± í‰ê°€"""
        try:
            query_embedding = self.create_embedding(query)
            
            # HTML íƒœê·¸ ì œê±° í›„ ì„ë² ë”© ìƒì„±
            clean_answer = self.text_processor.preprocess_text(answer)
            answer_embedding = self.create_embedding(clean_answer)
            
            if query_embedding and answer_embedding:
                # numpy ë°°ì—´ë¡œ ë³€í™˜
                q_vec = np.array(query_embedding)
                a_vec = np.array(answer_embedding)
                
                # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
                similarity = np.dot(q_vec, a_vec) / (np.linalg.norm(q_vec) * np.linalg.norm(a_vec))
                return float(similarity)
        except Exception as e:
            logging.error(f"ì¼ê´€ì„± í‰ê°€ ì‹¤íŒ¨: {e}")
        
        return 0.5  # ê¸°ë³¸ê°’

    def _filter_by_coherence(self, query: str, similar_answers: list) -> list:
        """ì¼ê´€ì„± ê¸°ë°˜ í•„í„°ë§ - ë” ê´€ëŒ€í•œ ë²„ì „"""
        try:
            query_embedding = self.create_embedding(query)
            if not query_embedding:
                return similar_answers
            
            q_vec = np.array(query_embedding)
            filtered = []
            
            for answer in similar_answers:
                try:
                    answer_embedding = self.create_embedding(answer['question'])
                    if answer_embedding:
                        a_vec = np.array(answer_embedding)
                        similarity = np.dot(q_vec, a_vec) / (np.linalg.norm(q_vec) * np.linalg.norm(a_vec))
                        
                        # ì„ê³„ê°’ì„ 0.4ë¡œ ë‚®ì¶¤ (0.6 â†’ 0.4)
                        if similarity > 0.4:
                            filtered.append(answer)
                except Exception as e:
                    logging.error(f"ë‹µë³€ í•„í„°ë§ ì¤‘ ì˜¤ë¥˜: {e}")
                    continue
            
            # í•„í„°ë§ëœ ë‹µë³€ì´ ë„ˆë¬´ ì ìœ¼ë©´ ìƒìœ„ ë‹µë³€ ì¶”ê°€
            if len(filtered) < 3:
                # ì›ë³¸ ë‹µë³€ ì¤‘ í•„í„°ë§ë˜ì§€ ì•Šì€ ê²ƒë“¤ ì¶”ê°€
                for answer in similar_answers:
                    if answer not in filtered:
                        filtered.append(answer)
                    if len(filtered) >= 5:  # ìµœëŒ€ 5ê°œê¹Œì§€
                        break
            
            logging.info(f"ì¼ê´€ì„± í•„í„°ë§: {len(similar_answers)}ê°œ â†’ {len(filtered)}ê°œ")
            return filtered
        
        except Exception as e:
            logging.error(f"ì¼ê´€ì„± í•„í„°ë§ ì‹¤íŒ¨: {e}")
            return similar_answers

    def _update_performance_stats(self, processing_time: float):
        """ì„±ëŠ¥ í†µê³„ ì—…ë°ì´íŠ¸"""
        api_stats = self.api_manager.get_performance_stats()
        
        self.performance_stats['cache_hit_rate'] = api_stats['cache_hit_rate']
        self.performance_stats['api_calls_saved'] = api_stats['api_calls_saved']
        
        # í‰ê·  ì²˜ë¦¬ ì‹œê°„ ê³„ì‚°
        total_requests = self.performance_stats['total_requests']
        current_avg = self.performance_stats['avg_processing_time']
        self.performance_stats['avg_processing_time'] = (
            (current_avg * (total_requests - 1) + processing_time) / total_requests
        )

    def get_optimization_summary(self) -> Dict:
        """ìµœì í™” ìš”ì•½ ì •ë³´"""
        api_stats = self.api_manager.get_performance_stats()
        search_stats = self.search_service.get_optimization_stats()
        
        return {
            'cache_hit_rate': api_stats['cache_hit_rate'],
            'api_calls_saved': api_stats['api_calls_saved'],
            'batch_processed': api_stats['batch_processed'],
            'avg_processing_time': self.performance_stats['avg_processing_time'],
            'embedding_cache_size': search_stats['embedding_cache_size']
        }

    def get_detailed_performance_stats(self) -> Dict:
        """ìƒì„¸ ì„±ëŠ¥ í†µê³„"""
        return {
            'performance_stats': self.performance_stats,
            'api_manager_stats': self.api_manager.get_performance_stats(),
            'search_stats': self.search_service.get_optimization_stats(),
            'cache_stats': self.cache_manager.get_cache_stats(),
            'batch_stats': self.batch_processor.get_stats(),
            'system_health': self.api_manager.health_check()
        }

    def optimize_for_production(self):
        """í”„ë¡œë•ì…˜ í™˜ê²½ ìµœì í™” ì„¤ì •"""
        # API ê´€ë¦¬ì ìµœì í™”
        self.api_manager.optimize_settings(
            enable_smart_caching=True, # ìºì‹± í™œì„±í™”
            enable_batch_processing=True, # ë°°ì¹˜ ì²˜ë¦¬ í™œì„±í™”
            min_batch_size=3, # ìµœì†Œ ë°°ì¹˜ í¬ê¸°
            max_wait_time=1.5, # ìµœëŒ€ ëŒ€ê¸° ì‹œê°„
            cache_hit_bonus=0.9 # ìºì‹œ íˆíŠ¸ ë³´ë„ˆìŠ¤
        )
        
        # ê²€ìƒ‰ ì„œë¹„ìŠ¤ ìµœì í™”
        self.search_service.update_search_config(
            adaptive_layer_count=True, # ë™ì  ë ˆì´ì–´ ì¹´ìš´íŠ¸ í™œì„±í™”
            early_termination=True, # ì¡°ê¸° ì¢…ë£Œ í™œì„±í™”
            similarity_threshold=0.8, # ìœ ì‚¬ë„ ì„ê³„ê°’
            enable_result_caching=True # ê²°ê³¼ ìºì‹± í™œì„±í™”
        )
        
        logging.info("í”„ë¡œë•ì…˜ ìµœì í™” ì„¤ì • ì ìš© ì™„ë£Œ")

    def cleanup(self):
        """ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
        try:
            if hasattr(self, 'batch_processor'):
                self.batch_processor.stop()
            
            if hasattr(self, 'search_service'):
                self.search_service.clear_caches()
            
            logging.info("ìµœì í™”ëœ AI ìƒì„±ê¸° ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì™„ë£Œ")
        except Exception as e:
            logging.error(f"ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")

    def __del__(self):
        """ì†Œë©¸ì"""
        self.cleanup()
