#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
ì§ˆë¬¸ ë¶„ì„ ëª¨ë¸ ëª¨ë“ˆ
- ì‚¬ìš©ì ì§ˆë¬¸ì˜ ì–¸ì–´ ê°ì§€ ë° ì˜ë„ ë¶„ì„
- ì˜ë¯¸ë¡ ì  ìœ ì‚¬ì„± ê³„ì‚°ì„ í†µí•œ ë‹µë³€ ë§¤ì¹­ ìµœì í™”
- GPT ê¸°ë°˜ ì§€ëŠ¥í˜• ì§ˆë¬¸ ë¶„ì„ ì‹œìŠ¤í…œ
"""

import json
import logging
import re
from typing import Dict
from langdetect import detect, LangDetectException
from src.utils.memory_manager import memory_cleanup

# ===== ì§ˆë¬¸ ë¶„ì„ ë° ì˜ë„ íŒŒì•…ì„ ë‹´ë‹¹í•˜ëŠ” ë©”ì¸ í´ë˜ìŠ¤ =====
class QuestionAnalyzer:
    
    # QuestionAnalyzer ì´ˆê¸°í™”
    # Args:
    #     openai_client: OpenAI API í´ë¼ì´ì–¸íŠ¸ ì¸ìŠ¤í„´ìŠ¤
    def __init__(self, openai_client):
        self.openai_client = openai_client                    # GPT ë¶„ì„ì„ ìœ„í•œ OpenAI í´ë¼ì´ì–¸íŠ¸
    
    # í…ìŠ¤íŠ¸ì˜ ì–¸ì–´ë¥¼ ìë™ ê°ì§€í•˜ëŠ” ë©”ì„œë“œ
    # Args:
    #     text: ì–¸ì–´ë¥¼ ê°ì§€í•  í…ìŠ¤íŠ¸
    # Returns:
    #     str: ê°ì§€ëœ ì–¸ì–´ ì½”ë“œ ('ko' ë˜ëŠ” 'en')
    def detect_language(self, text: str) -> str:
        try:
            # ===== 1ë‹¨ê³„: langdetect ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì‚¬ìš©í•œ ìë™ ì–¸ì–´ ê°ì§€ =====
            detected = detect(text)
            
            # ===== 2ë‹¨ê³„: ì§€ì› ì–¸ì–´ ê²€ì¦ (í•œêµ­ì–´/ì˜ì–´ë§Œ ì§€ì›) =====
            if detected == 'en':
                return 'en'                                   # ì˜ì–´ë¡œ ê°ì§€ë¨
            elif detected == 'ko':
                return 'ko'                                   # í•œêµ­ì–´ë¡œ ê°ì§€ë¨
            else:
                # ê¸°íƒ€ ì–¸ì–´ëŠ” ê¸°ë³¸ê°’(í•œêµ­ì–´)ìœ¼ë¡œ ì²˜ë¦¬
                return 'ko'
                
        except LangDetectException:
            # ===== 3ë‹¨ê³„: ê°ì§€ ì‹¤íŒ¨ì‹œ ë¬¸ì ë¹„ìœ¨ ê¸°ë°˜ í´ë°± ë¡œì§ =====
            # í…ìŠ¤íŠ¸ ë‚´ í•œê¸€ê³¼ ì˜ë¬¸ ë¬¸ì ìˆ˜ë¥¼ ì§ì ‘ ì¹´ìš´íŠ¸
            korean_chars = len(re.findall(r'[ê°€-í£]', text))  # í•œê¸€ ë¬¸ì ìˆ˜
            english_chars = len(re.findall(r'[a-zA-Z]', text)) # ì˜ë¬¸ ë¬¸ì ìˆ˜
            
            # ë¬¸ì ìˆ˜ ë¹„êµë¡œ ì–¸ì–´ íŒë‹¨
            if korean_chars > english_chars:
                return 'ko'                                   # í•œê¸€ì´ ë” ë§ìœ¼ë©´ í•œêµ­ì–´
            else:
                return 'en'                                   # ì˜ë¬¸ì´ ë” ë§ìœ¼ë©´ ì˜ì–´

    # GPTë¥¼ ì´ìš©í•´ ì§ˆë¬¸ì˜ ë³¸ì§ˆì  ì˜ë„ì™€ í•µì‹¬ ëª©ì ì„ ì •í™•íˆ ë¶„ì„í•˜ëŠ” ë©”ì„œë“œ
    # Args:
    #     query: ë¶„ì„í•  ì‚¬ìš©ì ì§ˆë¬¸
    # Returns:
    #     dict: ì˜ë„ ë¶„ì„ ê²°ê³¼ (core_intent, ì¹´í…Œê³ ë¦¬, í‚¤ì›Œë“œ ë“±)
    def analyze_question_intent(self, query: str) -> dict:
        try:
            # ===== ë©”ëª¨ë¦¬ ìµœì í™” ì»¨í…ìŠ¤íŠ¸ ì‹œì‘ =====
            with memory_cleanup():
                # ===== 1ë‹¨ê³„: GPT ì˜ë„ ë¶„ì„ì„ ìœ„í•œ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ êµ¬ì„± =====
                system_prompt = """ë‹¹ì‹ ì€ ë°”ì´ë¸” ì•± ë¬¸ì˜ ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. 
ê³ ê° ì§ˆë¬¸ì˜ ë³¸ì§ˆì  ì˜ë„ë¥¼ íŒŒì•…í•˜ì—¬ ì˜ë¯¸ë¡ ì ìœ¼ë¡œ ë™ë“±í•œ ì§ˆë¬¸ë“¤ì´ ê°™ì€ ê²°ê³¼ë¥¼ ì–»ë„ë¡ ë¶„ì„í•˜ì„¸ìš”.

ë¶„ì„ ê²°ê³¼ë¥¼ JSON í˜•íƒœë¡œ ë°˜í™˜:

{
  "core_intent": "í•µì‹¬ ì˜ë„ (í‘œì¤€í™”ëœ í˜•íƒœ)",
  "intent_category": "ì˜ë„ ì¹´í…Œê³ ë¦¬",
  "primary_action": "ì£¼ìš” í–‰ë™",
  "target_object": "ëŒ€ìƒ ê°ì²´",
  "constraint_conditions": ["ì œì•½ ì¡°ê±´ë“¤"],
  "standardized_query": "í‘œì¤€í™”ëœ ì§ˆë¬¸ í˜•íƒœ",
  "semantic_keywords": ["ì˜ë¯¸ë¡ ì  í•µì‹¬ í‚¤ì›Œë“œë“¤"]
}

ğŸ¯ ì˜ë¯¸ë¡ ì  ë™ë“±ì„± ë¶„ì„ ê¸°ì¤€:

1. **í•µì‹¬ ì˜ë„ íŒŒì•…**: ì§ˆë¬¸ì˜ ë³¸ì§ˆì  ëª©ì ì´ ë¬´ì—‡ì¸ì§€ íŒŒì•…
   - "ë‘ ë²ˆì—­ë³¸ì„ ë™ì‹œì— ë³´ê³  ì‹¶ë‹¤" â†’ core_intent: "multiple_translations_view"
   - "í…ìŠ¤íŠ¸ë¥¼ ë³µì‚¬í•˜ê³  ì‹¶ë‹¤" â†’ core_intent: "text_copy"
   - "ì—°ì†ìœ¼ë¡œ ë“£ê³  ì‹¶ë‹¤" â†’ core_intent: "continuous_audio_play"

2. **í‘œì¤€í™”ëœ í˜•íƒœë¡œ ë³€í™˜**: êµ¬ì²´ì  ì˜ˆì‹œë¥¼ ì œê±°í•˜ê³  ì¼ë°˜í™”
   - "ìš”í•œë³µìŒ 3ì¥ 16ì ˆ NIVì™€ KJV ë™ì‹œì—" â†’ "ì„œë¡œ ë‹¤ë¥¸ ë²ˆì—­ë³¸ ë™ì‹œ ë³´ê¸°"
   - "ê°œì—­í•œê¸€ê³¼ ê°œì—­ê°œì • ë™ì‹œì—" â†’ "ì„œë¡œ ë‹¤ë¥¸ ë²ˆì—­ë³¸ ë™ì‹œ ë³´ê¸°"

3. **ì˜ë¯¸ë¡ ì  í‚¤ì›Œë“œ ì¶”ì¶œ**: í‘œë©´ì  ë‹¨ì–´ê°€ ì•„ë‹Œ ì˜ë¯¸ì  ê°œë…
   - "ë™ì‹œì—", "í•¨ê»˜", "ë¹„êµí•˜ì—¬", "ë‚˜ë€íˆ" â†’ "simultaneous_view"
   - "NIV", "KJV", "ê°œì—­í•œê¸€", "ë²ˆì—­ë³¸" â†’ "translation_version"

4. **ì œì•½ ì¡°ê±´ ì‹ë³„**: ìš”ì²­ì˜ êµ¬ì²´ì  ì¡°ê±´ë“¤
   - "ì˜ì–´ ë²ˆì—­ë³¸ë§Œ", "í•œê¸€ ë²ˆì—­ë³¸ë§Œ", "íŠ¹ì • ì¥ì ˆ" ë“±

ì˜ˆì‹œ ë¶„ì„:
ì§ˆë¬¸1: "ìš”í•œë³µìŒ 3ì¥ 16ì ˆ ì˜ì–´ ë²ˆì—­ë³¸ NIVì™€ KJV ë™ì‹œì— ë³´ë ¤ë©´?"
ì§ˆë¬¸2: "ê°œì—­í•œê¸€ê³¼ ê°œì—­ê°œì •ì„ ë™ì‹œì— ë³´ë ¤ë©´?"
ì§ˆë¬¸3: "ë‘ ê°œì˜ ë²ˆì—­ë³¸ì„ ì–´ë–»ê²Œ ë™ì‹œì— ë³¼ ìˆ˜ ìˆì£ ?"

â†’ ëª¨ë‘ core_intent: "multiple_translations_simultaneous_view"
â†’ ëª¨ë‘ standardized_query: "ì„œë¡œ ë‹¤ë¥¸ ë²ˆì—­ë³¸ì„ ë™ì‹œì— ë³´ëŠ” ë°©ë²•"
"""

                # ===== 2ë‹¨ê³„: ì‚¬ìš©ì ì§ˆë¬¸ ë¶„ì„ì„ ìœ„í•œ í”„ë¡¬í”„íŠ¸ ìƒì„± =====
                user_prompt = f"""ë‹¤ìŒ ì§ˆë¬¸ì„ ì˜ë¯¸ë¡ ì ìœ¼ë¡œ ë¶„ì„í•˜ì—¬ ë³¸ì§ˆì  ì˜ë„ë¥¼ íŒŒì•…í•´ì£¼ì„¸ìš”:

ì§ˆë¬¸: {query}

íŠ¹íˆ ë‹¤ìŒ ì‚¬í•­ì— ì§‘ì¤‘í•˜ì„¸ìš”:
1. ì´ ì§ˆë¬¸ì´ ì •ë§ë¡œ ë¬»ê³ ì í•˜ëŠ” ë°”ê°€ ë¬´ì—‡ì¸ê°€?
2. êµ¬ì²´ì  ì˜ˆì‹œ(ì„±ê²½ êµ¬ì ˆ, ë²ˆì—­ë³¸ëª… ë“±)ë¥¼ ì œê±°í•˜ê³  ì¼ë°˜í™”í•˜ë©´?
3. ë¹„ìŠ·í•œ ì˜ë„ì˜ ë‹¤ë¥¸ ì§ˆë¬¸ë“¤ê³¼ ì–´ë–»ê²Œ í†µí•©í•  ìˆ˜ ìˆëŠ”ê°€?"""

                # ===== 3ë‹¨ê³„: GPT API í˜¸ì¶œë¡œ ì˜ë„ ë¶„ì„ ì‹¤í–‰ =====
                response = self.openai_client.chat.completions.create(
                    model='gpt-5-mini',
                    messages=[
                        {"role": "system", "content": system_prompt},
                        {"role": "user", "content": user_prompt}
                    ],
                    max_completion_tokens=400,                               # ì¶©ë¶„í•œ ë¶„ì„ ê²°ê³¼ ê¸¸ì´
                    # temperature=0.2                               # ì¼ê´€ì„± ìˆëŠ” ë¶„ì„ì„ ìœ„í•´ ë‚®ì€ ê°’
                    response_format={"type": "json_object"}                  # JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µ
                )
                
                # ===== 4ë‹¨ê³„: GPT ì‘ë‹µ í…ìŠ¤íŠ¸ ì¶”ì¶œ =====
                result_text = response.choices[0].message.content.strip()
                
                # ===== 5ë‹¨ê³„: JSON íŒŒì‹± ë° ê²°ê³¼ êµ¬ì¡°í™” =====
                try:
                    # JSON í˜•íƒœë¡œ ì‘ë‹µ íŒŒì‹±
                    content = response.choices[0].message.content
                    result = json.loads(content) 
                    logging.info(f"ê°•í™”ëœ ì˜ë„ ë¶„ì„ ê²°ê³¼: {result}")
                    
                    # ===== 6ë‹¨ê³„: ê¸°ì¡´ ì‹œìŠ¤í…œê³¼ì˜ í˜¸í™˜ì„±ì„ ìœ„í•œ í•„ë“œ ì¶”ê°€ =====
                    result['intent_type'] = result.get('intent_category', 'ì¼ë°˜ë¬¸ì˜')
                    result['main_topic'] = result.get('target_object', 'ê¸°íƒ€')
                    result['specific_request'] = result.get('standardized_query', query[:100])
                    result['keywords'] = result.get('semantic_keywords', [query[:20]])
                    result['urgency'] = 'medium'
                    result['action_type'] = result.get('primary_action', 'ê¸°íƒ€')
                    
                    return result
                except json.JSONDecodeError:
                    # ===== JSON íŒŒì‹± ì‹¤íŒ¨ì‹œ ê¸°ë³¸ê°’ ë°˜í™˜ =====
                    logging.warning(f"JSON íŒŒì‹± ì‹¤íŒ¨, ê¸°ë³¸ê°’ ë°˜í™˜: {result_text}")
                    return {
                        "core_intent": "general_inquiry",
                        "intent_category": "ì¼ë°˜ë¬¸ì˜",
                        "primary_action": "ê¸°íƒ€",
                        "target_object": "ê¸°íƒ€",
                        "constraint_conditions": [],
                        "standardized_query": query,
                        "semantic_keywords": [query[:20]],
                        # ê¸°ì¡´ í˜¸í™˜ì„± í•„ë“œ
                        "intent_type": "ì¼ë°˜ë¬¸ì˜",
                        "main_topic": "ê¸°íƒ€",
                        "specific_request": query[:100],
                        "keywords": [query[:20]],
                        "urgency": "medium",
                        "action_type": "ê¸°íƒ€"
                    }
                
        except Exception as e:
            # ===== ì „ì²´ ì˜ë„ ë¶„ì„ í”„ë¡œì„¸ìŠ¤ ì‹¤íŒ¨ì‹œ ê¸°ë³¸ê°’ ë°˜í™˜ =====
            logging.error(f"ê°•í™”ëœ ì˜ë„ ë¶„ì„ ì‹¤íŒ¨: {e}")
            return {
                "core_intent": "general_inquiry",
                "intent_category": "ì¼ë°˜ë¬¸ì˜", 
                "primary_action": "ê¸°íƒ€",
                "target_object": "ê¸°íƒ€",
                "constraint_conditions": [],
                "standardized_query": query,
                "semantic_keywords": [query[:20]],
                # ê¸°ì¡´ í˜¸í™˜ì„± í•„ë“œ
                "intent_type": "ì¼ë°˜ë¬¸ì˜",
                "main_topic": "ê¸°íƒ€",
                "specific_request": query[:100],
                "keywords": [query[:20]],
                "urgency": "medium",
                "action_type": "ê¸°íƒ€"
            }

    # ì§ˆë¬¸ì˜ ì˜ë„ì™€ ì°¸ì¡° ë‹µë³€ ê°„ì˜ ì˜ë¯¸ë¡ ì  ìœ ì‚¬ì„±ì„ ê³„ì‚°í•˜ëŠ” ë©”ì„œë“œ
    # Args:
    #     query_intent_analysis: ë¶„ì„ëœ ì§ˆë¬¸ ì˜ë„ ì •ë³´
    #     ref_question: ì°¸ì¡° ì§ˆë¬¸
    #     ref_answer: ì°¸ì¡° ë‹µë³€
    # Returns:
    #     float: ìœ ì‚¬ì„± ì ìˆ˜ (0.0 ~ 1.0)
    def calculate_intent_similarity(self, query_intent_analysis: dict, ref_question: str, ref_answer: str) -> float:
        
        try:
            # ===== 1ë‹¨ê³„: ì‚¬ìš©ì ì§ˆë¬¸ì˜ ì˜ë„ ì •ë³´ ì¶”ì¶œ =====
            query_core_intent = query_intent_analysis.get('core_intent', '')
            query_primary_action = query_intent_analysis.get('primary_action', '')
            query_target_object = query_intent_analysis.get('target_object', '')
            query_semantic_keywords = query_intent_analysis.get('semantic_keywords', [])
            
            # ì˜ë„ ì •ë³´ê°€ ì—†ìœ¼ë©´ ì¤‘ê°„ê°’ ë°˜í™˜
            if not query_core_intent:
                return 0.5
            
            # ===== 2ë‹¨ê³„: ì°¸ì¡° ì§ˆë¬¸ì˜ ì˜ë„ ë¶„ì„ ì‹¤í–‰ =====
            ref_text = ref_question + ' ' + ref_answer
                
            # ğŸ” ì‹¤ì‹œê°„ ì˜ë„ ë¶„ì„ ì‹œì‘ ë¡œê·¸
            logging.info(f"ğŸ” ê¸°ì¡´ ë‹µë³€ ì‹¤ì‹œê°„ ì˜ë„ ë¶„ì„ ì‹œì‘:")
            logging.info(f"   â””â”€â”€ ê¸°ì¡´ ì§ˆë¬¸: {ref_question[:80]}...")
                
            ref_intent_analysis = self.analyze_question_intent(ref_question)
                
                # ğŸ” ì‹¤ì‹œê°„ ì˜ë„ ë¶„ì„ ê²°ê³¼ ë¡œê·¸
            logging.info(f"ğŸ” ê¸°ì¡´ ë‹µë³€ ì˜ë„ ë¶„ì„ ê²°ê³¼:")
            logging.info(f"   â””â”€â”€ í•µì‹¬ ì˜ë„: {ref_intent_analysis.get('core_intent', 'N/A')}")
            logging.info(f"   â””â”€â”€ ì£¼ìš” í–‰ë™: {ref_intent_analysis.get('primary_action', 'N/A')}")
            logging.info(f"   â””â”€â”€ ëŒ€ìƒ ê°ì²´: {ref_intent_analysis.get('target_object', 'N/A')}")
            logging.info(f"   â””â”€â”€ í‚¤ì›Œë“œ: {ref_intent_analysis.get('semantic_keywords', [])}")
            
            ref_core_intent = ref_intent_analysis.get('core_intent', '')
            ref_primary_action = ref_intent_analysis.get('primary_action', '')
            ref_target_object = ref_intent_analysis.get('target_object', '')
            ref_semantic_keywords = ref_intent_analysis.get('semantic_keywords', [])
            
            # ===== 3ë‹¨ê³„: í•µì‹¬ ì˜ë„ ì¼ì¹˜ë„ ê³„ì‚° (ê°€ì¥ ì¤‘ìš”í•œ ì§€í‘œ) =====
            intent_match_score = 0.0
            if query_core_intent == ref_core_intent:
                # ì™„ì „ ì¼ì¹˜: ìµœê³  ì ìˆ˜
                intent_match_score = 1.0
            elif query_core_intent and ref_core_intent:
                # ë¶€ë¶„ ì¼ì¹˜: ì˜ë„ ì´ë¦„ì˜ ë‹¨ì–´ ìœ ì‚¬ì„± ê²€ì‚¬
                query_intent_words = set(query_core_intent.split('_'))
                ref_intent_words = set(ref_core_intent.split('_'))
                
                if query_intent_words & ref_intent_words:  # ê³µí†µ ë‹¨ì–´ê°€ ìˆìœ¼ë©´
                    overlap_ratio = len(query_intent_words & ref_intent_words) / len(query_intent_words | ref_intent_words)
                    intent_match_score = overlap_ratio * 0.8  # ì™„ì „ ì¼ì¹˜ë³´ë‹¤ëŠ” ë‚®ê²Œ ì„¤ì •
            
            # ===== 4ë‹¨ê³„: í–‰ë™ ìœ í˜• ì¼ì¹˜ë„ ê³„ì‚° =====
            action_match_score = 0.0
            if query_primary_action == ref_primary_action:
                # ì™„ì „ ì¼ì¹˜: ìµœê³  ì ìˆ˜
                action_match_score = 1.0
            elif query_primary_action and ref_primary_action:
                # ìœ ì‚¬í•œ í–‰ë™ ìœ í˜• ë§¤í•‘ í…Œì´ë¸”
                action_similarity_map = {
                    ('ë³´ê¸°', 'í™•ì¸'): 0.8,
                    ('ë³µì‚¬', 'ì €ì¥'): 0.7,
                    ('ë“£ê¸°', 'ì¬ìƒ'): 0.9,
                    ('ê²€ìƒ‰', 'ì°¾ê¸°'): 0.8,
                    ('ì„¤ì •', 'ë³€ê²½'): 0.7
                }
                
                action_key = (query_primary_action, ref_primary_action)
                reverse_key = (ref_primary_action, query_primary_action)
                
                # ì–‘ë°©í–¥ ë§¤í•‘ ê²€ì‚¬
                if action_key in action_similarity_map:
                    action_match_score = action_similarity_map[action_key]
                elif reverse_key in action_similarity_map:
                    action_match_score = action_similarity_map[reverse_key]
            
            # ===== 5ë‹¨ê³„: ëŒ€ìƒ ê°ì²´ ì¼ì¹˜ë„ ê³„ì‚° =====
            object_match_score = 0.0
            if query_target_object == ref_target_object:
                # ì™„ì „ ì¼ì¹˜: ìµœê³  ì ìˆ˜
                object_match_score = 1.0
            elif query_target_object and ref_target_object:
                # ìœ ì‚¬í•œ ê°ì²´ ìœ í˜• ë§¤í•‘ í…Œì´ë¸”
                object_similarity_map = {
                    ('ë²ˆì—­ë³¸', 'ì„±ê²½'): 0.8,
                    ('í…ìŠ¤íŠ¸', 'ë‚´ìš©'): 0.7,
                    ('ìŒì„±', 'ì˜¤ë””ì˜¤'): 0.9,
                    ('í™”ë©´', 'ë””ìŠ¤í”Œë ˆì´'): 0.7
                }
                
                object_key = (query_target_object, ref_target_object)
                reverse_key = (ref_target_object, query_target_object)
                
                # ì–‘ë°©í–¥ ë§¤í•‘ ê²€ì‚¬
                if object_key in object_similarity_map:
                    object_match_score = object_similarity_map[object_key]
                elif reverse_key in object_similarity_map:
                    object_match_score = object_similarity_map[reverse_key]
            
            # ===== 6ë‹¨ê³„: ì˜ë¯¸ë¡ ì  í‚¤ì›Œë“œ ì¼ì¹˜ë„ ê³„ì‚° =====
            keyword_match_score = 0.0
            if query_semantic_keywords and ref_semantic_keywords:
                query_keyword_set = set(query_semantic_keywords)
                ref_keyword_set = set(ref_semantic_keywords)
                
                # êµì§‘í•©ê³¼ í•©ì§‘í•©ì„ ì´ìš©í•œ Jaccard ìœ ì‚¬ë„ ê³„ì‚°
                common_keywords = query_keyword_set & ref_keyword_set
                total_keywords = query_keyword_set | ref_keyword_set
                
                if total_keywords:
                    keyword_match_score = len(common_keywords) / len(total_keywords)
            
            # ===== 7ë‹¨ê³„: ì „ì²´ ìœ ì‚¬ì„± ì ìˆ˜ ê³„ì‚° (ê°€ì¤‘ í‰ê· ) =====
            total_score = (
                intent_match_score * 0.4 +      # í•µì‹¬ ì˜ë„ ì¼ì¹˜ (40% - ê°€ì¥ ì¤‘ìš”)
                action_match_score * 0.25 +     # í–‰ë™ ìœ í˜• ì¼ì¹˜ (25%)
                object_match_score * 0.2 +      # ëŒ€ìƒ ê°ì²´ ì¼ì¹˜ (20%)
                keyword_match_score * 0.15      # í‚¤ì›Œë“œ ì¼ì¹˜ (15%)
            )
            
            # ===== 8ë‹¨ê³„: ë””ë²„ê·¸ ë¡œê¹… ë° ê²°ê³¼ ë°˜í™˜ =====
            logging.debug(f"ì˜ë„ ìœ ì‚¬ì„± ë¶„ì„: ì˜ë„={intent_match_score:.2f}, "
                         f"í–‰ë™={action_match_score:.2f}, ê°ì²´={object_match_score:.2f}, "
                         f"í‚¤ì›Œë“œ={keyword_match_score:.2f}, ì „ì²´={total_score:.2f}")
            
            # ğŸ” ì˜ë„ ìœ ì‚¬ì„± ê³„ì‚° ìƒì„¸ ë¡œê·¸
            logging.info(f"ğŸ” ì˜ë„ ìœ ì‚¬ì„± ê³„ì‚° ìƒì„¸:")
            logging.info(f"   â””â”€â”€ ì‚¬ìš©ì ì˜ë„: {query_core_intent}")
            logging.info(f"   â””â”€â”€ ê¸°ì¡´ ë‹µë³€ ì˜ë„: {ref_core_intent}")
            logging.info(f"   â””â”€â”€ ì˜ë„ ì¼ì¹˜ë„: {intent_match_score:.3f} (40%)")
            logging.info(f"   â””â”€â”€ í–‰ë™ ì¼ì¹˜ë„: {action_match_score:.3f} (25%)")
            logging.info(f"   â””â”€â”€ ê°ì²´ ì¼ì¹˜ë„: {object_match_score:.3f} (20%)")
            logging.info(f"   â””â”€â”€ í‚¤ì›Œë“œ ì¼ì¹˜ë„: {keyword_match_score:.3f} (15%)")
            logging.info(f"   â””â”€â”€ ìµœì¢… ì˜ë„ ê´€ë ¨ì„±: {total_score:.3f}")
            
            return min(total_score, 1.0)  # 1.0ì„ ì´ˆê³¼í•˜ì§€ ì•Šë„ë¡ ì œí•œ
            
        except Exception as e:
            # ===== ì˜ˆì™¸ ì²˜ë¦¬: ìœ ì‚¬ì„± ê³„ì‚° ì‹¤íŒ¨ì‹œ ê¸°ë³¸ê°’ ë°˜í™˜ =====
            logging.error(f"ì˜ë„ ìœ ì‚¬ì„± ê³„ì‚° ì‹¤íŒ¨: {e}")
            return 0.3  # ì˜¤ë¥˜ì‹œ ë‚®ì€ ê¸°ë³¸ê°’ ë°˜í™˜
