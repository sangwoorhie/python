#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
ìµœì í™”ëœ ê²€ìƒ‰ ì„œë¹„ìŠ¤ ëª¨ë“ˆ
- ìºì‹±, ë°°ì¹˜ ì²˜ë¦¬, ì§€ëŠ¥í˜• API ê´€ë¦¬ë¥¼ í†µí•©í•œ ê³ ì„±ëŠ¥ ê²€ìƒ‰ ì‹œìŠ¤í…œ
- ë‹¤ì¸µ ê²€ìƒ‰ ë° ì˜ë¯¸ë¡ ì  ìœ ì‚¬ì„± ê³„ì‚°
- ì‹¤ì‹œê°„ ì„±ëŠ¥ ìµœì í™” ë° ì¡°ê¸° ì¢…ë£Œ ë©”ì»¤ë‹ˆì¦˜
"""

import logging
import time
from typing import List, Dict, Set, Optional
from src.utils.memory_manager import memory_cleanup
from src.utils.text_preprocessor import TextPreprocessor
from src.utils.intelligent_api_manager import (
    IntelligentAPIManager, APICallRequest, APICallStrategy
)
from src.models.question_analyzer import QuestionAnalyzer
import numpy as np

# ===== ìµœì í™”ëœ Pinecone ë²¡í„° ê²€ìƒ‰ì„ ë‹´ë‹¹í•˜ëŠ” ë©”ì¸ í´ë˜ìŠ¤ =====
class OptimizedSearchService:
    
    # OptimizedSearchService ì´ˆê¸°í™”
    # Args:
    #     pinecone_index: Pinecone ë²¡í„° ì¸ë±ìŠ¤
    #     api_manager: ì§€ëŠ¥í˜• API ê´€ë¦¬ì
    def __init__(self, pinecone_index, api_manager: IntelligentAPIManager):
        self.index = pinecone_index                           # Pinecone ë²¡í„° ê²€ìƒ‰ ì¸ë±ìŠ¤
        self.api_manager = api_manager                        # ìºì‹±/ë°°ì¹˜ API ê´€ë¦¬ì
        self.text_processor = TextPreprocessor()              # í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ ë„êµ¬
        
        # QuestionAnalyzer ì´ˆê¸°í™” (ì˜ë„ ê´€ë ¨ì„± ê³„ì‚°ìš©)
        self._question_analyzer = QuestionAnalyzer(api_manager.openai_client)
        
        # ===== ê²€ìƒ‰ ìµœì í™” ì„¤ì • =====
        self.search_config = {
            'max_layers': 5,                                  # ìµœëŒ€ ê²€ìƒ‰ ë ˆì´ì–´ ìˆ˜
            'adaptive_layer_count': True,                     # ë™ì  ë ˆì´ì–´ ìˆ˜ ì¡°ì •
            'early_termination': True,                        # ì¡°ê¸° ì¢…ë£Œ í™œì„±í™”
            'similarity_threshold': 0.75,                     # ìœ ì‚¬ë„ ì„ê³„ê°’
            'min_results_threshold': 3,                       # ìµœì†Œ ê²°ê³¼ ìˆ˜ ì„ê³„ê°’
            'enable_result_caching': True,                    # ê²€ìƒ‰ ê²°ê³¼ ìºì‹± í™œì„±í™”
            'cache_ttl_hours': 24                            # ìºì‹œ ìœ íš¨ì‹œê°„ (ì‹œê°„)
        }
        
        # ===== ì„±ëŠ¥ ìµœì í™”ë¥¼ ìœ„í•œ ìºì‹œ ì‹œìŠ¤í…œ =====
        self.embedding_cache = {}                            # ì„ë² ë”© ì¬ì‚¬ìš© ìºì‹œ
        self.search_history = {}                             # ê²€ìƒ‰ ê¸°ë¡ ìºì‹œ
        
        logging.info("ìµœì í™”ëœ ê²€ìƒ‰ ì„œë¹„ìŠ¤ ì´ˆê¸°í™” ì™„ë£Œ")

    # ìµœì í™”ëœ ì˜ë¯¸ë¡ ì  ë‹¤ì¸µ ê²€ìƒ‰ - ë©”ì¸ ê²€ìƒ‰ ë©”ì„œë“œ
    # Args:
    #     query: ê²€ìƒ‰í•  ì‚¬ìš©ì ì§ˆë¬¸
    #     top_k: ë°˜í™˜í•  ìµœëŒ€ ê²°ê³¼ ìˆ˜
    #     lang: ì–¸ì–´ ì½”ë“œ
    # Returns:
    #     List[Dict]: ê²€ìƒ‰ëœ ìœ ì‚¬ ë‹µë³€ ë¦¬ìŠ¤íŠ¸
    def search_similar_answers_optimized(self, query: str, top_k: int = 8, lang: str = 'ko') -> List[Dict]:
        try:
            # ===== ë©”ëª¨ë¦¬ ìµœì í™” ì»¨í…ìŠ¤íŠ¸ ì‹œì‘ =====
            with memory_cleanup():
                search_start = time.time()
                logging.info(f"=== ìµœì í™”ëœ ë‹¤ì¸µ ê²€ìƒ‰ ì‹œì‘ ===")
                logging.info(f"ì›ë³¸ ì§ˆë¬¸: {query}")
                
                # ===== 1ë‹¨ê³„: ê²€ìƒ‰ ê²°ê³¼ ìºì‹œ í™•ì¸ =====
                if self.search_config['enable_result_caching']:
                    cached_results = self._check_search_cache(query, {'top_k': top_k, 'lang': lang})
                    if cached_results:
                        logging.info(f"ê²€ìƒ‰ ê²°ê³¼ ìºì‹œ íˆíŠ¸: {len(cached_results)}ê°œ ê²°ê³¼")
                        return cached_results
                
                # ===== 2ë‹¨ê³„: ê¸°ë³¸ ì „ì²˜ë¦¬ (ìºì‹± ì ìš©) =====
                processed_query = self._preprocess_with_caching(query, lang)
                
                # ===== 3ë‹¨ê³„: í•µì‹¬ ì˜ë„ ë¶„ì„ (ìºì‹± ì ìš©) =====
                # í†µí•© ë¶„ì„ê¸°ì—ì„œ ì´ë¯¸ ì²˜ë¦¬ëœ ì˜ë„ ë¶„ì„ì„ ì‚¬ìš©í•˜ë¯€ë¡œ ê°œë³„ í˜¸ì¶œ ë¶ˆí•„ìš”
                intent_analysis = self._get_default_intent_analysis(processed_query)
                
                # ===== 4ë‹¨ê³„: ê²€ìƒ‰ ë ˆì´ì–´ ê³„íš ìˆ˜ë¦½ =====
                search_plan = self._create_search_plan(processed_query, intent_analysis)
                
                # ===== 5ë‹¨ê³„: ìµœì í™”ëœ ë‹¤ì¸µ ê²€ìƒ‰ ì‹¤í–‰ =====
                search_results = self._execute_optimized_search(search_plan, top_k)
                
                # ===== 6ë‹¨ê³„: ê²°ê³¼ í›„ì²˜ë¦¬ ë° ì ìˆ˜ ê³„ì‚° =====
                final_results = self._postprocess_results(
                    search_results, processed_query, intent_analysis, top_k
                )
                
                # ===== 7ë‹¨ê³„: ê²€ìƒ‰ ê²°ê³¼ ìºì‹± =====
                if self.search_config['enable_result_caching']:
                    self._cache_search_results(query, {'top_k': top_k, 'lang': lang}, final_results)
                
                # ===== 8ë‹¨ê³„: ê²€ìƒ‰ ì™„ë£Œ ë° ì„±ëŠ¥ ë¡œê¹… =====
                search_time = time.time() - search_start
                logging.info(f"âœ… ìºì‹œëœ ì˜ë„ ë¶„ì„ í™œìš© ê²€ìƒ‰ ì™„ë£Œ: {len(final_results)}ê°œ ê²°ê³¼, {search_time:.2f}s")
                
                # ğŸ” ìµœì¢… ê²€ìƒ‰ ê²°ê³¼ ìƒì„¸ ë¡œê¹…
                if final_results:
                    logging.info(f"ğŸ¯ ìºì‹œëœ ì˜ë„ ë¶„ì„ ê²€ìƒ‰ ê²°ê³¼:")
                    for i, result in enumerate(final_results[:3]):
                        score = result.get('adjusted_score', 0.0)
                        question = result.get('question', 'N/A')[:80]
                        answer = result.get('answer', 'N/A')[:80]
                        logging.info(f"   â””â”€â”€ ê²°ê³¼ {i+1}: ì ìˆ˜={score:.3f}, ì§ˆë¬¸='{question}...', ë‹µë³€='{answer}...'")
                
                return final_results
                
        except Exception as e:
            # ===== ì˜ˆì™¸ ì²˜ë¦¬: ê²€ìƒ‰ ì‹¤íŒ¨ì‹œ ë¹ˆ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜ =====
            logging.error(f"ìµœì í™”ëœ ê²€ìƒ‰ ì‹¤íŒ¨: {str(e)}")
            return []

    # ìºì‹œëœ ì˜ë„ ë¶„ì„ì„ í™œìš©í•œ ìµœì í™”ëœ ê²€ìƒ‰ - API í˜¸ì¶œ ì ˆì•½
    def search_similar_answers_with_cached_intent(self, query: str, cached_intent: Dict, top_k: int = 8, lang: str = 'ko') -> List[Dict]:
        try:
            # ===== ë©”ëª¨ë¦¬ ìµœì í™” ì»¨í…ìŠ¤íŠ¸ ì‹œì‘ =====
            with memory_cleanup():
                search_start = time.time()
                logging.info(f"=== ìºì‹œëœ ì˜ë„ ë¶„ì„ í™œìš© ê²€ìƒ‰ ì‹œì‘ ===")
                logging.info(f"ì›ë³¸ ì§ˆë¬¸: {query}")
                logging.info(f"ìºì‹œëœ ì˜ë„: {cached_intent.get('core_intent', 'N/A')}")
                
                # ===== 1ë‹¨ê³„: ê²€ìƒ‰ ê²°ê³¼ ìºì‹œ í™•ì¸ =====
                if self.search_config['enable_result_caching']:
                    cached_results = self._check_search_cache(query, {'top_k': top_k, 'lang': lang})
                    if cached_results:
                        logging.info(f"ê²€ìƒ‰ ê²°ê³¼ ìºì‹œ íˆíŠ¸: {len(cached_results)}ê°œ ê²°ê³¼")
                        return cached_results
                
                # ===== 2ë‹¨ê³„: ê¸°ë³¸ ì „ì²˜ë¦¬ (ì˜¤íƒ€ëŠ” ì´ë¯¸ ìˆ˜ì •ë¨) =====
                processed_query = self.text_processor.preprocess_text(query)
                
                # ===== 3ë‹¨ê³„: ìºì‹œëœ ì˜ë„ ë¶„ì„ í™œìš© (API í˜¸ì¶œ ìƒëµ!) =====
                intent_analysis = cached_intent  # ì´ë¯¸ ìˆ˜ì •ëœ ì˜ë„ ë¶„ì„ ì‚¬ìš©
                logging.info(f"ğŸ” ìºì‹œëœ ì˜ë„ ë¶„ì„ í™œìš©: {intent_analysis.get('core_intent', 'N/A')}")
                
                # ===== 4ë‹¨ê³„: ê²€ìƒ‰ ë ˆì´ì–´ ê³„íš ìˆ˜ë¦½ =====
                search_plan = self._create_search_plan(processed_query, intent_analysis)
                
                # ===== 5ë‹¨ê³„: ìµœì í™”ëœ ë‹¤ì¸µ ê²€ìƒ‰ ì‹¤í–‰ =====
                search_results = self._execute_optimized_search(search_plan, top_k)
                
                # ===== 6ë‹¨ê³„: ê²°ê³¼ í›„ì²˜ë¦¬ ë° ì ìˆ˜ ê³„ì‚° =====
                final_results = self._postprocess_results(
                    search_results, processed_query, intent_analysis, top_k
                )
                
                # ===== 7ë‹¨ê³„: ê²€ìƒ‰ ê²°ê³¼ ìºì‹± =====
                if self.search_config['enable_result_caching']:
                    self._cache_search_results(query, {'top_k': top_k, 'lang': lang}, final_results)
                
                # ===== 8ë‹¨ê³„: ê²€ìƒ‰ ì™„ë£Œ ë° ì„±ëŠ¥ ë¡œê¹… =====
                search_time = time.time() - search_start
                logging.info(f"ğŸ” ìºì‹œëœ ì˜ë„ í™œìš© ê²€ìƒ‰ ì™„ë£Œ: {len(final_results)}ê°œ ê²°ê³¼, {search_time:.2f}s")
                
                return final_results
                
        except Exception as e:
            # ===== ì˜ˆì™¸ ì²˜ë¦¬: ê²€ìƒ‰ ì‹¤íŒ¨ì‹œ ë¹ˆ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜ =====
            logging.error(f"ìºì‹œëœ ì˜ë„ í™œìš© ê²€ìƒ‰ ì‹¤íŒ¨: {str(e)}")
            return []

    # ìºì‹± ê¸°ë°˜ í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ ë©”ì„œë“œ
    # Args:
    #     query: ì „ì²˜ë¦¬í•  ì§ˆë¬¸ í…ìŠ¤íŠ¸
    #     lang: ì–¸ì–´ ì½”ë“œ
    # Returns:
    #     str: ì „ì²˜ë¦¬ëœ ì§ˆë¬¸ í…ìŠ¤íŠ¸
    def _preprocess_with_caching(self, query: str, lang: str) -> str:
        # ===== ê¸°ë³¸ í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ë§Œ ìˆ˜í–‰ =====
        # ì˜¤íƒ€ ìˆ˜ì •ì€ main_optimized_ai_generator.pyì˜ í†µí•© ë¶„ì„ê¸°ì—ì„œ ì²˜ë¦¬ë¨
        processed_query = self.text_processor.preprocess_text(query)
        return processed_query

    # ìºì‹± ê¸°ë°˜ ì§ˆë¬¸ ì˜ë„ ë¶„ì„ ë©”ì„œë“œ
    # Args:
    #     query: ì˜ë„ë¥¼ ë¶„ì„í•  ì§ˆë¬¸
    # Returns:
    #     Dict: ë¶„ì„ëœ ì˜ë„ ì •ë³´ (core_intent, ì¹´í…Œê³ ë¦¬ ë“±)
    def _get_default_intent_analysis(self, query: str) -> Dict:
        """ê¸°ë³¸ ì˜ë„ ë¶„ì„ ê²°ê³¼ ë°˜í™˜ (í†µí•© ë¶„ì„ê¸° ì‚¬ìš©ì‹œ í´ë°±ìš©)"""
        return {
            "core_intent": "general_inquiry",
            "intent_category": "ì¼ë°˜ë¬¸ì˜",
            "primary_action": "ê¸°íƒ€",
            "target_object": "ê¸°íƒ€",
            "standardized_query": query,
            "semantic_keywords": [query[:20]]
        }

    # ì§€ëŠ¥í˜• ë‹¤ì¸µ ê²€ìƒ‰ ê³„íš ìˆ˜ë¦½ ë©”ì„œë“œ
    # Args:
    #     query: ê²€ìƒ‰í•  ì§ˆë¬¸
    #     intent_analysis: ì˜ë„ ë¶„ì„ ê²°ê³¼
    # Returns:
    #     Dict: ê²€ìƒ‰ ê³„íš (ë ˆì´ì–´ êµ¬ì„±, íƒ€ê²Ÿ ê²°ê³¼ ìˆ˜ ë“±)
    def _create_search_plan(self, query: str, intent_analysis: Dict) -> Dict:
        # ===== 1ë‹¨ê³„: ì˜ë„ ë¶„ì„ ê²°ê³¼ì—ì„œ í•µì‹¬ ì •ë³´ ì¶”ì¶œ =====
        core_intent = intent_analysis.get('core_intent', '')
        standardized_query = intent_analysis.get('standardized_query', query)
        semantic_keywords = intent_analysis.get('semantic_keywords', [])
        
        # ===== 2ë‹¨ê³„: ê¸°ì¡´ ê°œë… ì¶”ì¶œ (ì¶”ê°€ ë¶„ì„) =====
        key_concepts = self.text_processor.extract_key_concepts(query)
        
        # ===== 3ë‹¨ê³„: ë™ì  ë ˆì´ì–´ ê°œìˆ˜ ê²°ì • =====
        if self.search_config['adaptive_layer_count']:
            layer_count = self._determine_optimal_layer_count(intent_analysis, key_concepts)
        else:
            layer_count = self.search_config['max_layers']
        
        # ===== 4ë‹¨ê³„: ê²€ìƒ‰ ë ˆì´ì–´ êµ¬ì„± (ìš°ì„ ìˆœìœ„ë³„) =====
        search_layers = []
        
        # Layer 1: ì›ë³¸ ì§ˆë¬¸ (í•„ìˆ˜ ë ˆì´ì–´ - ê°€ì¥ ë†’ì€ ê°€ì¤‘ì¹˜)
        search_layers.append({
            'query': query,
            'weight': 1.0,  # ìµœê³  ê°€ì¤‘ì¹˜
            'type': 'original',
            'priority': 1
        })
        
        # Layer 2: í‘œì¤€í™”ëœ ì˜ë„ ê¸°ë°˜ ì§ˆë¬¸ (GPT ë¶„ì„ ê²°ê³¼)
        if standardized_query and standardized_query != query:
            search_layers.append({
                'query': standardized_query,
                'weight': 0.95,                               # ë†’ì€ ê°€ì¤‘ì¹˜
                'type': 'intent_based',
                'priority': 2
            })
        
        # Layer 3: í•µì‹¬ ì˜ë„ë§Œ (ì¶”ìƒí™”ëœ ê²€ìƒ‰)
        if core_intent and layer_count >= 3:
            search_layers.append({
                'query': core_intent.replace('_', ' '),       # ì–¸ë”ìŠ¤ì½”ì–´ ì œê±°
                'weight': 0.9,
                'type': 'core_intent',
                'priority': 3
            })
        
        # Layer 4: ì˜ë¯¸ë¡ ì  í‚¤ì›Œë“œ ì¡°í•© (GPT ì¶”ì¶œ í‚¤ì›Œë“œ)
        if semantic_keywords and len(semantic_keywords) >= 2 and layer_count >= 4:
            semantic_query = ' '.join(semantic_keywords[:3]) # ìƒìœ„ 3ê°œ í‚¤ì›Œë“œ
            search_layers.append({
                'query': semantic_query,
                'weight': 0.8,
                'type': 'semantic_keywords',
                'priority': 4
            })
        
        # Layer 5: ê¸°ì¡´ ê°œë… ê¸°ë°˜ ê²€ìƒ‰ (ê·œì¹™ ê¸°ë°˜ í‚¤ì›Œë“œ)
        if key_concepts and len(key_concepts) >= 2 and layer_count >= 5:
            concept_query = ' '.join(key_concepts[:3])       # ìƒìœ„ 3ê°œ ê°œë…
            search_layers.append({
                'query': concept_query,
                'weight': 0.7,                               # ë‚®ì€ ê°€ì¤‘ì¹˜
                'type': 'concept_based',
                'priority': 5
            })
        
        # ===== 5ë‹¨ê³„: ê²€ìƒ‰ ê³„íš ë°˜í™˜ =====
        return {
            'layers': search_layers,
            'target_results': self._calculate_target_results(len(search_layers)),
            'early_termination_enabled': self.search_config['early_termination']
        }

    # ì§ˆë¬¸ ë³µì¡ë„ì— ë”°ë¥¸ ìµœì  ë ˆì´ì–´ ìˆ˜ ê²°ì • ë©”ì„œë“œ
    # Args:
    #     intent_analysis: ì˜ë„ ë¶„ì„ ê²°ê³¼
    #     key_concepts: ì¶”ì¶œëœ í•µì‹¬ ê°œë… ë¦¬ìŠ¤íŠ¸
    # Returns:
    #     int: ìµœì  ë ˆì´ì–´ ìˆ˜
    def _determine_optimal_layer_count(self, intent_analysis: Dict, key_concepts: List) -> int:
        base_count = 2  # ê¸°ë³¸ ë ˆì´ì–´ (ì›ë³¸ + ì˜ë„ ê¸°ë°˜)
        
        # ===== ë³µì¡ì„± ê¸°ë°˜ ì¶”ê°€ ë ˆì´ì–´ ê³„ì‚° =====
        complexity_score = 0
        
        # ì˜ë¯¸ë¡ ì  í‚¤ì›Œë“œ ê°œìˆ˜ í‰ê°€
        semantic_keywords = intent_analysis.get('semantic_keywords', [])
        if len(semantic_keywords) >= 2:
            complexity_score += 1
        
        # í•µì‹¬ ê°œë… ê°œìˆ˜ í‰ê°€
        if len(key_concepts) >= 2:
            complexity_score += 1
        
        # ì˜ë„ ì¹´í…Œê³ ë¦¬ ë³µì¡ë„ í‰ê°€ (ë³µì¡í•œ ë¬¸ì˜ ìœ í˜•)
        intent_category = intent_analysis.get('intent_category', '')
        if intent_category in ['ê°œì„ /ì œì•ˆ', 'ì˜¤ë¥˜/ì¥ì• ']:
            complexity_score += 1
        
        # ===== ìµœì¢… ë ˆì´ì–´ ìˆ˜ ê²°ì • (ìµœëŒ€ê°’ ì œí•œ) =====
        final_count = min(base_count + complexity_score, self.search_config['max_layers'])
        
        logging.debug(f"ë™ì  ë ˆì´ì–´ ê³„ì‚°: ê¸°ë³¸={base_count}, ë³µì¡ë„={complexity_score}, ìµœì¢…={final_count}")
        
        return final_count

    # ë ˆì´ì–´ë³„ íƒ€ê²Ÿ ê²°ê³¼ ìˆ˜ ê³„ì‚° ë©”ì„œë“œ
    # Args:
    #     layer_count: ê²€ìƒ‰ ë ˆì´ì–´ ìˆ˜
    # Returns:
    #     Dict[str, int]: ë ˆì´ì–´ë³„ íƒ€ê²Ÿ ê²°ê³¼ ìˆ˜
    def _calculate_target_results(self, layer_count: int) -> Dict[str, int]:
        base_results = 8
        
        return {
            'first_layer': base_results * 2,  # ì²« ë²ˆì§¸ ë ˆì´ì–´ëŠ” ë” ë§ì´ ê²€ìƒ‰
            'other_layers': base_results,     # ë‚˜ë¨¸ì§€ ë ˆì´ì–´ëŠ” ê¸°ë³¸ ìˆ˜ëŸ‰
            'total_unique': base_results * layer_count  # ì „ì²´ ìœ ë‹ˆí¬ ê²°ê³¼ ëª©í‘œ
        }

    # ìµœì í™”ëœ ë‹¤ì¸µ ê²€ìƒ‰ ì‹¤í–‰ ë©”ì„œë“œ (í•µì‹¬ ê²€ìƒ‰ ë¡œì§)
    # Args:
    #     search_plan: ê²€ìƒ‰ ê³„íš (ë ˆì´ì–´ êµ¬ì„± ì •ë³´)
    #     top_k: ìµœëŒ€ ë°˜í™˜ ê²°ê³¼ ìˆ˜
    # Returns:
    #     List[Dict]: ê²€ìƒ‰ëœ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
    def _execute_optimized_search(self, search_plan: Dict, top_k: int) -> List[Dict]:
        layers = search_plan['layers']
        target_results = search_plan['target_results']
        
        all_results = []
        seen_ids = set()
        sufficient_results = False
        
        # ì„ë² ë”© ìš”ì²­ ë°°ì¹˜ ì¤€ë¹„
        embedding_requests = []
        
        for i, layer in enumerate(layers):
            search_query = layer['query']
            if not search_query or len(search_query.strip()) < 2:
                continue
            
            # ì„ë² ë”© ìºì‹œ í™•ì¸
            if search_query in self.embedding_cache:
                layer['embedding'] = self.embedding_cache[search_query]
                logging.debug(f"ì„ë² ë”© ìºì‹œ íˆíŠ¸: ë ˆì´ì–´ {i+1}")
            else:
                # ë°°ì¹˜ ìš”ì²­ì— ì¶”ê°€
                embedding_requests.append({
                    'layer_index': i,
                    'query': search_query,
                    'priority': layer['priority']
                })
        
        # í•„ìš”í•œ ì„ë² ë”© ë°°ì¹˜ ìƒì„±
        if embedding_requests:
            self._generate_embeddings_batch(embedding_requests, layers)
        
        # ê° ë ˆì´ì–´ë³„ ê²€ìƒ‰ ì‹¤í–‰
        for i, layer in enumerate(layers):
            if 'embedding' not in layer:
                continue
            
            layer_type = layer['type']
            weight = layer['weight']
            
            logging.debug(f"ë ˆì´ì–´ {i+1} ({layer_type}) ê²€ìƒ‰ ì‹¤í–‰: {layer['query'][:50]}...")
            
            # íƒ€ê²Ÿ ê²°ê³¼ ìˆ˜ ê²°ì •
            if i == 0:
                search_top_k = target_results['first_layer']
            else:
                search_top_k = target_results['other_layers']
            
            # Pinecone ê²€ìƒ‰ ì‹¤í–‰
            try:
                results = self.index.query(
                    vector=layer['embedding'],
                    top_k=search_top_k,
                    include_metadata=True
                )
                
                # ğŸ” ë²¡í„° DB ê²€ìƒ‰ ê²°ê³¼ ìƒì„¸ ë¡œê¹…
                logging.info(f"ğŸ” ë²¡í„° DB ê²€ìƒ‰ ê²°ê³¼ (ë ˆì´ì–´ {i+1}):")
                logging.info(f"   â””â”€â”€ ê²€ìƒ‰ ì¿¼ë¦¬: '{layer['query'][:100]}...'")
                logging.info(f"   â””â”€â”€ ê²€ìƒ‰ íƒ€ì…: {layer_type}")
                logging.info(f"   â””â”€â”€ ê°€ì¤‘ì¹˜: {weight}")
                logging.info(f"   â””â”€â”€ ìš”ì²­ëœ ê²°ê³¼ ìˆ˜: {search_top_k}")
                logging.info(f"   â””â”€â”€ ì‹¤ì œ ë°˜í™˜ëœ ê²°ê³¼ ìˆ˜: {len(results.matches) if hasattr(results, 'matches') else 0}")
                
                # ìƒìœ„ 3ê°œ ê²°ê³¼ ìƒì„¸ ë¡œê¹…
                if hasattr(results, 'matches') and results.matches:
                    for j, match in enumerate(results.matches[:3]):
                        similarity_score = getattr(match, 'score', 0.0)
                        metadata = getattr(match, 'metadata', {})
                        question = metadata.get('question', 'N/A')[:100]
                        answer = metadata.get('answer', 'N/A')[:100]
                        logging.info(f"   â””â”€â”€ ê²°ê³¼ {j+1}: ìœ ì‚¬ë„={similarity_score:.3f}, ì§ˆë¬¸='{question}...', ë‹µë³€='{answer}...'")
                
                # ê²°ê³¼ ì²˜ë¦¬ ë° ê°€ì¤‘ì¹˜ ì ìš©
                layer_results = self._process_layer_results(
                    results, weight, layer_type, seen_ids
                )
                
                all_results.extend(layer_results)
                
                # ì²˜ë¦¬ëœ ê²°ê³¼ ë¡œê¹…
                logging.info(f"   â””â”€â”€ ì²˜ë¦¬ëœ ê²°ê³¼ ìˆ˜: {len(layer_results)}")
                if layer_results:
                    for j, result in enumerate(layer_results[:3]):
                        logging.info(f"   â””â”€â”€ ì²˜ë¦¬ëœ ê²°ê³¼ {j+1}: ì¡°ì •ëœ ì ìˆ˜={result.get('adjusted_score', 0.0):.3f}, ì§ˆë¬¸='{result.get('question', 'N/A')[:50]}...'")
                
                # ì¡°ê¸° ì¢…ë£Œ ì¡°ê±´ í™•ì¸
                if (search_plan['early_termination_enabled'] and 
                    len(all_results) >= self.search_config['min_results_threshold'] and
                    self._check_early_termination_condition(all_results)):
                    logging.info(f"ì¡°ê¸° ì¢…ë£Œ: ë ˆì´ì–´ {i+1}ì—ì„œ ì¶©ë¶„í•œ ê²°ê³¼ íšë“")
                    sufficient_results = True
                    break
                
            except Exception as e:
                logging.error(f"ë ˆì´ì–´ {i+1} ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
                continue
        
        # ê²°ê³¼ ì •ë ¬
        all_results.sort(key=lambda x: x['adjusted_score'], reverse=True)
        
        # ğŸ” ìµœì¢… ê²€ìƒ‰ ê²°ê³¼ ìƒì„¸ ë¡œê¹…
        logging.info(f"ğŸ¯ ìµœì¢… ê²€ìƒ‰ ê²°ê³¼ ìš”ì•½:")
        logging.info(f"   â””â”€â”€ ì‹¤í–‰ëœ ë ˆì´ì–´ ìˆ˜: {len(layers)}")
        logging.info(f"   â””â”€â”€ ì´ ê²€ìƒ‰ëœ ê²°ê³¼ ìˆ˜: {len(all_results)}")
        logging.info(f"   â””â”€â”€ ì¡°ê¸° ì¢…ë£Œ ì—¬ë¶€: {'ì˜ˆ' if sufficient_results else 'ì•„ë‹ˆì˜¤'}")
        
        if all_results:
            logging.info(f"   â””â”€â”€ ìƒìœ„ 3ê°œ ê²°ê³¼:")
            for i, result in enumerate(all_results[:3]):
                score = result.get('adjusted_score', 0.0)
                question = result.get('question', 'N/A')[:80]
                answer = result.get('answer', 'N/A')[:80]
                logging.info(f"      {i+1}. ì ìˆ˜={score:.3f}, ì§ˆë¬¸='{question}...', ë‹µë³€='{answer}...'")
        
        logging.info(f"ê²€ìƒ‰ ì‹¤í–‰ ì™„ë£Œ: {len(layers)}ê°œ ë ˆì´ì–´, {len(all_results)}ê°œ ê²°ê³¼"
                    f"{', ì¡°ê¸°ì¢…ë£Œ' if sufficient_results else ''}")
        
        return all_results

    # ì„ë² ë”© ë°°ì¹˜ ìƒì„± ë©”ì„œë“œ
    # Args:
    #     embedding_requests: ì„ë² ë”© ìš”ì²­ ë¦¬ìŠ¤íŠ¸
    #     layers: ê²€ìƒ‰ ë ˆì´ì–´ ë¦¬ìŠ¤íŠ¸
    def _generate_embeddings_batch(self, embedding_requests: List[Dict], layers: List[Dict]):
        if not embedding_requests:
            return
        
        # ë°°ì¹˜ ìš”ì²­ ìƒì„±
        batch_requests = []
        for req in embedding_requests:
            api_request = APICallRequest(
                operation='embedding',
                data={'text': req['query']},
                priority=req['priority'],
                strategy=APICallStrategy.BATCH_ONLY if len(embedding_requests) > 1 else APICallStrategy.CACHE_FIRST
            )
            batch_requests.append((req, api_request))
        
        # ë°°ì¹˜ ì²˜ë¦¬ ë˜ëŠ” ê°œë³„ ì²˜ë¦¬
        if len(batch_requests) > 1:
            # ë°°ì¹˜ ì²˜ë¦¬
            logging.info(f"ì„ë² ë”© ë°°ì¹˜ ì²˜ë¦¬: {len(batch_requests)}ê°œ ìš”ì²­")
            for req_info, api_request in batch_requests:
                response = self.api_manager.process_request(api_request)
                if response.success:
                    layer_index = req_info['layer_index']
                    embedding = response.data
                    layers[layer_index]['embedding'] = embedding
                    # ë©”ëª¨ë¦¬ ìºì‹œì—ë„ ì €ì¥
                    self.embedding_cache[req_info['query']] = embedding
        else:
            # ë‹¨ì¼ ìš”ì²­
            req_info, api_request = batch_requests[0]
            response = self.api_manager.process_request(api_request)
            if response.success:
                layer_index = req_info['layer_index']
                embedding = response.data
                layers[layer_index]['embedding'] = embedding
                self.embedding_cache[req_info['query']] = embedding

    # ë ˆì´ì–´ ê²€ìƒ‰ ê²°ê³¼ ì²˜ë¦¬ ë©”ì„œë“œ
    # Args:
    #     results: Pinecone ê²€ìƒ‰ ê²°ê³¼
    #     weight: ë ˆì´ì–´ ê°€ì¤‘ì¹˜
    #     layer_type: ë ˆì´ì–´ íƒ€ì…
    #     seen_ids: ì´ë¯¸ ë³¸ ID ì§‘í•©
    # Returns:
    #     List[Dict]: ì²˜ë¦¬ëœ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
    def _process_layer_results(self, results: Dict, weight: float, layer_type: str, seen_ids: Set) -> List[Dict]:
        layer_results = []
        
        for match in results.get('matches', []):
            match_id = match['id']
            if match_id not in seen_ids:
                seen_ids.add(match_id)
                
                # ê°€ì¤‘ì¹˜ ì ìš© ì ìˆ˜ ê³„ì‚°
                adjusted_score = match['score'] * weight
                
                processed_match = {
                    'id': match_id,
                    'score': match['score'],
                    'adjusted_score': adjusted_score,
                    'search_type': layer_type,
                    'layer_weight': weight,
                    'metadata': match.get('metadata', {})
                }
                
                layer_results.append(processed_match)
        
        return layer_results

    # ì¡°ê¸° ì¢…ë£Œ ì¡°ê±´ í™•ì¸ ë©”ì„œë“œ
    # Args:
    #     results: í˜„ì¬ê¹Œì§€ì˜ ê²€ìƒ‰ ê²°ê³¼
    # Returns:
    #     bool: ì¡°ê¸° ì¢…ë£Œ ê°€ëŠ¥ ì—¬ë¶€
    def _check_early_termination_condition(self, results: List[Dict]) -> bool:
        if not results:
            return False
        
        # ìƒìœ„ ê²°ê³¼ì˜ í’ˆì§ˆ í™•ì¸
        top_results = results[:3]
        high_quality_count = sum(1 for r in top_results if r['adjusted_score'] >= self.search_config['similarity_threshold'])
        
        # ìƒìœ„ 3ê°œ ì¤‘ 2ê°œ ì´ìƒì´ ê³ í’ˆì§ˆì´ë©´ ì¡°ê¸° ì¢…ë£Œ
        return high_quality_count >= 2

    # ê²€ìƒ‰ ê²°ê³¼ í›„ì²˜ë¦¬ ë° ìµœì¢… ì ìˆ˜ ê³„ì‚° ë©”ì„œë“œ
    # Args:
    #     search_results: ê²€ìƒ‰ëœ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
    #     query: ì›ë³¸ ì§ˆë¬¸
    #     intent_analysis: ì˜ë„ ë¶„ì„ ê²°ê³¼
    #     top_k: ìµœëŒ€ ë°˜í™˜ ê²°ê³¼ ìˆ˜
    # Returns:
    #     List[Dict]: í›„ì²˜ë¦¬ëœ ìµœì¢… ê²°ê³¼
    def _postprocess_results(self, search_results: List[Dict], query: str, 
                         intent_analysis: Dict, top_k: int) -> List[Dict]:
        # ===== ğŸ” ê²€ìƒ‰ ê²°ê³¼ ë””ë²„ê·¸ ì¶œë ¥ =====
        print("\n" + "="*80)
        print(f"ğŸ” [SEARCH DEBUG] ê²€ìƒ‰ ê²°ê³¼ í›„ì²˜ë¦¬ ì‹œì‘: {len(search_results)}ê°œ ê²°ê³¼")
        print("="*80)
        for i, result in enumerate(search_results[:5]):  # ìƒìœ„ 5ê°œë§Œ ì¶œë ¥
            question = result.get('metadata', {}).get('question', 'N/A')[:100]
            print(f"ê²€ìƒ‰ê²°ê³¼ #{i+1}: ì ìˆ˜={result['score']:.3f}, ì§ˆë¬¸={question}...")
        print("="*80)
        
        if not search_results:
            logging.info("ğŸ” [SEARCH DEBUG] ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ì–´ì„œ ë¹ˆ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜")
            return []
        
        # ë²¡í„° ìœ ì‚¬ë„ ê¸°ë°˜ ë™ì  ì„ê³„ê°’ ê³„ì‚°
        scores = [r['score'] for r in search_results[:top_k*2]]
        if scores:
            # ìƒìœ„ 20%ì™€ í•˜ìœ„ 20% ì ìˆ˜ ì°¨ì´ë¡œ ë™ì  ì„ê³„ê°’ ì„¤ì •
            top_percentile = np.percentile(scores, 80)
            bottom_percentile = np.percentile(scores, 20)
            dynamic_threshold = (top_percentile + bottom_percentile) / 2
        else:
            dynamic_threshold = 0.5
        
        # ê°œë… ê´€ë ¨ì„± ê³„ì‚°ì„ ìœ„í•œ í‚¤ì›Œë“œ ì¶”ì¶œ
        key_concepts = self.text_processor.extract_key_concepts(query)
        
        final_results = []
        
        logging.info(f"ğŸ” ê²€ìƒ‰ ê²°ê³¼ í›„ì²˜ë¦¬ ì‹œì‘: {len(search_results)}ê°œ ê²°ê³¼")
        
        for i, match in enumerate(search_results[:top_k*2]):
            metadata = match.get('metadata', {})
            question = metadata.get('question', '')
            answer = metadata.get('answer', '')
            category = metadata.get('category', 'ì¼ë°˜')
            
            logging.info(f"ğŸ” ê²°ê³¼ #{i+1} ì²˜ë¦¬ ì‹œì‘: ì§ˆë¬¸='{question[:50]}...'")
            
            # ê¸°ë³¸ ì ìˆ˜
            vector_score = match['score']
            adjusted_score = match['adjusted_score']
            
            # ì˜ë„ ê´€ë ¨ì„± ê³„ì‚° (ìºì‹± ì ìš©)
            logging.info(f"ğŸ” ì˜ë„ ê´€ë ¨ì„± ê³„ì‚° ì‹œì‘: ì§ˆë¬¸='{question[:50]}...'")
            logging.info(f"ğŸ” ì‚¬ìš©ì ì˜ë„ ë¶„ì„: {intent_analysis.get('core_intent', 'N/A')}")
            intent_relevance = self._calculate_intent_relevance_cached(
                intent_analysis, question, answer
            )
            logging.info(f"ğŸ” ì˜ë„ ê´€ë ¨ì„± ê³„ì‚° ì™„ë£Œ: {intent_relevance:.3f}")
            
            # ğŸ” ì˜ë„ ê´€ë ¨ì„± ê³„ì‚° ìƒì„¸ ë¡œê·¸
            logging.info(f"ğŸ” ì˜ë„ ê´€ë ¨ì„± ê³„ì‚°:")
            logging.info(f"   â””â”€â”€ ì‚¬ìš©ì ì˜ë„: {intent_analysis.get('core_intent', 'N/A')}")
            logging.info(f"   â””â”€â”€ ê¸°ì¡´ ì§ˆë¬¸: {question[:80]}...")
            logging.info(f"   â””â”€â”€ ì˜ë„ ê´€ë ¨ì„± ì ìˆ˜: {intent_relevance:.3f}")
            
            # ê°œë… ê´€ë ¨ì„± ê³„ì‚°
            concept_relevance = self._calculate_concept_relevance(
                query, key_concepts, question, answer
            )
            
            # ìµœì¢… ì ìˆ˜ = ë²¡í„° ìœ ì‚¬ë„(60%) + ì˜ë„ ê´€ë ¨ì„±(25%) + ê°œë… ê´€ë ¨ì„±(15%)
            final_score = (adjusted_score * 0.6 + 
                        intent_relevance * 0.25 + 
                        concept_relevance * 0.15)
            
            # ğŸ” ìµœì¢… ì ìˆ˜ ê³„ì‚° ìƒì„¸ ë¡œê·¸
            logging.info(f"ğŸ” ìµœì¢… ì ìˆ˜ ê³„ì‚°:")
            logging.info(f"   â””â”€â”€ ë²¡í„° ìœ ì‚¬ë„: {adjusted_score:.3f} Ã— 0.6 = {adjusted_score * 0.6:.3f}")
            logging.info(f"   â””â”€â”€ ì˜ë„ ê´€ë ¨ì„±: {intent_relevance:.3f} Ã— 0.25 = {intent_relevance * 0.25:.3f}")
            logging.info(f"   â””â”€â”€ ê°œë… ê´€ë ¨ì„±: {concept_relevance:.3f} Ã— 0.15 = {concept_relevance * 0.15:.3f}")
            logging.info(f"   â””â”€â”€ ìµœì¢… ì ìˆ˜: {final_score:.3f}")
            
            # === dynamic_threshold í™œìš© ë¶€ë¶„ ì¶”ê°€ ===
            # ë™ì  ì„ê³„ê°’ ì‚¬ìš©: final_scoreê°€ ì•„ë‹Œ vector_scoreì— ì ìš©
            min_threshold = dynamic_threshold if i >= 3 else 0.3  # ìƒìœ„ 3ê°œëŠ” ë” ë‚®ì€ ì„ê³„ê°’
            
            # ìµœì†Œ ì„ê³„ê°’ ë˜ëŠ” ìƒìœ„ ìˆœìœ„ í™•ì¸
            if final_score >= min_threshold or i < 3:
                print(f"âœ… [SEARCH DEBUG] ê²°ê³¼ #{i+1} ì„ íƒë¨: ìµœì¢…ì ìˆ˜={final_score:.3f} (ì„ê³„ê°’={min_threshold:.3f})")
                final_results.append({
                    'score': final_score,
                    'vector_score': vector_score,
                    'intent_relevance': intent_relevance,
                    'concept_relevance': concept_relevance,
                    'question': question,
                    'answer': answer,
                    'category': category,
                    'rank': i + 1,
                    'search_type': match['search_type'],
                    'layer_weight': match.get('layer_weight', 1.0),
                    'lang': 'ko'
                })
                
                logging.debug(f"ì„ íƒ: #{i+1} ìµœì¢…ì ìˆ˜={final_score:.3f} "
                            f"(ë²¡í„°={vector_score:.3f}, ì˜ë„={intent_relevance:.3f}, "
                            f"ê°œë…={concept_relevance:.3f}) íƒ€ì…={match['search_type']}")
            else:
                print(f"âŒ [SEARCH DEBUG] ê²°ê³¼ #{i+1} ì œì™¸ë¨: ìµœì¢…ì ìˆ˜={final_score:.3f} < ì„ê³„ê°’={min_threshold:.3f}")
            
            if len(final_results) >= top_k:
                break
    
        # ===== ğŸ” ìµœì¢… ê²°ê³¼ ìš”ì•½ =====
        print("\n" + "="*80)
        print(f"ğŸ” [SEARCH DEBUG] ìµœì¢… ì„ íƒëœ ê²°ê³¼: {len(final_results)}ê°œ")
        print("="*80)
        for i, result in enumerate(final_results):
            print(f"ìµœì¢…ê²°ê³¼ #{i+1}: ì ìˆ˜={result['score']:.3f}, ì§ˆë¬¸={result['question'][:80]}...")
        print("="*80)
        
        # ë””ë²„ê·¸ íŒŒì¼ì—ë„ ì €ì¥
        try:
            with open('/home/ec2-user/python/debug_search_results.txt', 'w', encoding='utf-8') as f:
                f.write(f"ê²€ìƒ‰ ì§ˆë¬¸: {query}\n")
                f.write("="*80 + "\n")
                f.write(f"ìµœì¢… ì„ íƒëœ ê²°ê³¼: {len(final_results)}ê°œ\n")
                f.write("="*80 + "\n")
                for i, result in enumerate(final_results):
                    f.write(f"\nê²°ê³¼ #{i+1}:\n")
                    f.write(f"ì ìˆ˜: {result['score']:.3f}\n")
                    f.write(f"ì§ˆë¬¸: {result['question']}\n")
                    f.write(f"ë‹µë³€: {result['answer'][:200]}...\n")
                    f.write("-" * 40 + "\n")
        except Exception as e:
            print(f"ğŸ” [DEBUG] ê²€ìƒ‰ê²°ê³¼ íŒŒì¼ ì €ì¥ ì‹¤íŒ¨: {e}")
    
        return final_results

    # ìºì‹± ê¸°ë°˜ ì˜ë„ ê´€ë ¨ì„± ê³„ì‚° ë©”ì„œë“œ
    # Args:
    #     query_intent: ì§ˆë¬¸ì˜ ì˜ë„ ì •ë³´
    #     ref_question: ì°¸ì¡° ì§ˆë¬¸
    #     ref_answer: ì°¸ì¡° ë‹µë³€
    # Returns:
    #     float: ì˜ë„ ê´€ë ¨ì„± ì ìˆ˜
    def _calculate_intent_relevance_cached(self, query_intent: Dict, ref_question: str, ref_answer: str) -> float:
        # ê°„ë‹¨í•œ ë©”ëª¨ë¦¬ ìºì‹œ ì‚¬ìš© (ë¹ ë¥¸ ì•¡ì„¸ìŠ¤)
        cache_key = f"{query_intent.get('core_intent', '')[:20]}:{ref_question[:30]}"
        
        if cache_key in self.search_history:
            return self.search_history[cache_key]
        
        # ì˜ë„ ê´€ë ¨ì„± ê³„ì‚° (ê¸°ì¡´ ë¡œì§)
        relevance = self._calculate_intent_similarity(query_intent, ref_question, ref_answer)
        
        # ìºì‹œ ì €ì¥ (ìµœëŒ€ 1000ê°œ)
        if len(self.search_history) > 1000:
            self.search_history.clear()
        self.search_history[cache_key] = relevance
        
        return relevance

    # ì˜ë„ ìœ ì‚¬ì„± ê³„ì‚° ë©”ì„œë“œ (ê¸°ì¡´ ë¡œì§ ìœ ì§€)
    # Args:
    #     query_intent_analysis: ì§ˆë¬¸ ì˜ë„ ë¶„ì„ ê²°ê³¼
    #     ref_question: ì°¸ì¡° ì§ˆë¬¸
    #     ref_answer: ì°¸ì¡° ë‹µë³€
    # Returns:
    #     float: ì˜ë„ ìœ ì‚¬ì„± ì ìˆ˜
    def _calculate_intent_similarity(self, query_intent_analysis: dict, ref_question: str, ref_answer: str) -> float:
        try:
            # QuestionAnalyzerë¥¼ ì‚¬ìš©í•˜ë˜ ìºì‹± ì ìš©
            if not hasattr(self, '_question_analyzer'):
                logging.error("QuestionAnalyzerê°€ ì´ˆê¸°í™”ë˜ì§€ ì•ŠìŒ!")
                return 0.5
            
            logging.info(f"ğŸ” QuestionAnalyzerë¥¼ í†µí•œ ì˜ë„ ìœ ì‚¬ì„± ê³„ì‚° ì‹œì‘")
            result = self._question_analyzer.calculate_intent_similarity(
                query_intent_analysis, ref_question, ref_answer
            )
            logging.info(f"ğŸ” QuestionAnalyzer ê³„ì‚° ê²°ê³¼: {result:.3f}")
            return result
        except Exception as e:
            logging.error(f"ì˜ë„ ìœ ì‚¬ì„± ê³„ì‚° ì‹¤íŒ¨: {e}")
            return 0.3

    # ê°œë… ê´€ë ¨ì„± ê³„ì‚° ë©”ì„œë“œ (ê¸°ì¡´ ë¡œì§ ìœ ì§€)
    # Args:
    #     query: ì›ë³¸ ì§ˆë¬¸
    #     query_concepts: ì§ˆë¬¸ì—ì„œ ì¶”ì¶œëœ ê°œë…ë“¤
    #     ref_question: ì°¸ì¡° ì§ˆë¬¸
    #     ref_answer: ì°¸ì¡° ë‹µë³€
    # Returns:
    #     float: ê°œë… ê´€ë ¨ì„± ì ìˆ˜
    def _calculate_concept_relevance(self, query: str, query_concepts: List, ref_question: str, ref_answer: str) -> float:
        if not query_concepts:
            return 0.5
        
        ref_concepts = self.text_processor.extract_key_concepts(ref_question + ' ' + ref_answer)
        
        if not ref_concepts:
            return 0.3
        
        matched_concepts = 0
        total_weight = 0
        
        for query_concept in query_concepts:
            concept_weight = len(query_concept) / 10.0
            total_weight += concept_weight
            
            if query_concept in ref_concepts:
                matched_concepts += concept_weight
                continue
            
            # ë¶€ë¶„ ì¼ì¹˜ ê²€ì‚¬
            for ref_concept in ref_concepts:
                if len(query_concept) >= 3 and len(ref_concept) >= 3:
                    common_chars = set(query_concept) & set(ref_concept)
                    similarity = len(common_chars) / max(len(set(query_concept)), len(set(ref_concept)))
                    
                    if similarity >= 0.7:
                        matched_concepts += concept_weight * similarity
                        break
        
        relevance = matched_concepts / total_weight if total_weight > 0 else 0
        return min(relevance, 1.0)

    # ê²€ìƒ‰ ê²°ê³¼ ìºì‹œ í™•ì¸ ë©”ì„œë“œ
    def _check_search_cache(self, query: str, search_params: Dict) -> Optional[List[Dict]]:
        return self.api_manager.cache_manager.get_search_results_cache(query, search_params)

    # ê²€ìƒ‰ ê²°ê³¼ ìºì‹± ë©”ì„œë“œ
    def _cache_search_results(self, query: str, search_params: Dict, results: List[Dict]):
        self.api_manager.cache_manager.set_search_results_cache(
            query, search_params, results, self.search_config['cache_ttl_hours']
        )

    # ì»¨í…ìŠ¤íŠ¸ í’ˆì§ˆ ë¶„ì„ ë©”ì„œë“œ (ê¸°ì¡´ í˜¸í™˜ì„± ìœ ì§€)
    def analyze_context_quality(self, similar_answers: list, query: str) -> dict:
        if not similar_answers:
            return {
                'has_good_context': False,
                'best_score': 0.0,
                'recommended_approach': 'fallback',
                'quality_level': 'none'
            }
        
        best_answer = similar_answers[0]
        best_score = best_answer['score']
        relevance_score = best_answer.get('intent_relevance', 0.5)
        
        high_quality_count = len([ans for ans in similar_answers if ans['score'] >= 0.7])
        good_relevance_count = len([ans for ans in similar_answers if ans.get('intent_relevance', 0) >= 0.6])
        
        # ì ‘ê·¼ ë°©ì‹ ê²°ì •
        if best_score >= 0.9 and relevance_score >= 0.7:
            approach = 'direct_use'
            quality_level = 'excellent'
        elif best_score >= 0.8 and relevance_score >= 0.6:
            approach = 'direct_use'
            quality_level = 'very_high'
        elif best_score >= 0.7 and relevance_score >= 0.5:
            approach = 'gpt_with_strong_context'
            quality_level = 'high'
        elif best_score >= 0.6 and (high_quality_count + good_relevance_count) >= 2:
            approach = 'gpt_with_strong_context'
            quality_level = 'medium'
        elif best_score >= 0.4 and relevance_score >= 0.4:
            approach = 'gpt_with_weak_context'
            quality_level = 'low'
        else:
            approach = 'fallback'
            quality_level = 'very_low'
        
        return {
            'has_good_context': quality_level in ['excellent', 'very_high', 'high', 'medium'],
            'best_score': best_score,
            'relevance_score': relevance_score,
            'high_quality_count': high_quality_count,
            'good_relevance_count': good_relevance_count,
            'recommended_approach': approach,
            'quality_level': quality_level,
            'context_summary': f"í’ˆì§ˆ: {quality_level}, ì ìˆ˜: {best_score:.3f}, ê´€ë ¨ì„±: {relevance_score:.3f}"
        }

    # ìµœì  í´ë°± ë‹µë³€ ì„ íƒ ë©”ì„œë“œ (ê¸°ì¡´ í˜¸í™˜ì„± ìœ ì§€)
    def get_best_fallback_answer(self, similar_answers: list, lang: str = 'ko') -> str:
        logging.info(f"=== get_best_fallback_answer ì‹œì‘ ===")
        logging.info(f"ì…ë ¥ëœ similar_answers ê°œìˆ˜: {len(similar_answers)}")
        
        if not similar_answers:
            logging.warning("similar_answersê°€ ë¹„ì–´ìˆìŒ")
            return ""
        
        # ì ìˆ˜ì™€ í…ìŠ¤íŠ¸ í’ˆì§ˆì„ ì¢…í•© í‰ê°€
        best_answer = ""
        best_score = 0
        
        for i, ans in enumerate(similar_answers[:3]):
            score = ans['score']
            answer_text = ans['answer']
            
            # ë§¤ìš° ë†’ì€ ìœ ì‚¬ë„ë©´ ë°”ë¡œ ë°˜í™˜
            if score >= 0.9:
                logging.info(f"ë§¤ìš° ë†’ì€ ìœ ì‚¬ë„({score:.3f}) - ì›ë³¸ ë‹µë³€ ë°”ë¡œ ë°˜í™˜")
                return answer_text.strip()
            
            # ê¸°ë³¸ ì •ë¦¬
            answer_text = self.text_processor.preprocess_text(answer_text)
            
            # ì˜ì–´ ë²ˆì—­ (í•„ìš”ì‹œ ìºì‹± ì ìš©)
            if lang == 'en' and ans.get('lang', 'ko') == 'ko':
                translation_request = APICallRequest(
                    operation='translation',
                    data={'text': answer_text, 'source_lang': 'ko', 'target_lang': 'en'},
                    priority=4,
                    strategy=APICallStrategy.CACHE_FIRST
                )
                translation_response = self.api_manager.process_request(translation_request)
                if translation_response.success:
                    answer_text = translation_response.data
            
            # ë†’ì€ ìœ ì‚¬ë„ì¸ ê²½ìš° ì²« ë²ˆì§¸ ë‹µë³€ ì„ íƒ
            if score >= 0.8:
                logging.info(f"ë†’ì€ ìœ ì‚¬ë„({score:.3f})ë¡œ ë‹µë³€ #{i+1} ì§ì ‘ ì„ íƒ")
                return answer_text if answer_text else ans['answer'].strip()
            
            # ì¢…í•© ì ìˆ˜ ê³„ì‚°
            length_score = min(len(answer_text) / 200, 1.0)
            completeness_score = 1.0 if answer_text.endswith(('.', '!', '?')) else 0.8
            total_score = score * 0.8 + length_score * 0.1 + completeness_score * 0.1
            
            if total_score > best_score:
                best_score = total_score
                best_answer = answer_text
        
        # ì•ˆì „ì¥ì¹˜
        if not best_answer and similar_answers:
            logging.error("ìµœì¢… ë‹µë³€ì´ ë¹„ì–´ìˆìŒ! ì²« ë²ˆì§¸ ì›ë³¸ ë‹µë³€ ê°•ì œ ë°˜í™˜")
            return similar_answers[0]['answer'].strip()
        
        return best_answer

    # ìµœì í™” í†µê³„ ì¡°íšŒ ë©”ì„œë“œ
    def get_optimization_stats(self) -> Dict:
        return {
            'search_config': self.search_config,
            'embedding_cache_size': len(self.embedding_cache),
            'search_history_size': len(self.search_history),
            'api_manager_stats': self.api_manager.get_performance_stats()
        }

    # ìºì‹œ ì§€ìš°ê¸° ë©”ì„œë“œ
    def clear_caches(self):
        self.embedding_cache.clear()
        self.search_history.clear()
        logging.info("ê²€ìƒ‰ ì„œë¹„ìŠ¤ ìºì‹œ ì§€ì›Œì§")

    # ê²€ìƒ‰ ì„¤ì • ì—…ë°ì´íŠ¸ ë©”ì„œë“œ
    def update_search_config(self, **kwargs):
        for key, value in kwargs.items():
            if key in self.search_config:
                self.search_config[key] = value
                logging.info(f"ê²€ìƒ‰ ì„¤ì • ì—…ë°ì´íŠ¸: {key} = {value}")
