#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
í’ˆì§ˆ ê²€ì¦ ì„œë¹„ìŠ¤ ëª¨ë“ˆ
- AIê°€ ìƒì„±í•œ ë‹µë³€ì˜ í’ˆì§ˆì„ ë‹¤ê°ë„ë¡œ ê²€ì¦
- í…ìŠ¤íŠ¸ ìœ íš¨ì„±, ì™„ì„±ë„, ê´€ë ¨ì„±, í• ë£¨ì‹œë„¤ì´ì…˜ ê°ì§€
- í•œêµ­ì–´/ì˜ì–´ ë‹¤êµ­ì–´ ì§€ì› í’ˆì§ˆ ê²€ì¦ ì‹œìŠ¤í…œ
"""

import re
import logging
from typing import Dict, List, Any
from src.utils.memory_manager import memory_cleanup
from src.utils.text_preprocessor import TextPreprocessor

# ===== AI ë‹µë³€ í’ˆì§ˆ ê²€ì¦ì„ ë‹´ë‹¹í•˜ëŠ” ë©”ì¸ í´ë˜ìŠ¤ =====
class QualityValidator:
    
    # QualityValidator ì´ˆê¸°í™” - í’ˆì§ˆ ê²€ì¦ì— í•„ìš”í•œ ë„êµ¬ë“¤ ì„¤ì •
    # Args:
    #     openai_client: OpenAI API í´ë¼ì´ì–¸íŠ¸ (AI ê²€ì¦ìš©)
    def __init__(self, openai_client):
        self.openai_client = openai_client                    # GPT ê¸°ë°˜ í’ˆì§ˆ ê²€ì¦ìš©
        self.text_processor = TextPreprocessor()              # í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ ë„êµ¬
    
    # ë‹¤êµ­ì–´ í…ìŠ¤íŠ¸ ìœ íš¨ì„± ê²€ì¦ - ë©”ì¸ ì§„ì…ì 
    # Args:
    #     text: ê²€ì¦í•  í…ìŠ¤íŠ¸
    #     lang: ì–¸ì–´ ì½”ë“œ ('ko' ë˜ëŠ” 'en')
    # Returns:
    #     bool: í…ìŠ¤íŠ¸ ìœ íš¨ì„± ì—¬ë¶€
    def is_valid_text(self, text: str, lang: str = 'ko') -> bool:
        # ===== 1ë‹¨ê³„: ê¸°ë³¸ ìœ íš¨ì„± ê²€ì‚¬ =====
        if not text or len(text.strip()) < 3:
            return False
        
        # ===== 2ë‹¨ê³„: ì–¸ì–´ë³„ ì „ë¬¸ ê²€ì¦ =====
        if lang == 'ko':
            return self.is_valid_korean_text(text)          # í•œêµ­ì–´ ì „ìš© ê²€ì¦
        else:  # ì˜ì–´
            return self.is_valid_english_text(text)         # ì˜ì–´ ì „ìš© ê²€ì¦

    # í•œêµ­ì–´ í…ìŠ¤íŠ¸ ì „ìš© ìœ íš¨ì„± ê²€ì¦ ë©”ì„œë“œ
    # Args:
    #     text: ê²€ì¦í•  í•œêµ­ì–´ í…ìŠ¤íŠ¸
    # Returns:
    #     bool: í•œêµ­ì–´ í…ìŠ¤íŠ¸ ìœ íš¨ì„± ì—¬ë¶€
    def is_valid_korean_text(self, text: str) -> bool:
        # ===== 1ë‹¨ê³„: ê¸°ë³¸ ê¸¸ì´ ê²€ì¦ =====
        if not text or len(text.strip()) < 3:
            logging.info(f"í•œêµ­ì–´ ê²€ì¦ ì‹¤íŒ¨: í…ìŠ¤íŠ¸ê°€ ë„ˆë¬´ ì§§ìŒ (ê¸¸ì´: {len(text.strip()) if text else 0})")
            return False
        
        # ===== 2ë‹¨ê³„: í•œêµ­ì–´ ë¬¸ì ë¹„ìœ¨ ê³„ì‚° =====
        korean_chars = len(re.findall(r'[ê°€-í£]', text))       # í•œê¸€ ë¬¸ì ê°œìˆ˜
        total_chars = len(re.sub(r'\s', '', text))             # ê³µë°± ì œì™¸ ì „ì²´ ë¬¸ì
        
        if total_chars == 0:
            logging.info("í•œêµ­ì–´ ê²€ì¦ ì‹¤íŒ¨: ì´ ê¸€ì ìˆ˜ê°€ 0")
            return False
            
        korean_ratio = korean_chars / total_chars
        logging.info(f"í•œêµ­ì–´ ë¹„ìœ¨: {korean_ratio:.3f} (í•œêµ­ì–´: {korean_chars}, ì „ì²´: {total_chars})")
        
        # ===== 3ë‹¨ê³„: í•œêµ­ì–´ ë¹„ìœ¨ ê¸°ì¤€ ê²€ì‚¬ (ì™„í™”ëœ ê¸°ì¤€ 10%) =====
        if korean_ratio < 0.1:
            logging.info(f"í•œêµ­ì–´ ê²€ì¦ ì‹¤íŒ¨: í•œêµ­ì–´ ë¹„ìœ¨ ë¶€ì¡± ({korean_ratio:.3f} < 0.1)")
            return False
        
        # ===== 4ë‹¨ê³„: GPT í• ë£¨ì‹œë„¤ì´ì…˜ ë°©ì§€ - ë¬´ì˜ë¯¸í•œ íŒ¨í„´ ê°ì§€ =====
        meaningless_patterns = [
            r'^[a-z\s\.,;:\(\)\[\]\-_&\/\'"]+$',             # ìˆœìˆ˜ ì˜ì–´ ì†Œë¬¸ì
            r'^[A-Z\s\.,;:\(\)\[\]\-_&\/\'"]+$',             # ìˆœìˆ˜ ì˜ì–´ ëŒ€ë¬¸ì
            r'^[\s\.,;:\(\)\[\]\-_&\/\'"]+$',                # ê³µë°±/ê¸°í˜¸ë§Œ
            r'^[0-9\s\.,;:\(\)\[\]\-_&\/\'"]+$',             # ìˆ«ì/ê¸°í˜¸ë§Œ
            r'.*[Ğ°-Ñ].*',                                    # ëŸ¬ì‹œì•„ì–´ ë¬¸ì
            r'.*[Î±-Ï‰].*',                                    # ê·¸ë¦¬ìŠ¤ì–´ ë¬¸ì
        ]
        
        for pattern in meaningless_patterns:
            if re.match(pattern, text, re.IGNORECASE):
                logging.info(f"í•œêµ­ì–´ ê²€ì¦ ì‹¤íŒ¨: ë¬´ì˜ë¯¸í•œ íŒ¨í„´ ê°ì§€")
                return False
        
        # ===== 5ë‹¨ê³„: ë°˜ë³µ ë¬¸ì ì˜¤ë¥˜ ê°ì§€ =====
        # ê°™ì€ ë¬¸ìê°€ 5ë²ˆ ì´ìƒ ì—°ì†ìœ¼ë¡œ ë‚˜íƒ€ë‚˜ë©´ ë¹„ì •ìƒ í…ìŠ¤íŠ¸ë¡œ ê°„ì£¼
        if re.search(r'(.)\1{5,}', text):
            logging.info("í•œêµ­ì–´ ê²€ì¦ ì‹¤íŒ¨: ë°˜ë³µ ë¬¸ì ê°ì§€")
            return False
        
        # ===== 6ë‹¨ê³„: ì˜ì–´ ë‹¨ì–´ ê¸¸ì´ ê²€ì‚¬ (GPT ì˜¤ë¥˜ ë°©ì§€) =====
        # ê¸´ ì˜ì–´ ë‹¨ì–´ê°€ ìˆìœ¼ë©´ì„œ í•œêµ­ì–´ ë¹„ìœ¨ì´ ë‚®ìœ¼ë©´ ì˜¤ë¥˜ë¡œ íŒë‹¨
        random_pattern = r'[a-zA-Z]{8,}'                     # 8ì ì´ìƒ ì˜ì–´ ë‹¨ì–´
        if re.search(random_pattern, text) and korean_ratio < 0.3:
            logging.info(f"í•œêµ­ì–´ ê²€ì¦ ì‹¤íŒ¨: ê¸´ ì˜ì–´ ë‹¨ì–´ì™€ ë‚®ì€ í•œêµ­ì–´ ë¹„ìœ¨")
            return False
        
        # ===== 7ë‹¨ê³„: ê²€ì¦ ì™„ë£Œ =====
        logging.info("í•œêµ­ì–´ ê²€ì¦ ì„±ê³µ")
        return True

    # ì˜ì–´ í…ìŠ¤íŠ¸ ì „ìš© ìœ íš¨ì„± ê²€ì¦ ë©”ì„œë“œ
    # Args:
    #     text: ê²€ì¦í•  ì˜ì–´ í…ìŠ¤íŠ¸
    # Returns:
    #     bool: ì˜ì–´ í…ìŠ¤íŠ¸ ìœ íš¨ì„± ì—¬ë¶€
    def is_valid_english_text(self, text: str) -> bool:
        # ===== 1ë‹¨ê³„: ê¸°ë³¸ ê¸¸ì´ ê²€ì¦ =====
        if not text or len(text.strip()) < 3:
            return False
        
        # ===== 2ë‹¨ê³„: ì˜ì–´ ë¬¸ì ë¹„ìœ¨ ê³„ì‚° =====
        english_chars = len(re.findall(r'[a-zA-Z]', text))    # ì˜ë¬¸ ë¬¸ì ê°œìˆ˜
        total_chars = len(re.sub(r'\s', '', text))            # ê³µë°± ì œì™¸ ì „ì²´ ë¬¸ì
        
        if total_chars == 0:
            return False
            
        english_ratio = english_chars / total_chars
        
        # ===== 3ë‹¨ê³„: ì˜ì–´ ë¹„ìœ¨ ê¸°ì¤€ ê²€ì‚¬ (70% ì´ìƒ) =====
        if english_ratio < 0.7:  # ì˜ì–´ ë¹„ìœ¨ì´ 70% ë¯¸ë§Œì´ë©´ ë¬´íš¨
            return False
        
        # ===== 4ë‹¨ê³„: ë°˜ë³µ ë¬¸ì ì˜¤ë¥˜ ê°ì§€ =====
        if re.search(r'(.)\1{5,}', text):
            return False
        
        # ===== 5ë‹¨ê³„: ê²€ì¦ ì™„ë£Œ =====
        return True

    # AI ìƒì„± ë‹µë³€ì˜ ì™„ì„±ë„ì™€ ìœ ìš©ì„±ì„ ì¢…í•© í‰ê°€í•˜ëŠ” ë©”ì„œë“œ
    # Args:
    #     answer: ê²€ì¦í•  AI ìƒì„± ë‹µë³€
    #     query: ì›ë³¸ ì‚¬ìš©ì ì§ˆë¬¸
    #     lang: ì–¸ì–´ ì½”ë“œ
    # Returns:
    #     float: ë‹µë³€ ì™„ì„±ë„ ì ìˆ˜ (0.0 ~ 1.0)
    def check_answer_completeness(self, answer: str, query: str, lang: str = 'ko') -> float:
        try:
            # ===== 1ë‹¨ê³„: ê¸°ë³¸ ê¸¸ì´ ê²€ì‚¬ =====
            if len(answer.strip()) < 10:
                return 0.0
                
            # ===== 2ë‹¨ê³„: ì‹¤ì§ˆì  ë‚´ìš© ë¹„ìœ¨ ê²€ì‚¬ =====
            # ì¸ì‚¬ë§, ëë§ºìŒë§ ë“±ì„ ì œì™¸í•œ ìˆœìˆ˜ ì •ë³´ ë¹„ìœ¨ ê³„ì‚°
            meaningful_content_ratio = self.calculate_meaningful_content_ratio(answer, lang)
            
            # ===== 3ë‹¨ê³„: ì§ˆë¬¸-ë‹µë³€ ê´€ë ¨ì„± í‚¤ì›Œë“œ ë§¤ì¹­ =====
            # ì§ˆë¬¸ê³¼ ë‹µë³€ì—ì„œ ê³µí†µ í‚¤ì›Œë“œ ì¶”ì¶œí•˜ì—¬ ê´€ë ¨ì„± ì¸¡ì •
            query_keywords = set(self.text_processor.extract_keywords(query.lower()))
            answer_keywords = set(self.text_processor.extract_keywords(answer.lower()))
            keyword_overlap = len(query_keywords & answer_keywords)
            keyword_relevance = keyword_overlap / max(len(query_keywords), 1) if query_keywords else 0.5
            
            # ===== 4ë‹¨ê³„: ë‹µë³€ ì™„ê²°ì„± ê²€ì‚¬ =====
            # ë¬¸ì¥ì´ ì™„ì„±ë˜ì–´ ìˆëŠ”ì§€, ì¤‘ë„ì— ëŠê¸°ì§€ ì•Šì•˜ëŠ”ì§€ í™•ì¸
            completeness_score = self.check_sentence_completeness(answer, lang)
            
            # ===== 5ë‹¨ê³„: êµ¬ì²´ì„± ê²€ì‚¬ =====
            # êµ¬ì²´ì ì¸ ì •ë³´ê°€ í¬í•¨ë˜ì–´ ìˆëŠ”ì§€, ë¹ˆ ì•½ì†ë§Œ í•˜ì§€ ì•ŠëŠ”ì§€ í™•ì¸
            specificity_score = self.check_answer_specificity(answer, query, lang)
            
            # ===== 6ë‹¨ê³„: ì¢…í•© ì ìˆ˜ ê³„ì‚° (ê°€ì¤‘ í‰ê· ) =====
            final_score = (
                meaningful_content_ratio * 0.3 +    # ì˜ë¯¸ìˆëŠ” ë‚´ìš© ë¹„ìœ¨ (30%)
                keyword_relevance * 0.25 +          # í‚¤ì›Œë“œ ê´€ë ¨ì„± (25%)
                completeness_score * 0.25 +         # ë¬¸ì¥ ì™„ê²°ì„± (25%)
                specificity_score * 0.2             # êµ¬ì²´ì„± (20%)
            )
            
            # ===== 7ë‹¨ê³„: ìƒì„¸ ë¡œê¹… ë° ê²°ê³¼ ë°˜í™˜ =====
            logging.info(f"ë‹µë³€ ì™„ì„±ë„ ë¶„ì„: ë‚´ìš©ë¹„ìœ¨={meaningful_content_ratio:.2f}, "
                        f"í‚¤ì›Œë“œê´€ë ¨ì„±={keyword_relevance:.2f}, ì™„ê²°ì„±={completeness_score:.2f}, "
                        f"êµ¬ì²´ì„±={specificity_score:.2f}, ìµœì¢…ì ìˆ˜={final_score:.2f}")
            
            return min(final_score, 1.0)
            
        except Exception as e:
            # ===== ì˜ˆì™¸ ì²˜ë¦¬: ê²€ì¦ ì‹¤íŒ¨ì‹œ ì¤‘ê°„ê°’ ë°˜í™˜ =====
            logging.error(f"ë‹µë³€ ì™„ì„±ë„ ê²€ì¦ ì‹¤íŒ¨: {e}")
            return 0.5  # ì˜¤ë¥˜ì‹œ ì¤‘ê°„ê°’ ë°˜í™˜

    # í…ìŠ¤íŠ¸ì—ì„œ ì˜ë¯¸ìˆëŠ” ì‹¤ì œ ë‚´ìš©ì˜ ë¹„ìœ¨ì„ ê³„ì‚°í•˜ëŠ” ë©”ì„œë“œ
    # Args:
    #     text: ë¶„ì„í•  í…ìŠ¤íŠ¸
    #     lang: ì–¸ì–´ ì½”ë“œ
    # Returns:
    #     float: ì˜ë¯¸ìˆëŠ” ë‚´ìš© ë¹„ìœ¨ (0.0 ~ 1.0)
    def calculate_meaningful_content_ratio(self, text: str, lang: str = 'ko') -> float:
        # ===== 1ë‹¨ê³„: ê¸°ë³¸ ìœ íš¨ì„± ê²€ì‚¬ =====
        if not text:
            return 0.0
            
        # ===== 2ë‹¨ê³„: HTML íƒœê·¸ ì œê±° =====
        clean_text = re.sub(r'<[^>]+>', '', text)
        
        # ===== 3ë‹¨ê³„: ì–¸ì–´ë³„ ë¶ˆìš©êµ¬ íŒ¨í„´ ì •ì˜ =====
        if lang == 'ko':
            # í•œêµ­ì–´ ì¸ì‚¬ë§/ëë§ºìŒë§ íŒ¨í„´
            filler_patterns = [
                r'ì•ˆë…•í•˜ì„¸ìš”[^.]*\.',                              # ì¸ì‚¬ë§
                r'ê°ì‚¬[ë“œë¦½]*ë‹ˆë‹¤[^.]*\.',                         # ê°ì‚¬ ì¸ì‚¬
                r'í‰ì•ˆí•˜ì„¸ìš”[^.]*\.',                              # ë§ˆë¬´ë¦¬ ì¸ì‚¬
                r'ì£¼ë‹˜\s*ì•ˆì—ì„œ[^.]*\.',                           # ì¢…êµì  ì¸ì‚¬
                r'ë°”ì´ë¸”\s*ì• í”Œ[^.]*\.',                           # ì•± ì´ë¦„ ì–¸ê¸‰
                r'GOODTV[^.]*\.',                                # íšŒì‚¬ëª… ì–¸ê¸‰
                r'ë¬¸ì˜[í•´ì£¼ì…”ì„œ]*\s*ê°ì‚¬[^.]*\.',                   # ë¬¸ì˜ ê°ì‚¬
                r'ì•ˆë‚´[í•´]*ë“œë¦¬ê² ìŠµë‹ˆë‹¤[^.]*\.',                    # ì•ˆë‚´ ì•½ì†
                r'ë„ì›€ì´\s*[ë˜]*[ì‹œ]*[ê¸¸]*[ë°”ë¼]*[ë©°]*[^.]*\.',      # ë„ì›€ í¬ë§
                r'í•­ìƒ[^.]*ë°”ì´ë¸”\s*ì• í”Œ[^.]*\.'                   # ë§ˆë¬´ë¦¬ ë©˜íŠ¸
            ]
        else:
            # ì˜ì–´ ì¸ì‚¬ë§/ëë§ºìŒë§ íŒ¨í„´
            filler_patterns = [
                r'Hello[^.]*\.',                                  # ì¸ì‚¬ë§
                r'Thank you[^.]*\.',                              # ê°ì‚¬ ì¸ì‚¬
                r'Best regards[^.]*\.',                           # ë§ˆë¬´ë¦¬ ì¸ì‚¬
                r'God bless[^.]*\.',                              # ì¢…êµì  ì¸ì‚¬
                r'Bible App[^.]*\.',                              # ì•± ì´ë¦„ ì–¸ê¸‰
                r'GOODTV[^.]*\.',                                # íšŒì‚¬ëª… ì–¸ê¸‰
                r'We will[^.]*\.',                                # ì•½ì† í‘œí˜„
                r'Please contact[^.]*\.'                          # ì—°ë½ ìš”ì²­
            ]
        
        # ===== 4ë‹¨ê³„: ë¶ˆìš©êµ¬ ì œê±° =====
        for pattern in filler_patterns:
            clean_text = re.sub(pattern, '', clean_text, flags=re.IGNORECASE)
        
        # ===== 5ë‹¨ê³„: ê³µë°± ì •ë¦¬ =====
        clean_text = re.sub(r'\s+', ' ', clean_text).strip()
        
        # ===== 6ë‹¨ê³„: ì˜ë¯¸ìˆëŠ” ë‚´ìš© ë¹„ìœ¨ ê³„ì‚° =====
        original_length = len(re.sub(r'<[^>]+>', '', text).strip())    # ì›ë³¸ ê¸¸ì´
        meaningful_length = len(clean_text)                             # ì •ì œ í›„ ê¸¸ì´
        
        if original_length == 0:
            return 0.0
            
        ratio = meaningful_length / original_length
        return min(ratio, 1.0)

    def check_sentence_completeness(self, text: str, lang: str = 'ko') -> float:
        """ë¬¸ì¥ì´ ì™„ì„±ë˜ì–´ ìˆëŠ”ì§€ ê²€ì‚¬"""
        
        if not text:
            return 0.0
            
        # HTML íƒœê·¸ ì œê±°
        clean_text = re.sub(r'<[^>]+>', '', text).strip()
        
        if len(clean_text) < 5:
            return 0.0
        
        # ë¬¸ì¥ ë í‘œì‹œ í™•ì¸
        if lang == 'ko':
            sentence_endings = r'[.!?ë‹ˆë‹¤ìš”ìŒë©ë‹¤ìŒê¹Œë‹¤í•˜ì„¸ìš”ìŠµë‹ˆë‹¤ë‹ˆê¹Œ]'
        else:
            sentence_endings = r'[.!?]'
        
        # ë§ˆì§€ë§‰ ë¬¸ì¥ì´ ì™„ì„±ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸
        if re.search(sentence_endings + r'\s*$', clean_text):
            return 1.0
        
        # ì¤‘ê°„ì— ì™„ì„±ëœ ë¬¸ì¥ì´ ìˆëŠ”ì§€ í™•ì¸
        sentences = re.split(sentence_endings, clean_text)
        if len(sentences) > 1:
            return 0.7  # ë¶€ë¶„ì ìœ¼ë¡œ ì™„ì„±ë¨
        
        # ë¬¸ì¥ì´ ë¶ˆì™„ì „í•œ ê²½ìš°
        return 0.3

    def check_answer_specificity(self, answer: str, query: str, lang: str = 'ko') -> float:
        """ë‹µë³€ì´ êµ¬ì²´ì ì¸ ì •ë³´ë¥¼ í¬í•¨í•˜ëŠ”ì§€ ê²€ì‚¬ (ë¹ˆ ì•½ì† íŒ¨í„´ ì—„ê²© ê°ì§€)"""
        
        if not answer:
            return 0.0
        
        # ë¹ˆ ì•½ì† íŒ¨í„´ ì—„ê²© ê°ì§€
        empty_promise_score = self.detect_empty_promises(answer, lang)
        if empty_promise_score < 0.3:  # ë¹ˆ ì•½ì†ì´ ê°ì§€ë˜ë©´ ë§¤ìš° ë‚®ì€ ì ìˆ˜
            logging.warning(f"ë¹ˆ ì•½ì† íŒ¨í„´ ê°ì§€! ì ìˆ˜: {empty_promise_score:.2f}")
            return empty_promise_score
            
        specificity_score = 0.0
        
        if lang == 'ko':
            # êµ¬ì²´ì  ì •ë³´ íŒ¨í„´ (í•œêµ­ì–´) - ë” ì—„ê²©í•˜ê²Œ ê°•í™”
            specific_patterns = [
                r'\d+[ê°€ì§€ê°œë‹¨ê³„ë²ˆì§¸ì°¨ë¡€]',  # ìˆ«ìê°€ í¬í•¨ëœ ë‹¨ê³„
                r'[ë©”ë‰´ì„¤ì •í™”ë©´ë²„íŠ¼íƒ­]ì—ì„œ',    # êµ¬ì²´ì  ìœ„ì¹˜
                r'ë‹¤ìŒê³¼\s*ê°™[ì€ì´]',         # êµ¬ì²´ì  ë°©ë²• ì œì‹œ
                r'[í´ë¦­ì„ íƒí„°ì¹˜ëˆ„ë¥´]',         # êµ¬ì²´ì  ë™ì‘
                r'[ë°©ë²•ë‹¨ê³„ì ˆì°¨ê³¼ì •]',         # êµ¬ì²´ì  í”„ë¡œì„¸ìŠ¤
                r'\w+\s*ë²„íŠ¼',               # ë²„íŠ¼ëª…
                r'\w+\s*ë©”ë‰´',               # ë©”ë‰´ëª…
                r'NIV|KJV|ESV|ë²ˆì—­ë³¸',       # êµ¬ì²´ì  ë²ˆì—­ë³¸
                r'[ìƒí•˜ì¢Œìš°]ë‹¨[ì—ì˜]',         # êµ¬ì²´ì  ìœ„ì¹˜
                r'ì„¤ì •[ì—ì„œìœ¼ë¡œ]',            # ì„¤ì • ê´€ë ¨
                r'í™”ë©´\s*[ìƒí•˜ì¢Œìš°ì¤‘ì•™]',      # í™”ë©´ ìœ„ì¹˜
                r'íƒ­í•˜ì—¬|í´ë¦­í•˜ì—¬|í„°ì¹˜í•˜ì—¬',    # êµ¬ì²´ì  í–‰ë™
                r'ë‹¤ìŒ\s*ìˆœì„œ',              # ìˆœì„œ ì•ˆë‚´
                r'ë¨¼ì €|ê·¸ë‹¤ìŒ|ë§ˆì§€ë§‰ìœ¼ë¡œ'       # ë‹¨ê³„ë³„ ì•ˆë‚´
            ]
            
            # ë¹ˆ ì•½ì†/ëª¨í˜¸í•œ í‘œí˜„ íŒ¨í„´ (ë” ì—„ê²©í•˜ê²Œ)
            vague_patterns = [
                r'ì•ˆë‚´[í•´]*ë“œë¦¬ê² ìŠµë‹ˆë‹¤',
                r'ë„ì›€[ì„ì´]\s*ë“œë¦¬ê² ìŠµë‹ˆë‹¤',
                r'í™•ì¸[í•˜ê³ í•˜ì—¬í•´ì„œ]',
                r'ê²€í† [í•˜ê³ í•˜ì—¬]',
                r'ì¤€ë¹„[í•˜ê³ í•˜ê² ìŠµë‹ˆë‹¤]',
                r'ì „ë‹¬[í•˜ê³ í•˜ê² ë“œë¦¬ê² ]',
                r'ì œê³µ[í•˜ê³ í•˜ê² ë“œë¦¬ê² ]',
                r'ë…¸ë ¥[í•˜ê³ í•˜ê² ]',
                r'ì‚´í´[ë³´ê³ ë³´ê² ]',
                r'ë°©ë²•[ì„ì´]\s*ì°¾ì•„[ë“œë¦¬ê² ë³´ê² ]'
            ]
        else:
            # êµ¬ì²´ì  ì •ë³´ íŒ¨í„´ (ì˜ì–´)
            specific_patterns = [
                r'\d+\s*steps?',
                r'follow\s+these',
                r'click\s+on',
                r'go\s+to',
                r'select\s+\w+',
                r'settings?\s+menu',
                r'NIV|KJV|ESV|translation',
                r'top\s+of\s+screen',
                r'button\s+\w+'
            ]
            
            vague_patterns = [
                r'we\s+will\s+review',
                r'we\s+are\s+working',
                r'please\s+contact',
                r'will\s+be\s+available'
            ]
        
        # êµ¬ì²´ì„± ì ìˆ˜ ê³„ì‚°
        specific_count = 0
        for pattern in specific_patterns:
            specific_count += len(re.findall(pattern, answer, re.IGNORECASE))
        
        vague_count = 0
        for pattern in vague_patterns:
            vague_count += len(re.findall(pattern, answer, re.IGNORECASE))
        
        # êµ¬ì²´ì  ì •ë³´ê°€ ë§ê³  ëª¨í˜¸í•œ í‘œí˜„ì´ ì ì„ìˆ˜ë¡ ë†’ì€ ì ìˆ˜
        if specific_count > 0:
            specificity_score = specific_count / (specific_count + vague_count + 1)
        else:
            specificity_score = 0.1 if vague_count == 0 else 0.0
        
        return min(specificity_score, 1.0)

    def detect_empty_promises(self, answer: str, lang: str = 'ko') -> float:
        """ì•½ì†ë§Œ í•˜ê³  ì‹¤ì œ ë‚´ìš©ì´ ì—†ëŠ” ë¹ˆ ì•½ì† íŒ¨í„´ì„ ê°ì§€"""
        
        if not answer:
            return 0.0
        
        # HTML íƒœê·¸ ì œê±°í•˜ì—¬ ìˆœìˆ˜ í…ìŠ¤íŠ¸ë¡œ ë¶„ì„
        clean_text = re.sub(r'<[^>]+>', '', answer)
        
        if lang == 'ko':
            # ìœ„í—˜í•œ ì•½ì† í‘œí˜„ë“¤ (ì´í›„ ì‹¤ì œ ë‚´ìš©ì´ ì™€ì•¼ í•¨)
            promise_patterns = [
                r'ì•ˆë‚´[í•´]*ë“œë¦¬ê² ìŠµë‹ˆë‹¤',
                r'ë„ì›€[ì„ì´]?\s*ë“œë¦¬ê² ìŠµë‹ˆë‹¤',
                r'ë°©ë²•[ì„ì´]?\s*ì•ˆë‚´[í•´]*ë“œë¦¬ê² ìŠµë‹ˆë‹¤',
                r'ì„¤ëª…[í•´]*ë“œë¦¬ê² ìŠµë‹ˆë‹¤',
                r'ì•Œë ¤[ë“œë¦¬ê² ë“œë¦´]',
                r'ì œê³µ[í•´]*ë“œë¦¬ê² ìŠµë‹ˆë‹¤',
                r'ë„ì™€[ë“œë¦¬ê² ë“œë¦´]',
                r'ì°¾ì•„[ë“œë¦¬ê² ë“œë¦´]'
            ]
            
            # ì‹¤ì œ ë‚´ìš©ì„ ë‚˜íƒ€ë‚´ëŠ” íŒ¨í„´ë“¤
            content_patterns = [
                r'\d+\.\s*',                    # ë²ˆí˜¸ ë§¤ê¸°ê¸° (1., 2., ...)
                r'ë¨¼ì €',                       # ë‹¨ê³„ë³„ ì„¤ëª… ì‹œì‘
                r'ë‹¤ìŒê³¼?\s*ê°™[ì€ì´]',           # êµ¬ì²´ì  ë°©ë²• ì œì‹œ
                r'[ë©”ë‰´ì„¤ì •í™”ë©´ë²„íŠ¼]',           # êµ¬ì²´ì  UI ìš”ì†Œ
                r'í´ë¦­|í„°ì¹˜|ì„ íƒ|ì´ë™',          # êµ¬ì²´ì  í–‰ë™
                r'NIV|KJV|ESV',               # êµ¬ì²´ì  ë²ˆì—­ë³¸
                r'ìƒë‹¨|í•˜ë‹¨|ì¢Œì¸¡|ìš°ì¸¡',         # êµ¬ì²´ì  ìœ„ì¹˜
                r'ì„¤ì •ì—ì„œ|ë©”ë‰´ì—ì„œ',           # êµ¬ì²´ì  ê²½ë¡œ
                r'ë‹¤ìŒ\s*[ìˆœì„œë‹¨ê³„ë°©ë²•ì ˆì°¨]',    # ë‹¨ê³„ë³„ ì•ˆë‚´
                r'[0-9]+[ë²ˆì§¸ë‹¨ê³„]',           # ìˆœì„œ í‘œì‹œ
                r'í™”ë©´\s*[ìƒí•˜ì¢Œìš°ì¤‘ì•™]'        # ìœ„ì¹˜ ì„¤ëª…
            ]
        else:  # ì˜ì–´
            promise_patterns = [
                r'will\s+guide\s+you',
                r'will\s+help\s+you',
                r'will\s+show\s+you',
                r'will\s+provide',
                r'let\s+me\s+help',
                r'here[\'\"]s\s+how'
            ]
            
            content_patterns = [
                r'\d+\.\s*',
                r'first|second|third',
                r'step\s+\d+',
                r'click|tap|select',
                r'menu|setting|screen',
                r'NIV|KJV|ESV',
                r'top|bottom|left|right'
            ]
        
        # ì•½ì† í‘œí˜„ ì°¾ê¸°
        promise_count = 0
        promise_positions = []
        
        for pattern in promise_patterns:
            matches = list(re.finditer(pattern, clean_text, re.IGNORECASE))
            promise_count += len(matches)
            promise_positions.extend([match.start() for match in matches])
        
        if promise_count == 0:
            return 0.8  # ì•½ì† í‘œí˜„ì´ ì—†ìœ¼ë©´ ì¤‘ê°„ ì ìˆ˜
        
        # ì•½ì† ì´í›„ì— ì‹¤ì œ ë‚´ìš©ì´ ìˆëŠ”ì§€ í™•ì¸
        content_after_promise = 0
        total_text_after_promises = 0
        
        for pos in promise_positions:
            # ì•½ì† í‘œí˜„ ì´í›„ì˜ í…ìŠ¤íŠ¸ ì¶”ì¶œ
            text_after = clean_text[pos:]
            
            # ëë§ºìŒë§ ì œê±°í•˜ì—¬ ì‹¤ì œ ë‚´ìš©ë§Œ ê²€ì‚¬
            text_after = re.sub(r'í•­ìƒ\s*ì„±ë„ë‹˜ê»˜[^.]*\.', '', text_after, flags=re.IGNORECASE)
            text_after = re.sub(r'ê°ì‚¬í•©ë‹ˆë‹¤[^.]*\.', '', text_after, flags=re.IGNORECASE)
            text_after = re.sub(r'ì£¼ë‹˜\s*ì•ˆì—ì„œ[^.]*\.', '', text_after, flags=re.IGNORECASE)
            text_after = re.sub(r'í‰ì•ˆí•˜ì„¸ìš”[^.]*\.', '', text_after, flags=re.IGNORECASE)
            
            total_text_after_promises += len(text_after.strip())
            
            # ì‹¤ì œ ë‚´ìš© íŒ¨í„´ì´ ìˆëŠ”ì§€ í™•ì¸
            for content_pattern in content_patterns:
                if re.search(content_pattern, text_after, re.IGNORECASE):
                    content_after_promise += 1
                    break
        
        # ì ìˆ˜ ê³„ì‚°
        if promise_count > 0:
            # ì•½ì† ëŒ€ë¹„ ì‹¤ì œ ë‚´ìš© ë¹„ìœ¨
            content_ratio = content_after_promise / promise_count
            
            # ì•½ì† ì´í›„ í…ìŠ¤íŠ¸ ê¸¸ì´ ë¹„ìœ¨ (í‰ê· )
            avg_length_after = total_text_after_promises / len(promise_positions) if promise_positions else 0
            length_score = min(avg_length_after / 100, 1.0)  # 100ì ê¸°ì¤€ìœ¼ë¡œ ì •ê·œí™”
            
            # ìµœì¢… ì ìˆ˜ (ë‚´ìš© ë¹„ìœ¨ê³¼ ê¸¸ì´ë¥¼ ê³ ë ¤)
            final_score = content_ratio * 0.7 + length_score * 0.3
            
            logging.info(f"ë¹ˆ ì•½ì† ë¶„ì„: ì•½ì†={promise_count}ê°œ, ì‹¤ì œë‚´ìš©={content_after_promise}ê°œ, "
                        f"ë‚´ìš©ë¹„ìœ¨={content_ratio:.2f}, ê¸¸ì´ì ìˆ˜={length_score:.2f}, ìµœì¢…ì ìˆ˜={final_score:.2f}")
            
            return final_score
        
        return 0.5  # ê¸°ë³¸ê°’

    # AI ìƒì„± ë‹µë³€ì—ì„œ í• ë£¨ì‹œë„¤ì´ì…˜ê³¼ ì¼ê´€ì„± ë¬¸ì œë¥¼ ì¢…í•© ê°ì§€í•˜ëŠ” ë©”ì„œë“œ
    # Args:
    #     answer: ê²€ì¦í•  AI ìƒì„± ë‹µë³€
    #     query: ì›ë³¸ ì‚¬ìš©ì ì§ˆë¬¸
    #     lang: ì–¸ì–´ ì½”ë“œ
    # Returns:
    #     dict: ê°ì§€ëœ ë¬¸ì œë“¤ê³¼ ì „ì²´ ì ìˆ˜
    def detect_hallucination_and_inconsistency(self, answer: str, query: str, lang: str = 'ko') -> dict:
        # ===== 1ë‹¨ê³„: ê²€ì¦ ê²°ê³¼ êµ¬ì¡° ì´ˆê¸°í™” =====
        issues = {
            'external_app_recommendation': False,           # ì™¸ë¶€ ì•± ì¶”ì²œ ê°ì§€
            'bible_app_domain_violation': False,           # ë„ë©”ì¸ ìœ„ë°˜ ê°ì§€
            'content_inconsistency': False,                # ë‚´ìš© ì¼ê´€ì„± ë¬¸ì œ
            'translation_switching': False,                # ë²ˆì—­ë³¸ ë³€ê²½ ë¬¸ì œ
            'invalid_features': False,                     # ì¡´ì¬í•˜ì§€ ì•ŠëŠ” ê¸°ëŠ¥ ì•ˆë‚´
            'overall_score': 1.0,                         # ì „ì²´ ì ìˆ˜ (1.0 = ì™„ë²½)
            'detected_issues': []                          # ê°ì§€ëœ ë¬¸ì œ ëª©ë¡
        }
        
        # ===== 2ë‹¨ê³„: ê¸°ë³¸ ìœ íš¨ì„± ê²€ì‚¬ =====
        if not answer:
            return issues
        
        # ===== 3ë‹¨ê³„: í…ìŠ¤íŠ¸ ì •ì œ (HTML íƒœê·¸ ì œê±°) =====
        clean_answer = re.sub(r'<[^>]+>', '', answer)
        clean_query = re.sub(r'<[^>]+>', '', query)
        
        if lang == 'ko':
            # ===== 4ë‹¨ê³„: ì™¸ë¶€ ì•± ì¶”ì²œ ê°ì§€ (ì¹˜ëª…ì  ì˜¤ë¥˜) =====
            external_app_patterns = [
                r'Parallel\s*Bible',                           # ì™¸ë¶€ ì„±ê²½ ì•±ëª…
                r'ë³‘ë ¬\s*ì„±ê²½\s*ì•±',                             # ì™¸ë¶€ ì•± ì–¸ê¸‰
                r'ë‹¤ë¥¸\s*ì•±ì„?\s*(ë‹¤ìš´ë¡œë“œ|ì„¤ì¹˜)',                # ë‹¤ë¥¸ ì•± ì„¤ì¹˜ ìœ ë„
                r'ì•±\s*ìŠ¤í† ì–´ì—ì„œ\s*(ê²€ìƒ‰|ë‹¤ìš´ë¡œë“œ)',             # ì•±ìŠ¤í† ì–´ ìœ ë„
                r'êµ¬ê¸€\s*í”Œë ˆì´\s*ìŠ¤í† ì–´',                       # ì™¸ë¶€ ìŠ¤í† ì–´ ì–¸ê¸‰
                r'ì™¸ë¶€\s*(ì•±|ì–´í”Œë¦¬ì¼€ì´ì…˜)',                     # ëª…ì‹œì  ì™¸ë¶€ ì•±
                r'ë³„ë„[ì˜]*\s*(ì•±|ì–´í”Œ)',                       # ë³„ë„ ì•± ì–¸ê¸‰
                r'ì¶”ê°€ë¡œ\s*(ì•±ì„|ì–´í”Œì„)\s*ì„¤ì¹˜'                 # ì¶”ê°€ ì•± ì„¤ì¹˜ ìœ ë„
            ]
            
            for pattern in external_app_patterns:
                if re.search(pattern, clean_answer, re.IGNORECASE):
                    issues['external_app_recommendation'] = True
                    issues['detected_issues'].append(f"ì™¸ë¶€ ì•± ì¶”ì²œ ê°ì§€: {pattern}")
                    issues['overall_score'] -= 0.8  # ë§¤ìš° ì‹¬ê°í•œ ê°ì  (80% ê°ì )
            
            # ===== 5ë‹¨ê³„: ë²ˆì—­ë³¸ ë³€ê²½/êµì²´ ê°ì§€ (ì¼ê´€ì„± ìœ„ë°˜) =====
            # ì§ˆë¬¸ì—ì„œ ì–¸ê¸‰í•œ ë²ˆì—­ë³¸ì´ ë‹µë³€ì—ì„œ ë‹¤ë¥¸ ë²ˆì—­ë³¸ìœ¼ë¡œ ë°”ë€Œë©´ ë¬¸ì œ
            query_translations = self.text_processor.extract_translations_from_text(clean_query)
            answer_translations = self.text_processor.extract_translations_from_text(clean_answer)
            
            if query_translations and answer_translations:
                # ì§ˆë¬¸ê³¼ ë‹µë³€ì˜ ë²ˆì—­ë³¸ ì§‘í•© ë¹„êµ
                query_set = set(query_translations)
                answer_set = set(answer_translations)
                
                # ì§ˆë¬¸ì— ì—†ë˜ ë²ˆì—­ë³¸ì´ ë‹µë³€ì— ì¶”ê°€ë˜ì—ˆëŠ”ì§€ í™•ì¸
                unexpected_translations = answer_set - query_set
                if unexpected_translations:
                    # ì–¸ì–´ ê³„ì—´ì´ ì™„ì „íˆ ë‹¤ë¥¸ ë²ˆì—­ë³¸ ë³€ê²½ì€ ê¸ˆì§€
                    # ì˜ˆ: ê°œì—­í•œê¸€(í•œêµ­ì–´) â†’ NIV(ì˜ì–´) ë³€ê²½
                    problematic = False
                    for trans in unexpected_translations:
                        # ì˜ì–´ ë²ˆì—­ë³¸ìœ¼ë¡œ ë³€ê²½ (ì›ë˜ ì§ˆë¬¸ì€ í•œêµ­ì–´ ë²ˆì—­ë³¸)
                        if any(forbidden in trans.lower() for forbidden in ['ì˜ì–´', 'english', 'niv', 'kjv', 'esv']) and \
                           not any(allowed in q_trans.lower() for q_trans in query_translations for allowed in ['ì˜ì–´', 'english', 'niv', 'kjv', 'esv']):
                            problematic = True
                            break
                        # í•œêµ­ì–´ ë²ˆì—­ë³¸ìœ¼ë¡œ ë³€ê²½ (ì›ë˜ ì§ˆë¬¸ì€ ì˜ì–´ ë²ˆì—­ë³¸)
                        elif any(forbidden in trans.lower() for forbidden in ['í•œê¸€', 'ê°œì—­', 'korean']) and \
                             not any(allowed in q_trans.lower() for q_trans in query_translations for allowed in ['í•œê¸€', 'ê°œì—­', 'korean']):
                            problematic = True
                            break
                    
                    if problematic:
                        issues['translation_switching'] = True
                        issues['detected_issues'].append(f"ë²ˆì—­ë³¸ ë³€ê²½: {query_translations} â†’ {list(unexpected_translations)}")
                        issues['overall_score'] -= 0.7  # ì‹¬ê°í•œ ê°ì  (70% ê°ì )
        
        # ===== 6ë‹¨ê³„: ì¡´ì¬í•˜ì§€ ì•ŠëŠ” ê¸°ëŠ¥ ì•ˆë‚´ ê°ì§€ (ì¹˜ëª…ì  ì˜¤ë¥˜) =====
        # ì‹¤ì œ ì•±ì— ì—†ëŠ” ê¸°ëŠ¥ì„ ì•ˆë‚´í•˜ëŠ” ê²½ìš°ë¥¼ ê°ì§€
        invalid_feature_result = self._detect_non_existent_features(clean_answer, clean_query, lang)
        if invalid_feature_result:
            issues['invalid_features'] = True
            issues['detected_issues'].append("ì¡´ì¬í•˜ì§€ ì•ŠëŠ” ê¸°ëŠ¥ì— ëŒ€í•œ ì˜ëª»ëœ ì•ˆë‚´ ê°ì§€")
            issues['overall_score'] -= 0.9  # ë§¤ìš° ì‹¬ê°í•œ ê°ì  (90% ê°ì )
        else:
            issues['invalid_features'] = False
        
        # ===== 7ë‹¨ê³„: ìµœì¢… ì ìˆ˜ ì •ê·œí™” ë° ì•ˆì „ì¥ì¹˜ =====
        issues['overall_score'] = max(issues['overall_score'], 0.0)  # ìŒìˆ˜ ë°©ì§€
        
        # ===== 8ë‹¨ê³„: ì¹˜ëª…ì  ë¬¸ì œ ì¢…í•© í‰ê°€ =====
        # í•˜ë‚˜ë¼ë„ ì¹˜ëª…ì  ë¬¸ì œê°€ ìˆìœ¼ë©´ ì „ì²´ ì ìˆ˜ë¥¼ ë§¤ìš° ë‚®ê²Œ ì„¤ì •
        critical_issues = [
            issues['external_app_recommendation'],         # ì™¸ë¶€ ì•± ì¶”ì²œ
            issues['translation_switching'],              # ë²ˆì—­ë³¸ ë¬´ë‹¨ ë³€ê²½
            issues['invalid_features']                    # ì¡´ì¬í•˜ì§€ ì•ŠëŠ” ê¸°ëŠ¥ ì•ˆë‚´
        ]
        
        if any(critical_issues):
            issues['overall_score'] = min(issues['overall_score'], 0.1)  # ìµœëŒ€ 10%ë§Œ í—ˆìš©
        
        # ===== 9ë‹¨ê³„: ê²€ì¦ ê²°ê³¼ ë¡œê¹… ë° ë°˜í™˜ =====
        logging.info(f"í• ë£¨ì‹œë„¤ì´ì…˜ ê²€ì¦ ê²°ê³¼: ì ìˆ˜={issues['overall_score']:.2f}, ë¬¸ì œ={len(issues['detected_issues'])}ê°œ")
        
        return issues
    
    def _detect_non_existent_features(self, answer: str, query: str, lang: str = 'ko') -> bool:
        """ì¡´ì¬í•˜ì§€ ì•ŠëŠ” ê¸°ëŠ¥ì— ëŒ€í•œ ì˜ëª»ëœ ì•ˆë‚´ ê°ì§€"""
        
        if lang == 'ko':
            # 1. ì¡´ì¬í•˜ì§€ ì•ŠëŠ” ì•Œë¦¼ ì„¸ë¶€ ì„¤ì • ê¸°ëŠ¥ë“¤
            invalid_notification_patterns = [
                r'ì£¼ì¼ì—ë§Œ\s*(ì•Œë¦¼|ì˜ˆë°°\s*ì•Œë¦¼).*ì„¤ì •',
                r'ìš”ì¼ë³„.*ì•Œë¦¼.*ì„¤ì •',
                r'íŠ¹ì •\s*ìš”ì¼.*ì•Œë¦¼.*ë°›ê¸°',
                r'ì›”ìš”ì¼|í™”ìš”ì¼|ìˆ˜ìš”ì¼|ëª©ìš”ì¼|ê¸ˆìš”ì¼|í† ìš”ì¼|ì¼ìš”ì¼.*ë§Œ.*ì•Œë¦¼',
                r'ì£¼ì¤‘|ì£¼ë§.*ë§Œ.*ì•Œë¦¼.*ì„¤ì •',
                r'ì‹œê°„ëŒ€ë³„.*ì•Œë¦¼.*ì»¤ìŠ¤í„°ë§ˆì´ì§•',
                r'ê°œë³„.*ìš”ì¼.*ì„ íƒ.*ì•Œë¦¼'
            ]
            
            # 2. ì¡´ì¬í•˜ì§€ ì•ŠëŠ” ì„¤ì • ë©”ë‰´ ê²½ë¡œë“¤
            invalid_menu_patterns = [
                r'ì„¤ì •.*ë©”ë‰´ì—ì„œ.*"?ì£¼ì¼"?.*ì„ íƒ',
                r'ì•Œë¦¼.*ì„¤ì •.*"?ìš”ì¼"?.*ì„ íƒ',
                r'ì£¼ì¼.*ì˜µì…˜.*ì„ íƒí•˜ê³ .*ì €ì¥',
                r'ìš”ì¼.*ì„¤ì •.*ë©”ë‰´.*ë“¤ì–´ê°€ì„œ',
                r'"?ì£¼ì¼\s*ì•Œë¦¼"?.*í•­ëª©.*ì°¾ì•„ì„œ',
                r'ì£¼ì¼.*ì²´í¬ë°•ìŠ¤.*ì„ íƒ',
                r'ìš”ì¼ë³„.*ì²´í¬.*í•´ì œ'
            ]
            
            # 3. ì¡´ì¬í•˜ì§€ ì•ŠëŠ” ê³ ê¸‰ ê¸°ëŠ¥ë“¤
            invalid_advanced_patterns = [
                r'ë§ì¶¤í˜•.*ì•Œë¦¼.*ìŠ¤ì¼€ì¤„.*ì„¤ì •',
                r'ê°œì¸í™”ëœ.*ì•Œë¦¼.*ì‹œê°„.*ì¡°ì •',
                r'ì„¸ë°€í•œ.*ì•Œë¦¼.*ì˜µì…˜.*ì„¤ì •',
                r'ê³ ê¸‰.*ì•Œë¦¼.*ì„¤ì •.*ë©”ë‰´',
                r'ìƒì„¸.*ì•Œë¦¼.*ì»¤ìŠ¤í„°ë§ˆì´ì§•',
                r'ì•Œë¦¼.*ë¹ˆë„.*ì„¸ë¶€.*ì¡°ì •'
            ]
            
            # 4. íŠ¹ì • ì§ˆë¬¸ ìœ í˜•ë³„ ì¡´ì¬í•˜ì§€ ì•ŠëŠ” ê¸°ëŠ¥ë“¤
            query_specific_patterns = []
            
            # ì£¼ì¼ ì•Œë¦¼ ê´€ë ¨ ì§ˆë¬¸ì— ëŒ€í•œ ì˜ëª»ëœ ë‹µë³€ íŒ¨í„´
            if re.search(r'ì£¼ì¼.*ë§Œ.*ì•Œë¦¼|ì£¼ì¼.*ì˜ˆë°°.*ì•Œë¦¼', query, re.IGNORECASE):
                query_specific_patterns.extend([
                    r'ì£¼ì¼.*ì„ íƒí•˜ê³ .*ì €ì¥.*ë²„íŠ¼',
                    r'ì£¼ì¼.*ì²´í¬.*í‘œì‹œ.*í•˜ì„¸ìš”',
                    r'ì£¼ì¼.*ì˜µì…˜.*í™œì„±í™”.*í•˜ë©´',
                    r'ì£¼ì¼.*ì„¤ì •.*ì™„ë£Œ.*í•˜ì„¸ìš”'
                ])
            
            # ëª¨ë“  íŒ¨í„´ ê²€ì‚¬
            all_patterns = (invalid_notification_patterns + 
                          invalid_menu_patterns + 
                          invalid_advanced_patterns + 
                          query_specific_patterns)
            
            for pattern in all_patterns:
                if re.search(pattern, answer, re.IGNORECASE):
                    logging.error(f"ì¡´ì¬í•˜ì§€ ì•ŠëŠ” ê¸°ëŠ¥ ì•ˆë‚´ ê°ì§€: '{pattern}' íŒ¨í„´ ë§¤ì¹­")
                    return True
            
            # 5. ì‹¤ì œ ì•±ì— ì—†ëŠ” UI ìš”ì†Œ ì–¸ê¸‰ ê°ì§€
            ui_element_patterns = [
                r'"?ì£¼ì¼"?.*ë²„íŠ¼.*ëˆŒëŸ¬',
                r'"?ìš”ì¼.*ì„ íƒ"?.*ë©”ë‰´',
                r'"?ì£¼ì¼.*ì•Œë¦¼"?.*ì²´í¬ë°•ìŠ¤',
                r'"?ìš”ì¼ë³„.*ì„¤ì •"?.*ì˜µì…˜',
                r'ì£¼ì¼.*ë“œë¡­ë‹¤ìš´.*ë©”ë‰´'
            ]
            
            for pattern in ui_element_patterns:
                if re.search(pattern, answer, re.IGNORECASE):
                    logging.error(f"ì¡´ì¬í•˜ì§€ ì•ŠëŠ” UI ìš”ì†Œ ì–¸ê¸‰ ê°ì§€: '{pattern}' íŒ¨í„´ ë§¤ì¹­")
                    return True
        
        return False

    # GPTë¥¼ í™œìš©í•œ AI ë‹µë³€ ê´€ë ¨ì„± ì—„ê²© ê²€ì¦ ë©”ì„œë“œ
    # Args:
    #     answer: ê²€ì¦í•  AI ìƒì„± ë‹µë³€
    #     query: ì›ë³¸ ì‚¬ìš©ì ì§ˆë¬¸
    #     question_analysis: ì§ˆë¬¸ ë¶„ì„ ê²°ê³¼
    # Returns:
    #     bool: ë‹µë³€ì´ ì§ˆë¬¸ê³¼ ê´€ë ¨ì„±ì´ ìˆëŠ”ì§€ ì—¬ë¶€
    def validate_answer_relevance_ai(self, answer: str, query: str, question_analysis: dict) -> bool:
        try:
            # ===== ë©”ëª¨ë¦¬ ìµœì í™” ì»¨í…ìŠ¤íŠ¸ ì‹œì‘ =====
            with memory_cleanup():
                # ===== 1ë‹¨ê³„: GPT ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ êµ¬ì„± =====
                # ë‹µë³€-ì§ˆë¬¸ ì¼ì¹˜ë„ë¥¼ ì—„ê²©í•˜ê²Œ í‰ê°€í•˜ëŠ” ì „ë¬¸ê°€ ì—­í•  ë¶€ì—¬
                system_prompt = """ë‹¹ì‹ ì€ ë‹µë³€ í’ˆì§ˆ ê²€ì¦ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
ìƒì„±ëœ ë‹µë³€ì´ ê³ ê°ì˜ ì§ˆë¬¸ì— ì ì ˆíˆ ëŒ€ì‘í•˜ëŠ”ì§€ ì—„ê²©í•˜ê²Œ í‰ê°€í•˜ì„¸ìš”.

âš ï¸ ì—„ê²©í•œ í‰ê°€ ê¸°ì¤€:
1. ë‹µë³€ì´ ì§ˆë¬¸ì˜ í•µì‹¬ í–‰ë™ ìš”ì²­ê³¼ ì¼ì¹˜í•˜ëŠ”ê°€? (ë³µì‚¬â‰ ì¬ìƒ)
2. ë‹µë³€ì´ ì§ˆë¬¸ì˜ ì£¼ì œ ì˜ì—­ê³¼ ì¼ì¹˜í•˜ëŠ”ê°€? (í…ìŠ¤íŠ¸â‰ ìŒì„±)
3. ë‹µë³€ì´ ì‹¤ì œ ë¬¸ì œ í•´ê²°ì— ì§ì ‘ì ìœ¼ë¡œ ë„ì›€ì´ ë˜ëŠ”ê°€?
4. ë‹µë³€ì—ì„œ ì–¸ê¸‰í•˜ëŠ” ê¸°ëŠ¥ì´ ì§ˆë¬¸ì—ì„œ ìš”ì²­í•œ ê¸°ëŠ¥ê³¼ ê°™ì€ê°€?

ğŸš« ë¶€ì ì ˆí•œ ë‹µë³€ ì˜ˆì‹œ:
- í…ìŠ¤íŠ¸ ë³µì‚¬ ì§ˆë¬¸ì— ìŒì„± ì¬ìƒ ë‹µë³€
- ê²€ìƒ‰ ê¸°ëŠ¥ ì§ˆë¬¸ì— ì„¤ì • ë³€ê²½ ë‹µë³€  
- ì˜¤ë¥˜ ì‹ ê³ ì— ì¼ë°˜ ì‚¬ìš©ë²• ë‹µë³€
- êµ¬ì²´ì  ê¸°ëŠ¥ ì§ˆë¬¸ì— ì¶”ìƒì  ì•ˆë‚´ ë‹µë³€

ê²°ê³¼: "relevant" ë˜ëŠ” "irrelevant" ì¤‘ í•˜ë‚˜ë§Œ ë°˜í™˜í•˜ì„¸ìš”."""

                # ===== 2ë‹¨ê³„: ì‚¬ìš©ì í”„ë¡¬í”„íŠ¸ êµ¬ì„± (ìƒì„¸ ë¶„ì„ ì •ë³´ í¬í•¨) =====
                user_prompt = f"""ì§ˆë¬¸ ë¶„ì„:
ì˜ë„: {question_analysis.get('intent_type', 'N/A')}
ì£¼ì œ: {question_analysis.get('main_topic', 'N/A')}
í–‰ë™ìœ í˜•: {question_analysis.get('action_type', 'N/A')}
ìš”ì²­ì‚¬í•­: {question_analysis.get('specific_request', 'N/A')}

ì›ë³¸ ì§ˆë¬¸: {query}

ìƒì„±ëœ ë‹µë³€: {answer}

âš ï¸ íŠ¹íˆ ì£¼ì˜: ì§ˆë¬¸ì˜ í–‰ë™ìœ í˜•ê³¼ ë‹µë³€ì—ì„œ ë‹¤ë£¨ëŠ” í–‰ë™ì´ ë‹¤ë¥´ë©´ "irrelevant"ì…ë‹ˆë‹¤.
ì´ ë‹µë³€ì´ ì§ˆë¬¸ì— ì ì ˆí•œì§€ ì—„ê²©í•˜ê²Œ í‰ê°€í•´ì£¼ì„¸ìš”."""

                # ===== 3ë‹¨ê³„: GPT API í˜¸ì¶œ (ê´€ë ¨ì„± ê²€ì¦) =====
                response = self.openai_client.chat.completions.create(
                    model='gpt-5-mini',
                    messages=[
                        {"role": "system", "content": system_prompt},
                        {"role": "user", "content": user_prompt}
                    ],
                    max_completion_tokens=30,                              # ì§§ì€ ë‹µë³€ (relevant/irrelevant)
                    temperature=0.1                             # ì¼ê´€ì„± ì¤‘ì‹œ (ë‚®ì€ ì°½ì˜ì„±)
                )
                
                # ===== 4ë‹¨ê³„: GPT ì‘ë‹µ ë¶„ì„ ë° ê²°ê³¼ íŒì • =====
                result = response.choices[0].message.content.strip().lower()
                
                # "relevant"ê°€ í¬í•¨ë˜ê³  "irrelevant"ê°€ ì—†ìœ¼ë©´ ê´€ë ¨ì„± ìˆìŒ
                is_relevant = 'relevant' in result and 'irrelevant' not in result
                
                logging.info(f"AI ë‹µë³€ ê´€ë ¨ì„± ê²€ì¦: {result} -> {is_relevant}")
                
                return is_relevant
                
        except Exception as e:
            # ===== ì˜ˆì™¸ ì²˜ë¦¬: GPT ì‹¤íŒ¨ì‹œ í´ë°± ë¡œì§ =====
            logging.error(f"AI ë‹µë³€ ê´€ë ¨ì„± ê²€ì¦ ì‹¤íŒ¨: {e}")
            
            # ê¸°ë³¸ì ì¸ í‚¤ì›Œë“œ ë§¤ì¹­ìœ¼ë¡œ í´ë°±
            query_keywords = set(self.text_processor.extract_keywords(query.lower()))
            answer_keywords = set(self.text_processor.extract_keywords(answer.lower()))
            
            keyword_overlap = len(query_keywords & answer_keywords)
            keyword_relevance = keyword_overlap / max(len(query_keywords), 1)
            
            # 20% ì´ìƒ í‚¤ì›Œë“œ ì¼ì¹˜ì‹œ ê´€ë ¨ì„± ìˆìŒìœ¼ë¡œ íŒë‹¨
            return keyword_relevance >= 0.2

    # í´ë˜ìŠ¤ ëë¶€ë¶„ì— ì¶”ê°€ (ê¸°ì¡´ ë©”ì„œë“œë“¤ê³¼ ì¶©ëŒ ì—†ìŒ)
def check_semantic_consistency(self, query: str, answer: str) -> Dict[str, Any]:
    """ì˜ë¯¸ì  ì¼ê´€ì„± ì‹¤ì‹œê°„ ê²€ì¦"""
    try:
        # HTML íƒœê·¸ ì œê±°
        clean_answer = re.sub(r'<[^>]+>', '', answer)
        
        # ì§ˆë¬¸ê³¼ ë‹µë³€ì—ì„œ í•µì‹¬ ê°œë… ì¶”ì¶œ
        query_concepts = self.text_processor.extract_key_concepts(query)
        answer_concepts = self.text_processor.extract_key_concepts(clean_answer)
        
        # ê°œë…ì´ ì—†ìœ¼ë©´ ì¤‘ë¦½ ë°˜í™˜
        if not query_concepts:
            return {"consistent": True, "confidence": 0.5, "missing_concepts": []}
        
        if not answer_concepts:
            return {
                "consistent": False, 
                "confidence": 0.0, 
                "missing_concepts": query_concepts
            }
        
        # ì§‘í•© ì—°ì‚°
        query_set = set(query_concepts)
        answer_set = set(answer_concepts)
        common_concepts = query_set & answer_set
        union_concepts = query_set | answer_set
        
        # êµì§‘í•© ë¹„ìœ¨ ê³„ì‚°
        overlap_ratio = len(common_concepts) / len(union_concepts) if union_concepts else 0
        
        # ì˜ë¯¸ì  ê±°ë¦¬ ê³„ì‚°
        semantic_distance = 1.0 - overlap_ratio
        
        return {
            "consistent": overlap_ratio > 0.3,
            "confidence": overlap_ratio,
            "semantic_distance": semantic_distance,
            "query_concepts": list(query_set),
            "answer_concepts": list(answer_set),
            "missing_concepts": list(query_set - answer_set)
        }
        
    except Exception as e:
        logging.error(f"ì˜ë¯¸ì  ì¼ê´€ì„± ê²€ì‚¬ ì‹¤íŒ¨: {e}")
        return {"consistent": True, "confidence": 0.5, "missing_concepts": []}
