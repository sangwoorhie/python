#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
í†µí•© í…ìŠ¤íŠ¸ ë¶„ì„ê¸° ëª¨ë“ˆ
- ì˜¤íƒ€ ìˆ˜ì •ê³¼ ì˜ë„ ë¶„ì„ì„ í•œ ë²ˆì˜ GPT í˜¸ì¶œë¡œ ì²˜ë¦¬
- API ë¹„ìš© ì ˆê° ë° ì²˜ë¦¬ ì„±ëŠ¥ ìµœì í™”
"""

import logging
import json
from typing import Dict, Tuple
from src.utils.memory_manager import memory_cleanup

class UnifiedTextAnalyzer:
    """ì˜¤íƒ€ ìˆ˜ì • + ì˜ë„ ë¶„ì„ì„ í†µí•©í•œ ë¶„ì„ê¸°"""
    
    def __init__(self, openai_client):
        self.openai_client = openai_client
        self.model = 'gpt-5-mini'
    
    # í•œ ë²ˆì˜ GPT í˜¸ì¶œë¡œ ì˜¤íƒ€ ìˆ˜ì •ê³¼ ì˜ë„ ë¶„ì„ì„ ë™ì‹œì— ìˆ˜í–‰    
    # Args:
    #     text: ë¶„ì„í•  í…ìŠ¤íŠ¸
    # Returns:
    #     Tuple[str, Dict]: (ìˆ˜ì •ëœ_í…ìŠ¤íŠ¸, ì˜ë„_ë¶„ì„_ê²°ê³¼)
    def analyze_and_correct(self, text: str) -> Tuple[str, Dict]:
        try:
            with memory_cleanup():
                logging.info(f"í†µí•© ë¶„ì„ê¸° ì‹œì‘: ì…ë ¥ í…ìŠ¤íŠ¸='{text}'")
                
                # í†µí•© ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸
                system_prompt = """ë‹¹ì‹ ì€ ë°”ì´ë¸” ì•± ë¬¸ì˜ ì „ë¬¸ ë¶„ì„ê°€ì…ë‹ˆë‹¤.
    ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ëŒ€í•´ ë‹¤ìŒ ë‘ ê°€ì§€ ì‘ì—…ì„ ë™ì‹œì— ìˆ˜í–‰í•˜ì„¸ìš”:

    1. ë§ì¶¤ë²• ë° ì˜¤íƒ€ ìˆ˜ì • (ì˜ë¯¸ì™€ ì–´ì¡°ëŠ” ìœ ì§€)
    2. ì§ˆë¬¸ì˜ ë³¸ì§ˆì  ì˜ë„ ë¶„ì„

    ì‘ë‹µ í˜•ì‹ (JSON):
    {
        "corrected_text": "ì˜¤íƒ€ê°€ ìˆ˜ì •ëœ í…ìŠ¤íŠ¸",
        "intent_analysis": {
            "core_intent": "í•µì‹¬ ì˜ë„ (í‘œì¤€í™”ëœ í˜•íƒœ)",
            "intent_category": "ì˜ë„ ì¹´í…Œê³ ë¦¬",
            "primary_action": "ì£¼ìš” í–‰ë™",
            "target_object": "ëŒ€ìƒ ê°ì²´",
            "constraint_conditions": ["ì œì•½ ì¡°ê±´ë“¤"],
            "standardized_query": "í‘œì¤€í™”ëœ ì§ˆë¬¸ í˜•íƒœ",
            "semantic_keywords": ["ì˜ë¯¸ë¡ ì  í•µì‹¬ í‚¤ì›Œë“œë“¤"]
        }
    }

    ì˜¤íƒ€ ìˆ˜ì • ì§€ì¹¨:
    - ì•±/ì–´í”Œë¦¬ì¼€ì´ì…˜ â†’ ì•±ìœ¼ë¡œ í†µì¼
    - ë„ì–´ì“°ê¸°, ë§ì¶¤ë²•, ì¡°ì‚¬ ì‚¬ìš©ë²• ì •í™•íˆ êµì •
    - ì›ë¬¸ì˜ ì˜ë¯¸ì™€ ì–´ì¡°ëŠ” ì ˆëŒ€ ë³€ê²½ ê¸ˆì§€

    ì˜ë„ ë¶„ì„ ì§€ì¹¨:
    - ì§ˆë¬¸ì˜ ë³¸ì§ˆì  ëª©ì  íŒŒì•…
    - êµ¬ì²´ì  ì˜ˆì‹œ ì œê±°í•˜ê³  ì¼ë°˜í™”
    - ì˜ë¯¸ë¡ ì ìœ¼ë¡œ ë™ë“±í•œ ì§ˆë¬¸ë“¤ì´ ê°™ì€ ê²°ê³¼ ë„ì¶œí•˜ë„ë¡ ë¶„ì„

    ì¤‘ìš”: ë°˜ë“œì‹œ ìœ íš¨í•œ JSON í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µí•˜ê³ , ë‹¤ë¥¸ ì„¤ëª…ì€ í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”."""

                user_prompt = f"ë‹¤ìŒ í…ìŠ¤íŠ¸ë¥¼ ë¶„ì„í•´ì£¼ì„¸ìš”:\n\n{text}"
                
                # GPT API í˜¸ì¶œ (gpt-5-mini ëª¨ë¸ì— ë§ëŠ” íŒŒë¼ë¯¸í„° ì‚¬ìš©)
                response = self.openai_client.chat.completions.create(
                    model=self.model,
                    messages=[
                        {"role": "system", "content": system_prompt},
                        {"role": "user", "content": user_prompt}
                    ],
                    max_completion_tokens=600
                    # temperature íŒŒë¼ë¯¸í„° ì œê±° (gpt-5-miniì—ì„œ ì§€ì›í•˜ì§€ ì•ŠìŒ)
                )
                
                result_text = response.choices[0].message.content.strip()
                
                # ğŸ” GPT ì‘ë‹µ ê²€ì¦ ë° ë¡œê¹… ê°•í™”
                logging.info(f"í†µí•© ë¶„ì„ - GPT ì›ë³¸ ì‘ë‹µ: {result_text}")
                
                # ë¹ˆ ì‘ë‹µ ì²´í¬
                if not result_text or result_text.isspace():
                    logging.error("GPT ì‘ë‹µì´ ë¹„ì–´ìˆìŒ - ê¸°ë³¸ê°’ ë°˜í™˜")
                    return text, self._get_default_intent_analysis(text)
                
                # JSON íŒŒì‹± ì‹œë„
                try:
                    result = json.loads(result_text)
                    corrected_text = result.get('corrected_text', text)
                    intent_analysis = result.get('intent_analysis', {})
                    
                    # ê¸°ì¡´ í˜¸í™˜ì„±ì„ ìœ„í•œ í•„ë“œ ì¶”ê°€
                    intent_analysis.update({
                        'intent_type': intent_analysis.get('intent_category', 'ì¼ë°˜ë¬¸ì˜'),
                        'main_topic': intent_analysis.get('target_object', 'ê¸°íƒ€'),
                        'specific_request': intent_analysis.get('standardized_query', text[:100]),
                        'keywords': intent_analysis.get('semantic_keywords', [text[:20]]),
                        'urgency': 'medium',
                        'action_type': intent_analysis.get('primary_action', 'ê¸°íƒ€')
                    })
                    
                    # ìƒì„¸ ê²°ê³¼ ë¡œê·¸
                    logging.info(f"í†µí•© ë¶„ì„ ì™„ë£Œ - ìˆ˜ì •ëœ í…ìŠ¤íŠ¸: '{corrected_text}'")
                    logging.info(f"í†µí•© ë¶„ì„ ì™„ë£Œ - ì˜ë„ ë¶„ì„: {json.dumps(intent_analysis, ensure_ascii=False)}")
                    
                    # ë¡œê¹…
                    if corrected_text != text:
                        logging.info(f"í†µí•© ë¶„ì„ - ì˜¤íƒ€ ìˆ˜ì •: '{text[:50]}...' â†’ '{corrected_text[:50]}...'")
                    
                    logging.info(f"í†µí•© ë¶„ì„ - ì˜ë„: {intent_analysis.get('core_intent', 'N/A')}")
                    
                    # ğŸ” ë””ë²„ê·¸: íŒŒì‹±ëœ ê²°ê³¼ ì¶œë ¥
                    logging.info("ï¿½ï¿½ [ì‚¬ìš©ì ì§ˆë¬¸ ì˜ë„ ë¶„ì„ ê²°ê³¼]")
                    logging.info(f"ì…ë ¥ í…ìŠ¤íŠ¸: {text}")
                    logging.info(f"ìˆ˜ì •ëœ í…ìŠ¤íŠ¸: {corrected_text}")
                    logging.info(f"í•µì‹¬ ì˜ë„: {intent_analysis.get('core_intent', 'N/A')}")
                    logging.info(f"ì£¼ìš” í–‰ë™: {intent_analysis.get('primary_action', 'N/A')}")
                    logging.info(f"ëŒ€ìƒ ê°ì²´: {intent_analysis.get('target_object', 'N/A')}")
                    logging.info(f"ì˜ë¯¸ë¡ ì  í‚¤ì›Œë“œ: {intent_analysis.get('semantic_keywords', [])}")
                    logging.info(f"ì „ì²´ ì˜ë„ ë¶„ì„: {json.dumps(intent_analysis, ensure_ascii=False, indent=2)}")
                    logging.info("="*60)

                    return corrected_text, intent_analysis
                    
                except json.JSONDecodeError as e:
                    logging.error(f"í†µí•© ë¶„ì„ JSON íŒŒì‹± ì‹¤íŒ¨: {e}")
                    logging.error(f"íŒŒì‹± ì‹¤íŒ¨í•œ ì‘ë‹µ: {result_text}")
                    
                    # JSON íŒŒì‹± ì‹¤íŒ¨ì‹œ í…ìŠ¤íŠ¸ ê¸°ë°˜ íŒŒì‹± ì‹œë„
                    corrected_text, intent_analysis = self._parse_text_response(result_text, text)
                    
                    if not intent_analysis:
                        logging.warning("í…ìŠ¤íŠ¸ íŒŒì‹±ë„ ì‹¤íŒ¨, ê¸°ë³¸ê°’ ë°˜í™˜")
                        return text, self._get_default_intent_analysis(text)
                    
        except Exception as e:
            logging.error(f"í†µí•© í…ìŠ¤íŠ¸ ë¶„ì„ ì‹¤íŒ¨: {e}")
            return text, self._get_default_intent_analysis(text)

def _parse_text_response(self, response_text: str, original_text: str) -> Tuple[str, Dict]:
    """í…ìŠ¤íŠ¸ ì‘ë‹µì„ íŒŒì‹±í•˜ì—¬ ì˜ë„ ë¶„ì„ ê²°ê³¼ ì¶”ì¶œ"""
    try:
        # ê¸°ë³¸ê°’ ì„¤ì •
        corrected_text = original_text
        intent_analysis = {
            "core_intent": "general_inquiry",
            "intent_category": "ì¼ë°˜ë¬¸ì˜",
            "primary_action": "ê¸°íƒ€",
            "target_object": "ê¸°íƒ€",
            "constraint_conditions": [],
            "standardized_query": original_text,
            "semantic_keywords": [original_text[:20]],
            "intent_type": "ì¼ë°˜ë¬¸ì˜",
            "main_topic": "ê¸°íƒ€",
            "specific_request": original_text[:100],
            "keywords": [original_text[:20]],
            "urgency": "medium",
            "action_type": "ê¸°íƒ€"
        }
        
        # í…ìŠ¤íŠ¸ì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ ì‹œë„
        if "ì˜¤íƒ€" in response_text or "ìˆ˜ì •" in response_text:
            # ì˜¤íƒ€ ìˆ˜ì •ì´ ìˆëŠ” ê²½ìš°
            lines = response_text.split('\n')
            for line in lines:
                if "ìˆ˜ì •ëœ" in line or "corrected" in line.lower():
                    corrected_text = line.split(':')[-1].strip() if ':' in line else line.strip()
                    break
        
        # ì˜ë„ ë¶„ì„ í‚¤ì›Œë“œ ì¶”ì¶œ
        if "ì˜ë„" in response_text or "intent" in response_text.lower():
            lines = response_text.split('\n')
            for line in lines:
                if "í•µì‹¬" in line or "core" in line.lower():
                    intent_analysis["core_intent"] = line.split(':')[-1].strip() if ':' in line else "general_inquiry"
                elif "í–‰ë™" in line or "action" in line.lower():
                    intent_analysis["primary_action"] = line.split(':')[-1].strip() if ':' in line else "ê¸°íƒ€"
                elif "ëŒ€ìƒ" in line or "target" in line.lower():
                    intent_analysis["target_object"] = line.split(':')[-1].strip() if ':' in line else "ê¸°íƒ€"
        
        logging.info(f"í…ìŠ¤íŠ¸ íŒŒì‹± ê²°ê³¼ - ìˆ˜ì •ëœ í…ìŠ¤íŠ¸: '{corrected_text}'")
        logging.info(f"í…ìŠ¤íŠ¸ íŒŒì‹± ê²°ê³¼ - ì˜ë„ ë¶„ì„: {json.dumps(intent_analysis, ensure_ascii=False)}")
        
        return corrected_text, intent_analysis
        
    except Exception as e:
        logging.error(f"í…ìŠ¤íŠ¸ íŒŒì‹± ì‹¤íŒ¨: {e}")
        return original_text, self._get_default_intent_analysis(original_text)
    
    def _get_default_intent_analysis(self, text: str) -> Dict:
        """ë¶„ì„ ì‹¤íŒ¨ì‹œ ê¸°ë³¸ ì˜ë„ ë¶„ì„ ê²°ê³¼"""
        return {
            "core_intent": "general_inquiry",
            "intent_category": "ì¼ë°˜ë¬¸ì˜",
            "primary_action": "ê¸°íƒ€",
            "target_object": "ê¸°íƒ€",
            "constraint_conditions": [],
            "standardized_query": text,
            "semantic_keywords": [text[:20]],
            # ê¸°ì¡´ í˜¸í™˜ì„± í•„ë“œ
            "intent_type": "ì¼ë°˜ë¬¸ì˜",
            "main_topic": "ê¸°íƒ€",
            "specific_request": text[:100],
            "keywords": [text[:20]],
            "urgency": "medium",
            "action_type": "ê¸°íƒ€"
        }
