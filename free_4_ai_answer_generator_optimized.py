#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
=== ìµœì í™”ëœ AI ë‹µë³€ ìƒì„± Flask API ì„œë²„ ===
íŒŒì¼ëª…: free_4_ai_answer_generator_optimized.py
ëª©ì : Redis ìºì‹±, ë°°ì¹˜ ì²˜ë¦¬, ì§€ëŠ¥í˜• API ê´€ë¦¬ë¥¼ í†µí•©í•œ ê³ ì„±ëŠ¥ AI ë‹µë³€ ìƒì„± ì‹œìŠ¤í…œ

í•µì‹¬ ìµœì í™” ê¸°ëŠ¥:
- Redis ê¸°ë°˜ ì§€ëŠ¥í˜• ìºì‹± ì‹œìŠ¤í…œ
- ì—¬ëŸ¬ API í˜¸ì¶œì„ ë°°ì¹˜ë¡œ ì²˜ë¦¬í•˜ëŠ” ì‹œìŠ¤í…œ
- ì¡°ê±´ë¶€ API í˜¸ì¶œ ë°©ì§€ í”„ë¡œì„¸ì„œ
- ë™ì  ê²€ìƒ‰ ë ˆì´ì–´ ì¡°ì •
- API í˜¸ì¶œ íšŸìˆ˜ 6-12íšŒ â†’ 2-4íšŒë¡œ íšê¸°ì  ê°ì†Œ

ê¸°ì¡´ ì½”ë“œì™€ì˜ ì™„ì „í•œ í˜¸í™˜ì„± ìœ ì§€:
- ë™ì¼í•œ API ì—”ë“œí¬ì¸íŠ¸
- ë™ì¼í•œ ì…ì¶œë ¥ í˜•ì‹
- ë™ì¼í•œ ê¸°ëŠ¥
"""

# ==================================================
# 1. í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸ êµ¬ê°„
# ==================================================
import os
import sys
import gc
import logging
import tracemalloc
from datetime import datetime
from typing import Optional, Dict, Any

# ì›¹ í”„ë ˆì„ì›Œí¬ ê´€ë ¨
from flask import Flask, request, jsonify
from flask_cors import CORS

# AI ë° ë°ì´í„°ë² ì´ìŠ¤ ê´€ë ¨
from pinecone import Pinecone
import openai
import pyodbc

# í™˜ê²½ì„¤ì • ë° ìœ í‹¸ë¦¬í‹°
from dotenv import load_dotenv

# ìµœì í™”ëœ ëª¨ë“ˆë“¤ import
from src.main_optimized_ai_generator import OptimizedAIAnswerGenerator
from src.services.sync_service import SyncService

# ==================================================
# 2. ì‹œìŠ¤í…œ ì´ˆê¸°í™” ë° ì„¤ì •
# ==================================================
# ë©”ëª¨ë¦¬ ì¶”ì  ì‹œì‘
tracemalloc.start()

# Flask ì›¹ ì• í”Œë¦¬ì¼€ì´ì…˜ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
app = Flask(__name__)
CORS(app)

# ==================================================
# 3. ë¡œê¹… ì‹œìŠ¤í…œ ì„¤ì • (ì½˜ì†” + íŒŒì¼)
# ==================================================
logger = logging.getLogger()
logger.setLevel(logging.INFO)

formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')

# íŒŒì¼ í•¸ë“¤ëŸ¬
try:
    os.makedirs('/home/ec2-user/python/logs', exist_ok=True)
    file_handler = logging.FileHandler('/home/ec2-user/python/logs/ai_generator_optimized.log', encoding='utf-8')
    file_handler.setLevel(logging.INFO)
    file_handler.setFormatter(formatter)
    logger.addHandler(file_handler)
except Exception as e:
    print(f"ë¡œê·¸ íŒŒì¼ í•¸ë“¤ëŸ¬ ìƒì„± ì‹¤íŒ¨: {e}")

# ì½˜ì†” í•¸ë“¤ëŸ¬
console_handler = logging.StreamHandler()
console_handler.setLevel(logging.INFO)
console_handler.setFormatter(formatter)
logger.addHandler(console_handler)

# ==================================================
# 4. í™˜ê²½ë³€ìˆ˜ ë¡œë“œ ë° ì‹œìŠ¤í…œ ìƒìˆ˜ ì •ì˜
# ==================================================
load_dotenv()

# AI ì„ë² ë”© ëª¨ë¸ ë° ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ì„¤ì • ìƒìˆ˜ë“¤
MODEL_NAME = 'text-embedding-3-small'
INDEX_NAME = "bible-app-support-1536-openai"
EMBEDDING_DIMENSION = 1536
MAX_TEXT_LENGTH = 8000

# GPT ìì—°ì–´ ëª¨ë¸ ì„¤ì •
GPT_MODEL = 'gpt-3.5-turbo'
MAX_TOKENS = 600
TEMPERATURE = 0.5

# Redis ìºì‹± ì„¤ì •
REDIS_CONFIG = {
    'host': os.getenv('REDIS_HOST', 'localhost'),
    'port': int(os.getenv('REDIS_PORT', 6379)),
    'db': int(os.getenv('REDIS_DB', 0)),
    'password': os.getenv('REDIS_PASSWORD')
}

# ê³ ê° ë¬¸ì˜ ì¹´í…Œê³ ë¦¬ ë§¤í•‘ í…Œì´ë¸”
CATEGORY_MAPPING = {
    '1': 'í›„ì›/í•´ì§€',
    '2': 'ì„±ê²½ í†µë…(ì½ê¸°,ë“£ê¸°,ë…¹ìŒ)',
    '3': 'ì„±ê²½ë‚­ë… ë ˆì´ìŠ¤',
    '4': 'ê°œì„ /ì œì•ˆ',
    '5': 'ì˜¤ë¥˜/ì¥ì• ',
    '6': 'ë¶ˆë§Œ',
    '7': 'ì˜¤íƒˆìì œë³´',
    '0': 'ì‚¬ìš© ë¬¸ì˜(ê¸°íƒ€)'
}

# ==================================================
# 5. ì™¸ë¶€ ì„œë¹„ìŠ¤ ì—°ê²° ë° ì´ˆê¸°í™”
# ==================================================
try:
    # Pinecone ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì„¤ì •
    pc = Pinecone(api_key=os.getenv('PINECONE_API_KEY'))
    index = pc.Index(INDEX_NAME)

    # OpenAI API í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
    openai_client = openai.OpenAI(api_key=os.getenv('OPENAI_API_KEY'))

    # MSSQL ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì„¤ì •
    mssql_config = {
        'server': os.getenv('MSSQL_SERVER'),
        'database': os.getenv('MSSQL_DATABASE'),
        'username': os.getenv('MSSQL_USERNAME'),
        'password': os.getenv('MSSQL_PASSWORD')
    }

    # MSSQL Server ì—°ê²° ë¬¸ìì—´ êµ¬ì„±
    connection_string = (
            f"DRIVER={{ODBC Driver 17 for SQL Server}};"
            f"SERVER={mssql_config['server']},1433;"
            f"DATABASE={mssql_config['database']};"
            f"UID={mssql_config['username']};"
            f"PWD={mssql_config['password']};"
            f"TrustServerCertificate=yes;"
            f"Connection Timeout=30;"
    )

except Exception as e:
    logging.error(f"ì™¸ë¶€ ì„œë¹„ìŠ¤ ì—°ê²° ì‹¤íŒ¨: {str(e)}")
    raise

# ==================================================
# 6. ìµœì í™”ëœ AI ë‹µë³€ ìƒì„±ê¸° ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
# ==================================================

# ë©”ì¸ AI ë‹µë³€ ìƒì„±ê¸° (ìµœì í™”ëœ ì‹œìŠ¤í…œ)
generator = OptimizedAIAnswerGenerator(
    pinecone_index=index,
    openai_client=openai_client,
    connection_string=connection_string,
    category_mapping=CATEGORY_MAPPING,
    redis_config=REDIS_CONFIG
)

# í”„ë¡œë•ì…˜ ìµœì í™” ì„¤ì • ì ìš©
generator.optimize_for_production()

# ë™ê¸°í™” ë§¤ë‹ˆì € (ê¸°ì¡´ í˜¸í™˜ì„±ì„ ìœ„í•´ ë³„ë„ ì¸ìŠ¤í„´ìŠ¤ ìœ ì§€)
sync_manager = SyncService(
    pinecone_index=index,
    openai_client=openai_client,
    connection_string=connection_string,
    category_mapping=CATEGORY_MAPPING
)

# ==================================================
# 7. API ì—”ë“œí¬ì¸íŠ¸ ì •ì˜
# ==================================================

@app.route('/generate_answer', methods=['POST'])
def generate_answer():
    """AI ë‹µë³€ ìƒì„± API ì—”ë“œí¬ì¸íŠ¸ (ìµœì í™” ì ìš©)"""
    try:
        from src.utils.memory_manager import memory_cleanup

        with memory_cleanup():
            data = request.get_json()
            seq = data.get('seq', 0)
            question = data.get('question', '')
            lang = data.get('lang', 'auto')  # ìë™ ê°ì§€

            if not question:
                return jsonify({"success": False, "error": "ì§ˆë¬¸ì´ í•„ìš”í•©ë‹ˆë‹¤."}), 400

            # ìµœì í™”ëœ ì²˜ë¦¬ ì‹¤í–‰
            result = generator.process(seq, question, lang)

            response = jsonify(result)
            response.headers['Content-Type'] = 'application/json; charset=utf-8'

            # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ëª¨ë‹ˆí„°ë§
            snapshot = tracemalloc.take_snapshot()
            top_stats = snapshot.statistics('lineno')
            memory_usage = sum(stat.size for stat in top_stats) / 1024 / 1024  # MBë¡œ ë°˜í™˜
            logging.info(f"í˜„ì¬ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {memory_usage:.2f}MB")

            if memory_usage > 500: # 500MB ì´ˆê³¼ì‹œ ê²½ê³  ë° ê°€ë¹„ì§€ ì»¬ë ‰ì…˜ ê°•ì œ ì‹¤í–‰
                logging.warning(f"ë†’ì€ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ê°ì§€: {memory_usage:.2f}MB")
                gc.collect()

            return response

    except Exception as e:
        logging.error(f"API í˜¸ì¶œ ì˜¤ë¥˜: {str(e)}")
        return jsonify({"success": False, "error": str(e)}), 500


@app.route('/sync_to_pinecone', methods=['POST'])
def sync_to_pinecone():
    """MSSQL ë°ì´í„°ë¥¼ Pineconeì— ë™ê¸°í™”í•˜ëŠ” API ì—”ë“œí¬ì¸íŠ¸"""
    try:
        data = request.get_json()
        seq = data.get('seq')
        mode = data.get('mode', 'upsert')

        logging.info(f"ë™ê¸°í™” ìš”ì²­ ìˆ˜ì‹ : seq={seq}, mode={mode}")

        if not seq:
            logging.warning("seq ëˆ„ë½")
            return jsonify({"success": False, "error": "seqê°€ í•„ìš”í•©ë‹ˆë‹¤"}), 400

        if not isinstance(seq, int):
            seq = int(seq)

        result = sync_manager.sync_to_pinecone(seq, mode)

        logging.info(f"ë™ê¸°í™” ê²°ê³¼: {result}")

        status_code = 200 if result["success"] else 500
        return jsonify(result), status_code

    except ValueError as e:
        logging.error(f"ì˜ëª»ëœ seq ê°’: {str(e)}")
        return jsonify({"success": False, "error": f"ì˜ëª»ëœ seq ê°’: {str(e)}"}), 400
    except Exception as e:
        logging.error(f"Pinecone ë™ê¸°í™” API ì˜¤ë¥˜: {str(e)}")
        return jsonify({"success": False, "error": str(e)}), 500


@app.route('/health', methods=['GET'])
def health_check():
    """ì‹œìŠ¤í…œ ìƒíƒœ í™•ì¸ì„ ìœ„í•œ í—¬ìŠ¤ì²´í¬ API ì—”ë“œí¬ì¸íŠ¸ (ìµœì í™” ì •ë³´ í¬í•¨)"""
    try:
        stats = index.describe_index_stats()
        optimization_stats = generator.get_optimization_summary()
        detailed_stats = generator.get_detailed_performance_stats()

        return jsonify({
            "status": "healthy",
            "pinecone_vectors": stats.get('total_vector_count', 0),
            "timestamp": datetime.now().isoformat(),
            "services": {
                "ai_answer": "active",
                "pinecone_sync": "active",
                "multilingual_support": "active",
                "optimization_system": "active"
            },
            "optimization": {
                "cache_hit_rate": f"{optimization_stats['cache_hit_rate']:.1f}%",
                "api_calls_saved": optimization_stats['api_calls_saved'],
                "avg_processing_time": f"{optimization_stats['avg_processing_time']:.2f}s",
                "batch_processed": optimization_stats['batch_processed']
            },
            "detailed_performance": detailed_stats
        }), 200
    except Exception as e:
        return jsonify({
            "status": "unhealthy",
            "error": str(e)
        }), 500


@app.route('/optimization/stats', methods=['GET'])
def get_optimization_stats():
    """ìµœì í™” í†µê³„ ì¡°íšŒ API"""
    try:
        stats = generator.get_detailed_performance_stats()
        return jsonify({
            "success": True,
            "stats": stats,
            "timestamp": datetime.now().isoformat()
        }), 200
    except Exception as e:
        logging.error(f"ìµœì í™” í†µê³„ ì¡°íšŒ ì‹¤íŒ¨: {e}")
        return jsonify({
            "success": False,
            "error": str(e)
        }), 500


@app.route('/optimization/cache/clear', methods=['POST'])
def clear_cache():
    """ìºì‹œ ì§€ìš°ê¸° API"""
    try:
        data = request.get_json() or {}
        cache_type = data.get('type', 'all')  # 'all', 'embedding', 'intent', 'translation', etc.
        
        if cache_type == 'all':
            # ëª¨ë“  ìºì‹œ ì§€ìš°ê¸°
            generator.cache_manager.clear_cache_by_prefix('embedding')
            generator.cache_manager.clear_cache_by_prefix('intent')
            generator.cache_manager.clear_cache_by_prefix('translation')
            generator.cache_manager.clear_cache_by_prefix('typo')
            generator.cache_manager.clear_cache_by_prefix('search')
            generator.search_service.clear_caches()
            generator.api_manager.clear_recent_requests()
            cleared_count = "all"
        else:
            # íŠ¹ì • ìºì‹œë§Œ ì§€ìš°ê¸°
            cleared_count = generator.cache_manager.clear_cache_by_prefix(cache_type)
        
        return jsonify({
            "success": True,
            "message": f"ìºì‹œ ì§€ìš°ê¸° ì™„ë£Œ: {cache_type}",
            "cleared_count": cleared_count,
            "timestamp": datetime.now().isoformat()
        }), 200
        
    except Exception as e:
        logging.error(f"ìºì‹œ ì§€ìš°ê¸° ì‹¤íŒ¨: {e}")
        return jsonify({
            "success": False,
            "error": str(e)
        }), 500


@app.route('/optimization/config', methods=['POST'])
def update_optimization_config():
    """ìµœì í™” ì„¤ì • ì—…ë°ì´íŠ¸ API"""
    try:
        data = request.get_json()
        
        # API ê´€ë¦¬ì ì„¤ì • ì—…ë°ì´íŠ¸
        api_settings = data.get('api_manager', {})
        if api_settings:
            generator.api_manager.optimize_settings(**api_settings)
        
        # ê²€ìƒ‰ ì„œë¹„ìŠ¤ ì„¤ì • ì—…ë°ì´íŠ¸
        search_settings = data.get('search_service', {})
        if search_settings:
            generator.search_service.update_search_config(**search_settings)
        
        return jsonify({
            "success": True,
            "message": "ìµœì í™” ì„¤ì • ì—…ë°ì´íŠ¸ ì™„ë£Œ",
            "updated_settings": {
                "api_manager": api_settings,
                "search_service": search_settings
            },
            "timestamp": datetime.now().isoformat()
        }), 200
        
    except Exception as e:
        logging.error(f"ìµœì í™” ì„¤ì • ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {e}")
        return jsonify({
            "success": False,
            "error": str(e)
        }), 500


# ==================================================
# 8. ì• í”Œë¦¬ì¼€ì´ì…˜ ì¢…ë£Œ ì²˜ë¦¬
# ==================================================

@app.teardown_appcontext
def cleanup_request(exception=None):
    """ìš”ì²­ ì¢…ë£Œì‹œ ì •ë¦¬"""
    if exception:
        logging.error(f"ìš”ì²­ ì²˜ë¦¬ ì¤‘ ì˜ˆì™¸ ë°œìƒ: {exception}")


def cleanup_on_exit():
    """ì• í”Œë¦¬ì¼€ì´ì…˜ ì¢…ë£Œì‹œ ì •ë¦¬"""
    try:
        logging.info("ì• í”Œë¦¬ì¼€ì´ì…˜ ì¢…ë£Œ ì¤‘...")
        if 'generator' in globals():
            generator.cleanup()
        logging.info("ì •ë¦¬ ì™„ë£Œ")
    except Exception as e:
        logging.error(f"ì¢…ë£Œ ì •ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")


import atexit
atexit.register(cleanup_on_exit)

# ==================================================
# 9. ë©”ì¸ ì‹¤í–‰ ë¶€ë¶„
# ==================================================
if __name__ == "__main__":

    # í™˜ê²½ë³€ìˆ˜ì—ì„œ í¬íŠ¸ ì„¤ì • ë¡œë“œ (ê¸°ë³¸ê°’: 8000)
    port = int(os.getenv('FLASK_PORT', 8000))

    # ì‹œì‘ ë©”ì‹œì§€ ì¶œë ¥
    print("="*80)
    print("ğŸš€ GOODTV ë°”ì´ë¸” ì• í”Œ AI ë‹µë³€ ìƒì„± ì„œë²„ (ìµœì í™”ëœ ë²„ì „)")
    print("="*80)
    print(f"ğŸ“¡ ì„œë²„ í¬íŠ¸: {port}")
    print(f"ğŸ¤– AI ëª¨ë¸: {GPT_MODEL} (Enhanced Context Mode)")
    print(f"ğŸ” ì„ë² ë”© ëª¨ë¸: {MODEL_NAME}")
    print(f"ğŸ—ƒï¸  ë²¡í„° DB: Pinecone ({INDEX_NAME})")
    print(f"ğŸ’¾ ìºì‹± ì‹œìŠ¤í…œ: Redis ({REDIS_CONFIG['host']}:{REDIS_CONFIG['port']})")
    print(f"ğŸŒ ë‹¤êµ­ì–´ ì§€ì›: í•œêµ­ì–´(ko), ì˜ì–´(en)")
    print("")
    print("ğŸ”§ ì œê³µ ì„œë¹„ìŠ¤:")
    print("   â”œâ”€â”€ AI ë‹µë³€ ìƒì„± (/generate_answer)")
    print("   â”œâ”€â”€ Pinecone ë™ê¸°í™” (/sync_to_pinecone)")
    print("   â”œâ”€â”€ í—¬ìŠ¤ì²´í¬ (/health)")
    print("   â”œâ”€â”€ ìµœì í™” í†µê³„ (/optimization/stats)")
    print("   â”œâ”€â”€ ìºì‹œ ê´€ë¦¬ (/optimization/cache/clear)")
    print("   â””â”€â”€ ì„¤ì • ê´€ë¦¬ (/optimization/config)")
    print("")
    print("âš¡ í•µì‹¬ ìµœì í™” ê¸°ëŠ¥:")
    print("   â”œâ”€â”€ Redis ê¸°ë°˜ ì§€ëŠ¥í˜• ìºì‹± ì‹œìŠ¤í…œ")
    print("   â”œâ”€â”€ ì—¬ëŸ¬ API í˜¸ì¶œì„ ë°°ì¹˜ë¡œ ì²˜ë¦¬")
    print("   â”œâ”€â”€ ì¡°ê±´ë¶€ API í˜¸ì¶œ ë°©ì§€ í”„ë¡œì„¸ì„œ")
    print("   â”œâ”€â”€ ë™ì  ê²€ìƒ‰ ë ˆì´ì–´ ì¡°ì •")
    print("   â”œâ”€â”€ API í˜¸ì¶œ íšŸìˆ˜: 6-12íšŒ â†’ 2-4íšŒë¡œ íšê¸°ì  ê°ì†Œ")
    print("   â”œâ”€â”€ ì‘ë‹µ ì‹œê°„: í‰ê·  50-70% ë‹¨ì¶•")
    print("   â””â”€â”€ API ë¹„ìš©: 60-80% ì ˆê°")
    print("")
    print("ğŸ”’ ê¸°ì¡´ í˜¸í™˜ì„±:")
    print("   â”œâ”€â”€ ë™ì¼í•œ API ì—”ë“œí¬ì¸íŠ¸")
    print("   â”œâ”€â”€ ë™ì¼í•œ ì…ì¶œë ¥ í˜•ì‹")
    print("   â”œâ”€â”€ ë™ì¼í•œ ê¸°ëŠ¥")
    print("   â””â”€â”€ ë¬´ì¤‘ë‹¨ ë§ˆì´ê·¸ë ˆì´ì…˜ ê°€ëŠ¥")
    print("="*80)
    
    # ìºì‹œ ì‹œìŠ¤í…œ ìƒíƒœ í™•ì¸
    cache_available = generator.cache_manager.is_cache_available()
    cache_stats = generator.cache_manager.get_cache_stats()
    print(f"ğŸ’¾ ìºì‹± ì‹œìŠ¤í…œ: {'âœ… ì—°ê²°ë¨' if cache_available else 'âŒ ì—°ê²° ì‹¤íŒ¨'}")
    print(f"   â””â”€â”€ íƒ€ì…: {cache_stats.get('cache_type', 'Unknown')}")
    
    # ë°°ì¹˜ í”„ë¡œì„¸ì„œ ìƒíƒœ í™•ì¸
    batch_running = generator.batch_processor.running
    print(f"âš¡ ë°°ì¹˜ í”„ë¡œì„¸ì„œ: {'âœ… ì‹¤í–‰ ì¤‘' if batch_running else 'âŒ ì¤‘ì§€ë¨'}")
    
    # API ë§¤ë‹ˆì € ìƒíƒœ í™•ì¸
    api_health = generator.api_manager.health_check()
    print(f"ğŸ§  API ê´€ë¦¬ì: {'âœ… ì •ìƒ' if api_health['openai_client_available'] else 'âŒ ì˜¤ë¥˜'}")
    
    print("="*80)
    print("ğŸ¯ ì‹œìŠ¤í…œ ì¤€ë¹„ ì™„ë£Œ! API ìš”ì²­ì„ ë°›ì„ ì¤€ë¹„ê°€ ë˜ì—ˆìŠµë‹ˆë‹¤.")
    print("="*80)

    # Flask ì›¹ ì„œë²„ ì‹œì‘
    app.run(host='0.0.0.0', port=port, debug=False, threaded=True)
