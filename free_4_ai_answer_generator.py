#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
=== AI ë‹µë³€ ìƒì„± Flask API ì„œë²„ (ë‹¤êµ­ì–´ ì§€ì›) ===
íŒŒì¼ëª…: free_4_ai_answer_generator.py
ëª©ì : ASP Classicì—ì„œ í˜¸ì¶œí•˜ëŠ” AI ë‹µë³€ ìƒì„± API + Pinecone ë²¡í„°DB ë™ê¸°í™”
ì£¼ìš” ê¸°ëŠ¥:
1. OpenAI GPT-3.5-turboë¥¼ ì´ìš©í•œ ìì—°ì–´ ë‹µë³€ ìƒì„± (í•œêµ­ì–´/ì˜ì–´)
2. Pinecone ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ìœ ì‚¬ ë‹µë³€ ê²€ìƒ‰
3. MSSQL ë°ì´í„°ë² ì´ìŠ¤ì™€ Pinecone ë™ê¸°í™”
4. ë©”ëª¨ë¦¬ ìµœì í™” ë° ëª¨ë‹ˆí„°ë§
5. ë‹¤êµ­ì–´ ì§€ì› (í•œêµ­ì–´, ì˜ì–´)
"""

# ==================================================
# 1. í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸ êµ¬ê°„
# ==================================================
# ê¸°ë³¸ Python ëª¨ë“ˆë“¤
import os                   # í™˜ê²½ë³€ìˆ˜ ë° íŒŒì¼ ì‹œìŠ¤í…œ ì‘ì—…
import sys                  # ì‹œìŠ¤í…œ ê´€ë ¨ ê¸°ëŠ¥
import json                 # JSON ë°ì´í„° ì²˜ë¦¬
import json as json_module  # JSON ëª¨ë“ˆì˜ ë³„ì¹­ (ì¼ë¦¬ì•„ìŠ¤, ì½”ë“œ ë‚´ ì¤‘ë³µ ë°©ì§€)
import re                   # ì •ê·œí‘œí˜„ì‹ íŒ¨í„´ ë§¤ì¹­
import html                 # HTML ì—”í‹°í‹° ì²˜ë¦¬
import unicodedata          # ìœ ë‹ˆì½”ë“œ ë¬¸ì ì •ê·œí™”
import logging              # ë¡œê·¸ ê¸°ë¡ ì‹œìŠ¤í…œ
import gc                   # ê°€ë¹„ì§€ ì»¬ë ‰ì…˜ (ë©”ëª¨ë¦¬ ê´€ë¦¬)

# ì›¹ í”„ë ˆì„ì›Œí¬ ê´€ë ¨
from flask import Flask, request, jsonify  # Flask ì›¹ í”„ë ˆì„ì›Œí¬
from flask_cors import CORS                 # CORS(Cross-Origin Resource Sharing) ì²˜ë¦¬

# AI ë° ë°ì´í„°ë² ì´ìŠ¤ ê´€ë ¨
from pinecone import Pinecone      # Pinecone ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤
import openai                      # OpenAI API í´ë¼ì´ì–¸íŠ¸
import pyodbc                      # MSSQL ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²°

# í™˜ê²½ì„¤ì • ë° ìœ í‹¸ë¦¬í‹°
from dotenv import load_dotenv     # .env íŒŒì¼ì—ì„œ í™˜ê²½ë³€ìˆ˜ ë¡œë“œ
from datetime import datetime      # ë‚ ì§œ/ì‹œê°„ ì²˜ë¦¬
from typing import Optional, Dict, Any, List  # íƒ€ì… íŒíŒ…
import re                                     # ì •ê·œí‘œí˜„ì‹

# ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ê´€ë ¨
from memory_profiler import profile                  # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ í”„ë¡œíŒŒì¼ë§
import tracemalloc                                   # ë©”ëª¨ë¦¬ ì¶”ì 
import threading                                     # ë©€í‹°ìŠ¤ë ˆë”©
from contextlib import contextmanager                # ì»¨í…ìŠ¤íŠ¸ ë§¤ë‹ˆì € (withë¬¸ ì‚¬ìš©)
from langdetect import detect, LangDetectException   # ì–¸ì–´ ê°ì§€

# ==================================================
# 2. ì‹œìŠ¤í…œ ì´ˆê¸°í™” ë° ì„¤ì •
# ==================================================
# ë©”ëª¨ë¦¬ ì¶”ì  ì‹œì‘ - ë©”ëª¨ë¦¬ ëˆ„ìˆ˜ ë° ì‚¬ìš©ëŸ‰ì„ ëª¨ë‹ˆí„°ë§í•˜ê¸° ìœ„í•¨ (ì´ ì‹œì ë¶€í„° ëª¨ë“  ë©”ëª¨ë¦¬ í• ë‹¹ì´ ê¸°ë¡ë˜ì–´ ë‚˜ì¤‘ì— ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ë¶„ì„ì´ ê°€ëŠ¥í•´ì§)
tracemalloc.start() 

# Flask ì›¹ ì• í”Œë¦¬ì¼€ì´ì…˜ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
# __name__: í˜„ì¬ ëª¨ë“ˆëª…ì„ ì „ë‹¬í•˜ì—¬ Flaskê°€ ë¦¬ì†ŒìŠ¤ ìœ„ì¹˜ë¥¼ ì°¾ì„ ìˆ˜ ìˆê²Œ í•¨
app = Flask(__name__)

# CORS ì„¤ì • - ëª¨ë“  ì—”ë“œí¬ì¸íŠ¸ì—ì„œ cross-origin ìš”ì²­ì„ í—ˆìš© (ASP Classicì—ì„œ í˜¸ì¶œí•˜ê¸° ìœ„í•¨)
CORS(app)

# ==================================================
# 3. ë¡œê¹… ì‹œìŠ¤í…œ ì„¤ì • (ì½˜ì†” + íŒŒì¼)
# ==================================================
# ë¡œê±° ìƒì„±
logger = logging.getLogger()
logger.setLevel(logging.INFO)

# í¬ë§·í„° ìƒì„±
formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')

# íŒŒì¼ í•¸ë“¤ëŸ¬ (ê¸°ì¡´)
try:
    # ë¡œê·¸ ë””ë ‰í† ë¦¬ ìƒì„±
    os.makedirs('/home/ec2-user/python/logs', exist_ok=True)
    file_handler = logging.FileHandler('/home/ec2-user/python/logs/ai_generator.log', encoding='utf-8')
    file_handler.setLevel(logging.INFO)
    file_handler.setFormatter(formatter)
    logger.addHandler(file_handler)
except Exception as e:
    print(f"ë¡œê·¸ íŒŒì¼ í•¸ë“¤ëŸ¬ ìƒì„± ì‹¤íŒ¨: {e}")

# ì½˜ì†” í•¸ë“¤ëŸ¬ ì¶”ê°€ (ì‹¤ì‹œê°„ ë””ë²„ê¹…ìš©)
console_handler = logging.StreamHandler()
console_handler.setLevel(logging.INFO)
console_handler.setFormatter(formatter)
logger.addHandler(console_handler)

# ==================================================
# 4. í™˜ê²½ë³€ìˆ˜ ë¡œë“œ ë° ì‹œìŠ¤í…œ ìƒìˆ˜ ì •ì˜
# ==================================================
# .env íŒŒì¼ì—ì„œ API í‚¤ ë° ë°ì´í„°ë² ì´ìŠ¤ ì„¤ì • ë¡œë“œ
load_dotenv()

# AI ì„ë² ë”© ëª¨ë¸ ë° ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ì„¤ì • ìƒìˆ˜ë“¤
MODEL_NAME = 'text-embedding-3-small'          # OpenAI ì„ë² ë”© ëª¨ë¸ëª…
INDEX_NAME = "bible-app-support-1536-openai"   # Pinecone ì¸ë±ìŠ¤ëª…
EMBEDDING_DIMENSION = 1536                      # ì„ë² ë”© ë²¡í„° ì°¨ì›ìˆ˜
MAX_TEXT_LENGTH = 8000                          # í…ìŠ¤íŠ¸ ìµœëŒ€ ê¸¸ì´ ì œí•œ

# GPT ìì—°ì–´ ëª¨ë¸ ì„¤ì • - ë³´ìˆ˜ì  ì„¤ì •ìœ¼ë¡œ ì¼ê´€ì„± ìˆëŠ” ë‹µë³€ ìƒì„±
GPT_MODEL = 'gpt-3.5-turbo'     # ì‚¬ìš©í•  GPT ëª¨ë¸
MAX_TOKENS = 600                 # ìƒì„±í•  ìµœëŒ€ í† í° ìˆ˜ (ë‹µë³€ ê¸¸ì´ ì œí•œ)
TEMPERATURE = 0.5                # ì°½ì˜ì„± ìˆ˜ì¤€ (ë‚®ì„ìˆ˜ë¡ ì¼ê´€ëœ ë‹µë³€)

# ê³ ê° ë¬¸ì˜ ì¹´í…Œê³ ë¦¬ ë§¤í•‘ í…Œì´ë¸”
# ì¹´í…Œê³ ë¦¬ ë§¤í•‘ ë”•ì…”ë„ˆë¦¬ëŠ” MSSQLì˜ ìˆ«ì ì¸ë±ìŠ¤ë¥¼ ì‚¬ëŒì´ ì½ì„ ìˆ˜ ìˆëŠ” í•œê¸€ ì¹´í…Œê³ ë¦¬ëª…ìœ¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤. 
# ì´ëŠ” Pinecone ë©”íƒ€ë°ì´í„°ì— ì €ì¥ë˜ì–´ ê²€ìƒ‰ ê²°ê³¼ì˜ ê°€ë…ì„±ì„ ë†’ì…ë‹ˆë‹¤.
CATEGORY_MAPPING = {
    '1': 'í›„ì›/í•´ì§€',                   
    '2': 'ì„±ê²½ í†µë…(ì½ê¸°,ë“£ê¸°,ë…¹ìŒ)',   
    '3': 'ì„±ê²½ë‚­ë… ë ˆì´ìŠ¤',             
    '4': 'ê°œì„ /ì œì•ˆ',                   
    '5': 'ì˜¤ë¥˜/ì¥ì• ',               
    '6': 'ë¶ˆë§Œ',                        
    '7': 'ì˜¤íƒˆìì œë³´',                   
    '0': 'ì‚¬ìš© ë¬¸ì˜(ê¸°íƒ€)'               
}

# ì˜ì–´ ì¹´í…Œê³ ë¦¬ ë§¤í•‘ ì¶”ê°€
CATEGORY_MAPPING_EN = {
    '1': 'Sponsorship/Cancellation',
    '2': 'Bible Reading(Read,Listen,Record)',
    '3': 'Bible Reading Race',
    '4': 'Improvement/Suggestion',
    '5': 'Error/Failure',
    '6': 'Complaint',
    '7': 'Typo Report',
    '0': 'Usage Inquiry(Other)'
}

# ==================================================
# 5. ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ ì •ì˜
# ==================================================
# ë©”ëª¨ë¦¬ ì •ë¦¬ë¥¼ ìœ„í•œ ì»¨í…ìŠ¤íŠ¸ ë§¤ë‹ˆì €
# withë¬¸ ì§„ì…ì‹œ try ë¸”ë¡ ì‹¤í–‰
# yieldì—ì„œ ì¼ì‹œì •ì§€í•˜ê³  with ë¸”ë¡ ë‚´ë¶€ ì½”ë“œ ì‹¤í–‰
# yield í‚¤ì›Œë“œê°€ ë“¤ì–´ê°„ ì´ í•¨ìˆ˜ëŠ” ì œë„ˆë ˆì´í„°ì´ë©´ì„œ ë™ì‹œì— ì»¨í…ìŠ¤íŠ¸ ë§¤ë‹ˆì €ë¡œ ë™ì‘
# ì œë„ˆë ˆì´í„° í•¨ìˆ˜ëŠ” ê°’ì„ ë°˜í™˜í•˜ëŠ” ëŒ€ì‹  ê°’ì„ í•˜ë‚˜ì”© ìƒì„±í•˜ì—¬ ë°˜í™˜í•©ë‹ˆë‹¤. (íŒŒì´ì¬ì—ì„œ ëŒ€ìš©ëŸ‰ ë°ì´í„° ì²˜ë¦¬ë¥¼ ìœ„í•´ ì‚¬ìš©)
# with ë¸”ë¡ ì¢…ë£Œì‹œ finallyì—ì„œ gc.collect() ì‹¤í–‰

# ì´ë ‡ê²Œ í•˜ë©´ ëŒ€ìš©ëŸ‰ ë°ì´í„° ì²˜ë¦¬ í›„ ìë™ìœ¼ë¡œ ê°€ë¹„ì§€ ì»¬ë ‰ì…˜ì´ ì‹¤í–‰ë©ë‹ˆë‹¤.
@contextmanager
def memory_cleanup():
    try:
        yield  # with ë¸”ë¡ ë‚´ë¶€ ì½”ë“œ ì‹¤í–‰
    finally:
        gc.collect()  # ê°€ë¹„ì§€ ì»¬ë ‰ì…˜ ê°•ì œ ì‹¤í–‰ìœ¼ë¡œ ë©”ëª¨ë¦¬ ì •ë¦¬

# ==================================================
# 6. ì™¸ë¶€ ì„œë¹„ìŠ¤ ì—°ê²° ë° ì´ˆê¸°í™”
# ==================================================
try:
    # Pinecone ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì„¤ì •
    # ìœ ì‚¬ ë‹µë³€ ê²€ìƒ‰ì„ ìœ„í•œ ë²¡í„°DB - ì„ë² ë”©ëœ ì§ˆë¬¸/ë‹µë³€ ì €ì¥ì†Œ
    pc = Pinecone(api_key=os.getenv('PINECONE_API_KEY'))
    index = pc.Index(INDEX_NAME)
    
    # OpenAI API í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
    # GPT ëª¨ë¸ ë° ì„ë² ë”© ìƒì„±ì„ ìœ„í•œ í´ë¼ì´ì–¸íŠ¸
    # openai.api_key = ... ë°©ì‹ë³´ë‹¤ ê°ì²´ì§€í–¥ì ìœ¼ë¡œ ì„¤ê³„
    openai_client = openai.OpenAI(api_key=os.getenv('OPENAI_API_KEY'))
    
    # MSSQL ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì„¤ì •
    # ê¸°ì¡´ ê³ ê° ë¬¸ì˜ ë°ì´í„°ê°€ ì €ì¥ëœ ìš´ì˜ DB ì—°ê²°ì„ ìœ„í•œ ì„¤ì •
    mssql_config = {
        'server': os.getenv('MSSQL_SERVER'),       # DB ì„œë²„ ì£¼ì†Œ
        'database': os.getenv('MSSQL_DATABASE'),   # ë°ì´í„°ë² ì´ìŠ¤ëª…
        'username': os.getenv('MSSQL_USERNAME'),   # DB ì‚¬ìš©ìëª…
        'password': os.getenv('MSSQL_PASSWORD')    # DB ë¹„ë°€ë²ˆí˜¸
    }
    
    # MSSQL Server ì—°ê²° ë¬¸ìì—´ êµ¬ì„±
    # MSSQL Server í‘œì¤€ ODBC ë“œë¼ì´ë²„ë¥¼ ì‚¬ìš©í•œ ì—°ê²° ë¬¸ìì—´
    connection_string = (
            f"DRIVER={{ODBC Driver 17 for SQL Server}};"  # ODBC ë“œë¼ì´ë²„ ë²„ì „
            f"SERVER={mssql_config['server']},1433;"      # ì„œë²„ ì£¼ì†Œì™€ í¬íŠ¸
            f"DATABASE={mssql_config['database']};"       # ë°ì´í„°ë² ì´ìŠ¤ëª…
            f"UID={mssql_config['username']};"            # ì‚¬ìš©ì ID
            f"PWD={mssql_config['password']};"            # ë¹„ë°€ë²ˆí˜¸
            f"TrustServerCertificate=yes;"                # SSL ì¸ì¦ì„œ ì‹ ë¢°
            f"Connection Timeout=30;"                     # ì—°ê²° íƒ€ì„ì•„ì›ƒ (30ì´ˆ)
    )

except Exception as e:
    # ì´ˆê¸°í™” ì‹¤íŒ¨ ì‹œ ë¡œê·¸ ê¸°ë¡ í›„ í”„ë¡œê·¸ë¨ ì¢…ë£Œ
    logging.error(f"ëª¨ë“ˆ ë¡œë“œ ì‹¤íŒ¨: {str(e)}")
    app.logger.error(f"ëª¨ë“ˆ ë¡œë“œ ì‹¤íŒ¨: {str(e)}")
    raise  # ì˜ˆì™¸ë¥¼ ë‹¤ì‹œ ë°œìƒì‹œì¼œ í”„ë¡œê·¸ë¨ ì¤‘ë‹¨

# ==================================================
# 7. AI ë‹µë³€ ìƒì„± ë©”ì¸ í´ë˜ìŠ¤ (ê°ì²´ ì§€í–¥ í”„ë¡œê·¸ë˜ë°, ë‹¤êµ­ì–´ ì§€ì› ì¶”ê°€)
# ==================================================
    # AI ë‹µë³€ ìƒì„±ì„ ë‹´ë‹¹í•˜ëŠ” ë©”ì¸ í´ë˜ìŠ¤
    # ê°ì²´ì§€í–¥ ì„¤ê³„ë¡œ ê´€ë ¨ ê¸°ëŠ¥ë“¤ì„ í•˜ë‚˜ì˜ í´ë˜ìŠ¤ì— ìº¡ìŠí™”
    
    # ì£¼ìš” ê¸°ëŠ¥:
    # 1. í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ ë° ì •ì œ
    # 2. OpenAIë¥¼ ì´ìš©í•œ ì„ë² ë”© ìƒì„±
    # 3. Pineconeì—ì„œ ìœ ì‚¬ ë‹µë³€ ê²€ìƒ‰
    # 4. GPTë¥¼ ì´ìš©í•œ ë§ì¶¤í˜• ë‹µë³€ ìƒì„±
    # 5. í•œêµ­ì–´ í…ìŠ¤íŠ¸ ê²€ì¦ ë° í¬ë§·íŒ…

class AIAnswerGenerator:
    
    # í´ë˜ìŠ¤ ì´ˆê¸°í™” ë©”ì„œë“œ
    # OpenAI í´ë¼ì´ì–¸íŠ¸ë¥¼ ì¸ìŠ¤í„´ìŠ¤ ë³€ìˆ˜ë¡œ ì„¤ì •. ì´ëŠ” ì˜ì¡´ì„± ì£¼ì… íŒ¨í„´ì˜ ê°„ì†Œí™”ëœ í˜•íƒœ
    def __init__(self):
        self.openai_client = openai_client

    # â˜† í•œêµ­ì–´ ì˜¤íƒ€ ìˆ˜ì • ë©”ì„œë“œ
    def fix_korean_typos_with_ai(self, text: str) -> str:
        if not text or len(text.strip()) < 3:
            return text
        
        # ë„ˆë¬´ ê¸´ í…ìŠ¤íŠ¸ëŠ” ì²˜ë¦¬í•˜ì§€ ì•ŠìŒ (ë¹„ìš© ì ˆì•½)
        if len(text) > 500:
            logging.warning(f"í…ìŠ¤íŠ¸ê°€ ë„ˆë¬´ ê¸¸ì–´ ì˜¤íƒ€ ìˆ˜ì • ê±´ë„ˆëœ€: {len(text)}ì")
            return text
        
        try:
            with memory_cleanup():
                system_prompt = """ë‹¹ì‹ ì€ í•œêµ­ì–´ ë§ì¶¤ë²• ë° ì˜¤íƒ€ êµì • ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

ì§€ì¹¨:
1. ì…ë ¥ëœ í•œêµ­ì–´ í…ìŠ¤íŠ¸ì˜ ë§ì¶¤ë²•ê³¼ ì˜¤íƒ€ë§Œ ìˆ˜ì •í•˜ì„¸ìš”
2. ì›ë¬¸ì˜ ì˜ë¯¸ì™€ ì–´ì¡°ëŠ” ì ˆëŒ€ ë³€ê²½í•˜ì§€ ë§ˆì„¸ìš”
3. ë„ì–´ì“°ê¸°, ë§ì¶¤ë²•, ì¡°ì‚¬ ì‚¬ìš©ë²•ì„ ì •í™•íˆ êµì •í•˜ì„¸ìš”
4. ì•±/ì–´í”Œë¦¬ì¼€ì´ì…˜ ê´€ë ¨ ê¸°ìˆ  ìš©ì–´ëŠ” í‘œì¤€ ìš©ì–´ë¡œ í†µì¼í•˜ì„¸ìš”
5. ìˆ˜ì •ì´ í•„ìš”ì—†ë‹¤ë©´ ì›ë¬¸ ê·¸ëŒ€ë¡œ ë°˜í™˜í•˜ì„¸ìš”
6. ìˆ˜ì •ëœ í…ìŠ¤íŠ¸ë§Œ ë°˜í™˜í•˜ê³  ì¶”ê°€ ì„¤ëª…ì€ í•˜ì§€ ë§ˆì„¸ìš”

ì˜ˆì‹œ:
- "ì–´í”Œì´ ì•ˆë€ë‹¤" â†’ "ì•±ì´ ì•ˆ ë¼ìš”"
- "ë‹¤ìš´ë°›ê¸°ê°€ ì•ˆë˜ìš”" â†’ "ë‹¤ìš´ë¡œë“œê°€ ì•ˆ ë¼ìš”"
- "ì‚­ì¬í•˜ê³ ì‹¶ì–´ìš”" â†’ "ì‚­ì œí•˜ê³  ì‹¶ì–´ìš”"
- "ì—…ë°ì´ë“œí•´ì£¼ì„¸ìš”" â†’ "ì—…ë°ì´íŠ¸í•´ì£¼ì„¸ìš”"
"""

                user_prompt = f"ë‹¤ìŒ í…ìŠ¤íŠ¸ì˜ ë§ì¶¤ë²•ê³¼ ì˜¤íƒ€ë¥¼ ìˆ˜ì •í•´ì£¼ì„¸ìš”:\n\n{text}"

                response = self.openai_client.chat.completions.create(
                    model='gpt-3.5-turbo',
                    messages=[
                        {"role": "system", "content": system_prompt},
                        {"role": "user", "content": user_prompt}
                    ],
                    max_tokens=600,
                    temperature=0.1,  # ë§¤ìš° ë³´ìˆ˜ì ìœ¼ë¡œ ì„¤ì •
                    top_p=0.8,
                    frequency_penalty=0.0,
                    presence_penalty=0.0
                )
                
                corrected_text = response.choices[0].message.content.strip()
                del response # ë©”ëª¨ë¦¬ í•´ì œ
                
                # ê²°ê³¼ ê²€ì¦
                if not corrected_text or len(corrected_text) == 0:
                    logging.warning("AI ì˜¤íƒ€ ìˆ˜ì • ê²°ê³¼ê°€ ë¹„ì–´ìˆìŒ, ì›ë¬¸ ë°˜í™˜")
                    return text
                
                # ë„ˆë¬´ ë§ì´ ë³€ê²½ëœ ê²½ìš° ì˜ì‹¬ìŠ¤ëŸ¬ìš°ë¯€ë¡œ ì›ë¬¸ ë°˜í™˜
                if len(corrected_text) > len(text) * 2:
                    logging.warning("AI ì˜¤íƒ€ ìˆ˜ì • ê²°ê³¼ê°€ ì›ë¬¸ë³´ë‹¤ ë„ˆë¬´ ê¸¸ì–´ì§, ì›ë¬¸ ë°˜í™˜")
                    return text
                
                # ìˆ˜ì • ë‚´ìš©ì´ ìˆìœ¼ë©´ ë¡œê·¸ ê¸°ë¡
                if corrected_text != text:
                    logging.info(f"AI ì˜¤íƒ€ ìˆ˜ì •: '{text[:50]}...' â†’ '{corrected_text[:50]}...'")
                
                return corrected_text
                
        except Exception as e:
            logging.error(f"AI ì˜¤íƒ€ ìˆ˜ì • ì‹¤íŒ¨: {e}")
            # AI ì‹¤íŒ¨ ì‹œ ì›ë¬¸ ê·¸ëŒ€ë¡œ ë°˜í™˜
            return text
    
    # â˜† í…ìŠ¤íŠ¸ì˜ ì–¸ì–´ë¥¼ ê°ì§€í•˜ëŠ” ë©”ì„œë“œ
    def detect_language(self, text: str) -> str:
        try:
            # langdetect ë¼ì´ë¸ŒëŸ¬ë¦¬ ì‚¬ìš©
            detected = detect(text)
            
            # ì˜ì–´ì™€ í•œêµ­ì–´ë§Œ ì§€ì›
            if detected == 'en':
                return 'en'
            elif detected == 'ko':
                return 'ko'
            else:
                # ê¸°ë³¸ê°’ì€ í•œêµ­ì–´
                return 'ko'
        except LangDetectException:
            # ê°ì§€ ì‹¤íŒ¨ì‹œ í…ìŠ¤íŠ¸ ë‚´ í•œê¸€ ë¹„ìœ¨ë¡œ íŒë‹¨
            korean_chars = len(re.findall(r'[ê°€-í£]', text))
            english_chars = len(re.findall(r'[a-zA-Z]', text))
            
            if korean_chars > english_chars:
                return 'ko'
            else:
                return 'en'

    # â˜† ì…ë ¥ í…ìŠ¤íŠ¸ë¥¼ AI ì²˜ë¦¬ì— ì í•©í•˜ê²Œ ì „ì²˜ë¦¬í•˜ëŠ” ë©”ì„œë“œ (ì›ë³¸ í…ìŠ¤íŠ¸ -> ì •ì œëœ í…ìŠ¤íŠ¸)
    def preprocess_text(self, text: str) -> str:
        logging.info(f"ì „ì²˜ë¦¬ ì‹œì‘: ì…ë ¥ ê¸¸ì´={len(text) if text else 0}")
        logging.info(f"ì „ì²˜ë¦¬ ì…ë ¥ ë¯¸ë¦¬ë³´ê¸°: {text[:100] if text else 'None'}...")

        # null ì²´í¬
        if not text:
            logging.info("ì „ì²˜ë¦¬: ë¹ˆ í…ìŠ¤íŠ¸ ì…ë ¥")
            return ""
        
        # ë¬¸ìì—´ë¡œ ë³€í™˜ ë° HTML ì—”í‹°í‹° ë””ì½”ë”©
        text = str(text)
        text = html.unescape(text)  # &amp; â†’ &, &lt; â†’ < ë“±
        logging.info(f"HTML ë””ì½”ë”© í›„ ê¸¸ì´: {len(text)}")
        
        # HTML íƒœê·¸ ì œê±° ë°ë° í…ìŠ¤íŠ¸ í˜•íƒœë¡œ ë³€í™˜ (êµ¬ì¡° ìœ ì§€)
        text = re.sub(r'<br\s*/?>', '\n', text, flags=re.IGNORECASE)      # <br> â†’ ì¤„ë°”ê¿ˆ
        text = re.sub(r'</p>', '\n\n', text, flags=re.IGNORECASE)         # </p> â†’ ë‹¨ë½ êµ¬ë¶„
        text = re.sub(r'<p[^>]*>', '\n', text, flags=re.IGNORECASE)       # <p> â†’ ì¤„ë°”ê¿ˆ
        text = re.sub(r'<li[^>]*>', '\nâ€¢ ', text, flags=re.IGNORECASE)    # <li> â†’ ë¶ˆë¦¿í¬ì¸íŠ¸
        text = re.sub(r'</li>', '', text, flags=re.IGNORECASE)            # </li> ì œê±°
        text = re.sub(r'<[^>]+>', '', text)                               # ë‚˜ë¨¸ì§€ HTML íƒœê·¸ ëª¨ë‘ ì œê±°
        logging.info(f"HTML íƒœê·¸ ì œê±° í›„ ê¸¸ì´: {len(text)}")
        
        # ğŸ”¥ êµ¬ ì•± ì´ë¦„ì„ ë°”ì´ë¸” ì• í”Œë¡œ êµì²´ (ì „ì²˜ë¦¬ ë‹¨ê³„ì—ì„œ)
        # ì¤‘ë³µ ë°©ì§€ë¥¼ ìœ„í•´ ìˆœì„œë¥¼ ì¡°ì •: ì „ì²´ íŒ¨í„´ë¶€í„° ì²˜ë¦¬
        text = re.sub(r'ë°”ì´ë¸”\s*ì• í”Œ\s*\(êµ¬\)\s*ë‹¤ë²ˆì—­\s*ì„±ê²½\s*ì°¬ì†¡', 'ë°”ì´ë¸” ì• í”Œ', text, flags=re.IGNORECASE)
        text = re.sub(r'ë°”ì´ë¸”\s*ì• í”Œ\s*\(êµ¬\)\s*ë‹¤ë²ˆì—­ì„±ê²½ì°¬ì†¡', 'ë°”ì´ë¸” ì• í”Œ', text, flags=re.IGNORECASE)
        text = re.sub(r'\(êµ¬\)\s*ë‹¤ë²ˆì—­\s*ì„±ê²½\s*ì°¬ì†¡', 'ë°”ì´ë¸” ì• í”Œ', text, flags=re.IGNORECASE)
        text = re.sub(r'\(êµ¬\)\s*ë‹¤ë²ˆì—­ì„±ê²½ì°¬ì†¡', 'ë°”ì´ë¸” ì• í”Œ', text, flags=re.IGNORECASE)
        text = re.sub(r'ë‹¤ë²ˆì—­\s*ì„±ê²½\s*ì°¬ì†¡', 'ë°”ì´ë¸” ì• í”Œ', text, flags=re.IGNORECASE)
        text = re.sub(r'ë‹¤ë²ˆì—­ì„±ê²½ì°¬ì†¡', 'ë°”ì´ë¸” ì• í”Œ', text, flags=re.IGNORECASE)
        
        # ê³µë°± ë° ì¤„ë°”ê¿ˆ ì •ê·œí™” - ì¼ê´€ëœ í˜•íƒœë¡œ ë³€í™˜
        text = re.sub(r'\n{3,}', '\n\n', text)    # 3ê°œ ì´ìƒ ì¤„ë°”ê¿ˆ â†’ 2ê°œë¡œ ì œí•œ
        text = re.sub(r'[ \t]+', ' ', text)       # ì—°ì† ê³µë°±/íƒ­ â†’ ë‹¨ì¼ ê³µë°±
        text = text.strip()                       # ì•ë’¤ ê³µë°± ì œê±°
        
        logging.info(f"ì „ì²˜ë¦¬ ì™„ë£Œ: ìµœì¢… ê¸¸ì´={len(text)}")
        logging.info(f"ì „ì²˜ë¦¬ ê²°ê³¼ ë¯¸ë¦¬ë³´ê¸°: {text[:100]}...")
        
        return text

    # â˜† JSON ë¬¸ìì—´ ì´ìŠ¤ì¼€ì´í”„ ì²˜ë¦¬ (íŠ¹ìˆ˜ë¬¸ìê°€ í¬í•¨ëœ í…ìŠ¤íŠ¸ë¥¼ JSONìœ¼ë¡œ ì•ˆì „í•˜ê²Œ ë³€í™˜)
    def escape_json_string(self, text: str) -> str:
        
        if not text:
            return ""
        escaped = json_module.dumps(text, ensure_ascii=False) # ensure_ascii=False: í•œê¸€ ê¹¨ì§ ë°©ì§€
        return escaped[1:-1]  # ì•ë’¤ ë”°ì˜´í‘œ ì œê±°

    # â˜† OpenAI APIë¥¼ ì‚¬ìš©í•˜ì—¬ í…ìŠ¤íŠ¸ë¥¼ ë²¡í„°ë¡œ ë³€í™˜í•˜ëŠ” ë©”ì„œë“œ
    # ë²¡í„° ì„ë² ë”©: í…ìŠ¤íŠ¸ì˜ ì˜ë¯¸ë¥¼ ìˆ˜ì¹˜ ë°°ì—´ë¡œ í‘œí˜„í•˜ì—¬ ìœ ì‚¬ë„ ê³„ì‚° ê°€ëŠ¥
    def create_embedding(self, text: str) -> Optional[list]:

        # ë¹ˆ ë¬¸ìì—´ë¿ë§Œ ì•„ë‹ˆë¼ ê³µë°±ë§Œ ìˆëŠ” ë¬¸ìì—´ë„ ê±¸ëŸ¬ëƒ„ (ì´ëŸ° ê²½ìš° JSON ë³€í™˜ ë¶ˆê°€)
        if not text or not text.strip():
            return None
            
        try:
            with memory_cleanup(): # ë©”ëª¨ë¦¬ ëˆ„ìˆ˜ ë°©ì§€ (ë¸”ë¡ ì¢…ë£Œ ì‹œ ê°€ë¹„ì§€ ì»¬ë ‰ì…˜)
                # OpenAI Embedding API í˜¸ì¶œ
                response = self.openai_client.embeddings.create(
                    model='text-embedding-3-small',    # ë²¡í„° ì„ë² ë”© ëª¨ë¸ëª…
                    input=text[:8000]                  # í…ìŠ¤íŠ¸ ê¸¸ì´ ì œí•œ (í† í° ì œí•œ ë°©ì§€ì§€)
                )
                
                # ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±ì„ ìœ„í•´ ë²¡í„°ë§Œ ë³µì‚¬ í›„ ì‘ë‹µ ê°ì²´ ì‚­ì œ
                embedding = response.data[0].embedding.copy() # ì„ë² ë”© ë²¡í„°ë§Œ ë³µì‚¬í•˜ì—¬ ë°˜í™˜ (ê¹Šì€ ë³µì‚¬ë¡œ ë…ë¦½ì ì¸ ë¦¬ìŠ¤íŠ¸ ìƒì„±)
                del response  # ì›ë³¸ ì‘ë‹µ ê°ì²´ ì¦‰ì‹œ ì‚­ì œ (ë©”ëª¨ë¦¬ í•´ì œ)
                return embedding
                
        except Exception as e:
            logging.error(f"ì„ë² ë”© ìƒì„± ì‹¤íŒ¨: {e}")
            return None

    # â˜† Pinecone ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ìœ ì‚¬í•œ ë‹µë³€ì„ ê²€ìƒ‰í•˜ëŠ” ë©”ì„œë“œ
    # Args:
    #     query (str): ê²€ìƒ‰í•  ì§ˆë¬¸
    #     top_k (int): ê²€ìƒ‰í•  ìµœëŒ€ ê°œìˆ˜ (ê¸°ë³¸ê°’: 5)
    #     similarity_threshold (float): ìœ ì‚¬ë„ ì„ê³„ê°’ (ê¸°ë³¸ê°’: 0.3)
            
    # Returns:
    #     list: ìœ ì‚¬ ë‹µë³€ ë¦¬ìŠ¤íŠ¸ [{'score': float, 'question': str, 'answer': str, ...}, ...]
    def search_similar_answers(self, query: str, top_k: int = 5, similarity_threshold: float = 0.7, lang: str = 'ko') -> list:
        try:
            with memory_cleanup():
                # â˜… ë””ë²„ê¹… ë¡œê·¸ ì¶”ê°€
                logging.info(f"=== ê²€ìƒ‰ ì‹œì‘ ===")
                logging.info(f"ì›ë³¸ ì§ˆë¬¸: {query[:100]}")
                
                # â˜… ì˜¤íƒ€ ìˆ˜ì • ì ìš© (ë™ê¸°í™”ì™€ ë™ì¼í•˜ê²Œ!)
                if lang == 'ko':
                    corrected_query = self.fix_korean_typos_with_ai(query)
                    logging.info(f"ì˜¤íƒ€ ìˆ˜ì • í›„: {corrected_query[:100]}")
                    query_to_embed = corrected_query
                else:
                    query_to_embed = query
                
                # ì„ë² ë”© ìƒì„±
                query_vector = self.create_embedding(query_to_embed)
                
                if query_vector is None:
                    logging.error("ì„ë² ë”© ìƒì„± ì‹¤íŒ¨")
                    return []
                
                # Pinecone ê²€ìƒ‰
                results = index.query(
                    vector=query_vector,
                    top_k=top_k * 2,  # ë” ë§ì´ ê²€ìƒ‰
                    include_metadata=True
                )
                
                logging.info(f"Pinecone ê²€ìƒ‰ ê²°ê³¼: {len(results['matches'])}ê°œ")
                
                # í•œêµ­ì–´ ë²¡í„°ë¡œ ì¶”ê°€ ê²€ìƒ‰ (ì˜ì–´ ì§ˆë¬¸ì¸ ê²½ìš°)
                korean_vector = None  # ì´ˆê¸°í™”í•˜ì—¬ NameError ë°©ì§€
                if lang == 'en':
                    # ì˜ì–´ ì¿¼ë¦¬ë¥¼ í•œêµ­ì–´ë¡œ ë²ˆì—­ í›„ ì„ë² ë”© ìƒì„± (ëˆ„ë½ëœ ë¡œì§ ì¶”ê°€)
                    korean_query = self.translate_text(query_to_embed, 'en', 'ko')
                    korean_vector = self.create_embedding(korean_query)
                    if korean_vector:
                        korean_results = index.query(
                            vector=korean_vector,       # ê²€ìƒ‰í•  ë²¡í„°
                            top_k=3,                    # ë³´ì¡° ê²€ìƒ‰ì€ ì ê²Œ (3ê°œ)
                            include_metadata=True       # ë©”íƒ€ë°ì´í„° í¬í•¨ (ì§ˆë¬¸, ë‹µë³€, ì¹´í…Œê³ ë¦¬ ë“±)
                        )
                        # ê²°ê³¼ ë³‘í•© (ì¤‘ë³µ ì œê±°)
                        seen_ids = set()
                        merged_matches = []
                        for match in results['matches'] + korean_results['matches']:
                            if match['id'] not in seen_ids:
                                seen_ids.add(match['id'])
                                merged_matches.append(match)
                        results['matches'] = sorted(merged_matches, key=lambda x: x['score'], reverse=True)[:top_k]
                
                # 3. ê²°ê³¼ í•„í„°ë§ ë° êµ¬ì¡°í™”
                filtered_results = []
                for i, match in enumerate(results['matches']): # enumerateë¡œ ìˆœìœ„(rank) ìƒì„±
                    score = match['score'] # ìœ ì‚¬ë„ ì ìˆ˜ (0~1, ë†’ì„ìˆ˜ë¡ ìœ ì‚¬)
                    question = match['metadata'].get('question', '')
                    answer = match['metadata'].get('answer', '')
                    category = match['metadata'].get('category', 'ì¼ë°˜')
                    
                    # â˜… ì„ê³„ê°’ ë¡œì§ ëŒ€í­ ì™„í™” - í•­ìƒ ìµœì†Œ 3ê°œëŠ” ë°˜í™˜í•˜ë„ë¡ ìˆ˜ì •
                    include_result = False
                    
                    if score >= similarity_threshold:
                        include_result = True
                        logging.info(f"ì„ê³„ê°’ í†µê³¼: {score:.3f} >= {similarity_threshold:.2f}")
                    elif i < 3:  # ìƒìœ„ 3ê°œëŠ” ë¬´ì¡°ê±´ í¬í•¨
                        include_result = True
                        logging.info(f"ìƒìœ„ {i+1}ë²ˆì§¸ ê²°ê³¼ë¡œ ê°•ì œ í¬í•¨: {score:.3f}")
                    elif score >= 0.3:  # ë§¤ìš° ë‚®ì€ ì„ê³„ê°’ìœ¼ë¡œ ì¶”ê°€ í¬í•¨
                        include_result = True
                        logging.info(f"ë‚®ì€ ì„ê³„ê°’ í†µê³¼: {score:.3f} >= 0.3")
                    
                    if include_result:
                        filtered_results.append({
                            'score': score,
                            'question': question,
                            'answer': answer,
                            'category': category,
                            'rank': i + 1,
                            'lang': 'ko'  # ì›ë³¸ ë°ì´í„°ëŠ” í•œêµ­ì–´
                        })
                        
                        # ë””ë²„ê¹…ì„ ìœ„í•œ ìƒì„¸ ë¡œê¹…
                        logging.info(f"â˜… í¬í•¨ëœ ìœ ì‚¬ ë‹µë³€ #{i+1}: ì ìˆ˜={score:.3f}, ì¹´í…Œê³ ë¦¬={category}, ì–¸ì–´={lang}")
                        logging.info(f"ì°¸ê³  ì§ˆë¬¸: {question[:50]}...")
                        logging.info(f"ì°¸ê³  ë‹µë³€: {answer[:100]}...")
                    else:
                        logging.info(f"Ã— ì œì™¸ëœ ë‹µë³€ #{i+1}: ì ìˆ˜={score:.3f} (ë„ˆë¬´ ë‚®ìŒ)")
                        
                # 4. ë©”ëª¨ë¦¬ ì •ë¦¬
                del results # ì›ë³¸ ì‘ë‹µ ê°ì²´ ì¦‰ì‹œ ì‚­ì œ (ë©”ëª¨ë¦¬ í•´ì œ)
                if korean_vector is not None:
                    del korean_vector # í•œêµ­ì–´ ë²¡í„° ì¦‰ì‹œ ì‚­ì œ (ë©”ëª¨ë¦¬ í•´ì œ)
                del query_vector # ê²€ìƒ‰ ë²¡í„° ì¦‰ì‹œ ì‚­ì œ (ë©”ëª¨ë¦¬ í•´ì œ)
                
                logging.info(f"ì´ {len(filtered_results)}ê°œì˜ ìœ ì‚¬ ë‹µë³€ ê²€ìƒ‰ ì™„ë£Œ (ì–¸ì–´: {lang})")
                return filtered_results
                
        except Exception as e:
            logging.error(f"Pinecone ê²€ìƒ‰ ì‹¤íŒ¨: {str(e)}, query: {query[:50]}..., lang: {lang}")
            return []

    # â˜† GPTë¥¼ ì‚¬ìš©í•œ ë²ˆì—­
    # Args:
    #     text (str): ë²ˆì—­í•  í…ìŠ¤íŠ¸
    #     source_lang (str): ì›ë³¸ ì–¸ì–´
    #     target_lang (str): ë²ˆì—­ ì–¸ì–´
            
    # Returns:
    #     str: ë²ˆì—­ëœ í…ìŠ¤íŠ¸
    def translate_text(self, text: str, source_lang: str, target_lang: str) -> str:
        try:
            # ì–¸ì–´ ë§¤í•‘
            lang_map = {
                'ko': 'Korean',
                'en': 'English'
            }
            
            system_prompt = f"You are a professional translator. Translate the following text from {lang_map[source_lang]} to {lang_map[target_lang]}. Keep the same tone and style. Only provide the translation without any explanation."
            
            response = self.openai_client.chat.completions.create(
                model='gpt-3.5-turbo',
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": text}
                ],
                max_tokens=600,
                temperature=0.5
            )
            
            return response.choices[0].message.content.strip()
            
        except Exception as e:
            logging.error(f"ë²ˆì—­ ì‹¤íŒ¨: {e}")
            return text

    # â˜† AI ê¸°ë°˜ ì§ˆë¬¸ ì˜ë„ ë¶„ì„ ë©”ì„œë“œ (ê°œì„ ëœ ë²„ì „)
    def analyze_question_intent(self, query: str) -> dict:
        """AIë¥¼ ì´ìš©í•´ ì§ˆë¬¸ì˜ ì˜ë„ì™€ í•µì‹¬ ë‚´ìš©ì„ ë¶„ì„"""
        try:
            with memory_cleanup():
                system_prompt = """ë‹¹ì‹ ì€ ê³ ê° ë¬¸ì˜ ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. 
ê³ ê°ì˜ ì§ˆë¬¸ì„ ë¶„ì„í•˜ì—¬ ë‹¤ìŒ ì •ë³´ë¥¼ JSON í˜•íƒœë¡œ ë°˜í™˜í•˜ì„¸ìš”:

{
  "intent_type": "ë¬¸ì˜ ìœ í˜• (ì˜ˆ: ì˜¤íƒˆìì‹ ê³ , ê¸°ëŠ¥ë¬¸ì˜, ê¸°ìˆ ì§€ì›, ê°œì„ ì œì•ˆ, ì¼ë°˜ë¬¸ì˜)",
  "main_topic": "ì£¼ìš” ì£¼ì œ (ì˜ˆ: ì„±ê²½ë³¸ë¬¸, ìŒì›ì¬ìƒ, ê²€ìƒ‰ê¸°ëŠ¥, ë²ˆì—­ë³¸, ì•±ê¸°ëŠ¥)",
  "specific_request": "êµ¬ì²´ì  ìš”ì²­ì‚¬í•­ ìš”ì•½",
  "keywords": ["í•µì‹¬", "í‚¤ì›Œë“œ", "ëª©ë¡"],
  "urgency": "ê¸´ê¸‰ë„ (low/medium/high)"
}

ë¶„ì„ ì‹œ ì£¼ì˜ì‚¬í•­:
- ì§ˆë¬¸ì˜ í•µì‹¬ ì˜ë„ë¥¼ ì •í™•íˆ íŒŒì•…í•˜ì„¸ìš”
- êµ¬ì²´ì ì¸ ë¬¸ì œë‚˜ ìš”ì²­ì‚¬í•­ì„ ì‹ë³„í•˜ì„¸ìš”
- ê¸°ì¡´ ì¹´í…Œê³ ë¦¬ì— ì–½ë§¤ì´ì§€ ë§ê³  ìœ ì—°í•˜ê²Œ ë¶„ì„í•˜ì„¸ìš”"""

                user_prompt = f"ë‹¤ìŒ ê³ ê° ë¬¸ì˜ë¥¼ ë¶„ì„í•´ì£¼ì„¸ìš”: {query}"

                response = self.openai_client.chat.completions.create(
                    model='gpt-3.5-turbo',
                    messages=[
                        {"role": "system", "content": system_prompt},
                        {"role": "user", "content": user_prompt}
                    ],
                    max_tokens=300,
                    temperature=0.3
                )
                
                result_text = response.choices[0].message.content.strip()
                
                # JSON íŒŒì‹± ì‹œë„
                try:
                    
                    result = json.loads(result_text)
                    logging.info(f"AI ì˜ë„ ë¶„ì„ ê²°ê³¼: {result}")
                    return result
                except json.JSONDecodeError:
                    logging.warning(f"JSON íŒŒì‹± ì‹¤íŒ¨, ê¸°ë³¸ê°’ ë°˜í™˜: {result_text}")
                    return {
                        "intent_type": "ì¼ë°˜ë¬¸ì˜",
                        "main_topic": "ê¸°íƒ€",
                        "specific_request": query[:100],
                        "keywords": [query[:20]],
                        "urgency": "medium"
                    }
                
        except Exception as e:
            logging.error(f"AI ì˜ë„ ë¶„ì„ ì‹¤íŒ¨: {e}")
            return {
                "intent_type": "ì¼ë°˜ë¬¸ì˜", 
                "main_topic": "ê¸°íƒ€",
                "specific_request": query[:100],
                "keywords": [query[:20]],
                "urgency": "medium"
            }

    # â˜† ê²€ìƒ‰ëœ ìœ ì‚¬ ë‹µë³€ë“¤ì˜ í’ˆì§ˆì„ ë¶„ì„í•˜ì—¬ ìµœì ì˜ ë‹µë³€ ìƒì„± ì „ëµì„ ê²°ì •í•˜ëŠ” ë©”ì„œë“œ

    # Args:
    #     similar_answers (list): ê²€ìƒ‰ëœ ìœ ì‚¬ ë‹µë³€ ë¦¬ìŠ¤íŠ¸
    #     query (str): ì›ë³¸ ì§ˆë¬¸
    #     
    # Returns:
    #     dict: ë¶„ì„ ê²°ê³¼ ë° ê¶Œì¥ ì ‘ê·¼ ë°©ì‹
    def analyze_context_quality(self, similar_answers: list, query: str) -> dict:
        # ìœ ì‚¬ ë‹µë³€ì´ ì—†ìœ¼ë©´ ê¸°ë³¸ê°’ ë°˜í™˜
        if not similar_answers:
            return {
                'has_good_context': False,
                'best_score': 0.0,
                'recommended_approach': 'fallback',
                'context_summary': 'ìœ ì‚¬ ë‹µë³€ì´ ì—†ìŠµë‹ˆë‹¤.',
                'question_type': 'ì¼ë°˜ë¬¸ì˜',
                'context_relevance': 'none'
            }
        
        # ğŸ”¥ AI ê¸°ë°˜ ì§ˆë¬¸ ì˜ë„ ë¶„ì„ ì¶”ê°€
        question_analysis = self.analyze_question_intent(query)
        question_type = question_analysis.get('intent_type', 'ì¼ë°˜ë¬¸ì˜')
        logging.info(f"AI ë¶„ì„ ê²°ê³¼: {question_analysis}")
        
        # í’ˆì§ˆ ì§€í‘œ ê³„ì‚°
        best_score = similar_answers[0]['score']
        high_quality_count = len([ans for ans in similar_answers if ans['score'] >= 0.7])
        medium_quality_count = len([ans for ans in similar_answers if 0.5 <= ans['score'] < 0.7])
        
        # ğŸ”¥ ì¹´í…Œê³ ë¦¬ ì¼ì¹˜ë„ ë¶„ì„ ì¶”ê°€
        categories = [ans['category'] for ans in similar_answers[:5]]
        category_distribution = {cat: categories.count(cat) for cat in set(categories)}
        
        # ğŸ”¥ ì§ˆë¬¸ ì˜ë„ì™€ ë‹µë³€ ì¹´í…Œê³ ë¦¬ ì¼ì¹˜ë„ ê²€ì‚¬
        context_relevance = self.check_context_relevance_ai(question_analysis, categories, query, similar_answers[:3])
        logging.info(f"ì»¨í…ìŠ¤íŠ¸ ê´€ë ¨ì„±: {context_relevance}")
        
        # ğŸ”¥ ì˜ì‚¬ ê²°ì • íŠ¸ë¦¬ ê°œì„  - ê´€ë ¨ì„±ì„ ê³ ë ¤í•œ ì „ëµ ê²°ì •
        if context_relevance == 'irrelevant':
            # ê´€ë ¨ì„±ì´ ì—†ìœ¼ë©´ ë¬´ì¡°ê±´ í´ë°± ì²˜ë¦¬
            approach = 'fallback'
            logging.warning(f"ì§ˆë¬¸ ìœ í˜•({question_type})ê³¼ ê²€ìƒ‰ëœ ë‹µë³€ì˜ ê´€ë ¨ì„±ì´ ë‚®ì•„ í´ë°± ì²˜ë¦¬")
        elif best_score >= 0.95 and context_relevance in ['high', 'medium']:
            approach = 'direct_use'
        elif best_score >= 0.8 and context_relevance == 'high':
            approach = 'direct_use'
        elif best_score >= 0.7 and context_relevance in ['high', 'medium']:
            approach = 'gpt_with_strong_context'
        elif best_score >= 0.5 and context_relevance == 'high':
            approach = 'gpt_with_strong_context'
        elif best_score >= 0.4 and context_relevance in ['high', 'medium']:
            approach = 'gpt_with_weak_context'
        else:
            approach = 'fallback'
        
        # ë¶„ì„ ê²°ê³¼ êµ¬ì¡°í™”
        analysis = {
            'has_good_context': context_relevance in ['high', 'medium'] and best_score >= 0.4,
            'best_score': best_score,
            'high_quality_count': high_quality_count,
            'medium_quality_count': medium_quality_count,
            'category_distribution': category_distribution,
            'recommended_approach': approach,
            'question_analysis': question_analysis,
            'question_type': question_type,
            'context_relevance': context_relevance,
            'context_summary': f"ì˜ë„: {question_type}, ì£¼ì œ: {question_analysis.get('main_topic', 'N/A')}, ê´€ë ¨ì„±: {context_relevance}, ìµœê³ ì ìˆ˜: {best_score:.3f}"
        }
        
        logging.info(f"í–¥ìƒëœ ì»¨í…ìŠ¤íŠ¸ ë¶„ì„ ê²°ê³¼: {analysis}")
        return analysis

    # â˜† AI ê¸°ë°˜ ì»¨í…ìŠ¤íŠ¸ ê´€ë ¨ì„± ê²€ì‚¬ ë©”ì„œë“œ (ê°œì„ ëœ ë²„ì „)
    def check_context_relevance_ai(self, question_analysis: dict, answer_categories: list, query: str, top_answers: list) -> str:
        """AIë¥¼ ì´ìš©í•´ ì§ˆë¬¸ ì˜ë„ì™€ ë‹µë³€ì˜ ê´€ë ¨ì„±ì„ ì§€ëŠ¥ì ìœ¼ë¡œ ê²€ì‚¬"""
        
        try:
            # ìƒìœ„ ë‹µë³€ë“¤ì˜ ë‚´ìš© ìš”ì•½
            answer_summaries = []
            for i, answer in enumerate(top_answers[:3]):
                answer_text = answer.get('answer', '')[:200]  # ì²« 200ìë§Œ
                answer_summaries.append(f"ë‹µë³€{i+1}: {answer_text}")
            
            combined_answers = "\n".join(answer_summaries)
            
            with memory_cleanup():
                system_prompt = """ë‹¹ì‹ ì€ ë¬¸ì˜-ë‹µë³€ ê´€ë ¨ì„± ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
ê³ ê°ì˜ ì§ˆë¬¸ ì˜ë„ì™€ ê²€ìƒ‰ëœ ë‹µë³€ë“¤ì˜ ê´€ë ¨ì„±ì„ ë¶„ì„í•˜ì—¬ ë‹¤ìŒ ì¤‘ í•˜ë‚˜ë¡œ íŒì •í•˜ì„¸ìš”:

- "high": ë‹µë³€ì´ ì§ˆë¬¸ê³¼ ì§ì ‘ì ìœ¼ë¡œ ê´€ë ¨ë˜ê³  ë„ì›€ì´ ë¨
- "medium": ë‹µë³€ì´ ì–´ëŠ ì •ë„ ê´€ë ¨ì´ ìˆì§€ë§Œ ì™„ì „íˆ ì¼ì¹˜í•˜ì§€ëŠ” ì•ŠìŒ  
- "low": ë‹µë³€ì´ ì•½ê°„ ê´€ë ¨ì´ ìˆì§€ë§Œ ì§ˆë¬¸ì˜ í•µì‹¬ê³¼ëŠ” ê±°ë¦¬ê°€ ìˆìŒ
- "irrelevant": ë‹µë³€ì´ ì§ˆë¬¸ê³¼ ì „í˜€ ê´€ë ¨ì´ ì—†ìŒ

ë¶„ì„ ê¸°ì¤€:
1. ì§ˆë¬¸ì˜ í•µì‹¬ ì˜ë„ì™€ ë‹µë³€ ë‚´ìš©ì˜ ì¼ì¹˜ë„
2. ë¬¸ì œ í•´ê²°ì— ì‹¤ì§ˆì  ë„ì›€ì´ ë˜ëŠ”ì§€ ì—¬ë¶€
3. ì§ˆë¬¸ ìœ í˜•ê³¼ ë‹µë³€ ìœ í˜•ì˜ ì í•©ì„±

ê²°ê³¼ëŠ” "high", "medium", "low", "irrelevant" ì¤‘ í•˜ë‚˜ë§Œ ë°˜í™˜í•˜ì„¸ìš”."""

                user_prompt = f"""ì§ˆë¬¸ ë¶„ì„ ê²°ê³¼:
ì˜ë„: {question_analysis.get('intent_type', 'N/A')}
ì£¼ì œ: {question_analysis.get('main_topic', 'N/A')}
êµ¬ì²´ì  ìš”ì²­: {question_analysis.get('specific_request', 'N/A')}

ì›ë³¸ ì§ˆë¬¸: {query}

ê²€ìƒ‰ëœ ë‹µë³€ë“¤:
{combined_answers}

ìœ„ ì§ˆë¬¸ê³¼ ë‹µë³€ë“¤ì˜ ê´€ë ¨ì„±ì„ ë¶„ì„í•´ì£¼ì„¸ìš”."""

                response = self.openai_client.chat.completions.create(
                    model='gpt-3.5-turbo',
                    messages=[
                        {"role": "system", "content": system_prompt},
                        {"role": "user", "content": user_prompt}
                    ],
                    max_tokens=50,
                    temperature=0.2
                )
                
                result = response.choices[0].message.content.strip().lower()
                
                # ê²°ê³¼ ì •ê·œí™”
                if 'high' in result:
                    return 'high'
                elif 'medium' in result:
                    return 'medium'
                elif 'low' in result:
                    return 'low'
                elif 'irrelevant' in result:
                    return 'irrelevant'
                else:
                    logging.warning(f"AI ê´€ë ¨ì„± ë¶„ì„ ê²°ê³¼ íŒŒì‹± ì‹¤íŒ¨: {result}")
                    return 'medium'  # ê¸°ë³¸ê°’
                    
        except Exception as e:
            logging.error(f"AI ê´€ë ¨ì„± ë¶„ì„ ì‹¤íŒ¨: {e}")
            # í´ë°±: ê¸°ë³¸ì ì¸ í‚¤ì›Œë“œ ë§¤ì¹­
            return self.fallback_relevance_check(query, top_answers)
    
    # â˜† í´ë°± ê´€ë ¨ì„± ê²€ì‚¬ ë©”ì„œë“œ (AI ì‹¤íŒ¨ ì‹œ ì‚¬ìš©)
    def fallback_relevance_check(self, query: str, top_answers: list) -> str:
        """AI ë¶„ì„ ì‹¤íŒ¨ ì‹œ ì‚¬ìš©í•˜ëŠ” ê¸°ë³¸ì ì¸ í‚¤ì›Œë“œ ë§¤ì¹­"""
        query_words = set(self.extract_keywords(query.lower()))
        
        max_overlap = 0
        for answer in top_answers:
            answer_words = set(self.extract_keywords(answer.get('answer', '').lower()))
            overlap = len(query_words & answer_words)
            overlap_ratio = overlap / max(len(query_words), 1)
            max_overlap = max(max_overlap, overlap_ratio)
        
        if max_overlap >= 0.5:
            return 'high'
        elif max_overlap >= 0.3:
            return 'medium'
        elif max_overlap >= 0.1:
            return 'low'
        else:
            return 'irrelevant'
    
    # â˜† í•µì‹¬ í‚¤ì›Œë“œ ì¶”ì¶œ ë©”ì„œë“œ
    def extract_keywords(self, text: str) -> list:
        """í…ìŠ¤íŠ¸ì—ì„œ í•µì‹¬ í‚¤ì›Œë“œ ì¶”ì¶œ"""
        # ë¶ˆìš©ì–´ ì œê±°ìš© ë¦¬ìŠ¤íŠ¸
        stop_words = {'ëŠ”', 'ì€', 'ì´', 'ê°€', 'ì„', 'ë¥¼', 'ì—', 'ì—ì„œ', 'ë¡œ', 'ìœ¼ë¡œ', 'ì™€', 'ê³¼', 'ì˜', 'ë„', 'ë§Œ', 'ê¹Œì§€', 'ë¶€í„°', 'ê»˜ì„œ', 'ì—ê²Œ', 'í•œí…Œ', 'ë¡œë¶€í„°', 'ìœ¼ë¡œë¶€í„°'}
        
        # íŠ¹ìˆ˜ë¬¸ì ì œê±° ë° ë‹¨ì–´ ë¶„ë¦¬
        
        words = re.findall(r'[ê°€-í£a-zA-Z0-9]+', text)
        
        # ë¶ˆìš©ì–´ ì œê±° ë° 2ê¸€ì ì´ìƒ ë‹¨ì–´ë§Œ ì„ íƒ
        keywords = [word for word in words if len(word) >= 2 and word not in stop_words]
        
        return keywords
    

    # â˜† ì°¸ê³  ë‹µë³€ì—ì„œ ì¸ì‚¬ë§ê³¼ ëë§ºìŒë§ì„ ì œê±°í•˜ëŠ” ë©”ì„œë“œ
    # Args:
    #     text (str): ì œê±°í•  í…ìŠ¤íŠ¸
    #     lang (str): ì–¸ì–´ (ê¸°ë³¸ê°’: í•œêµ­ì–´)
            
    # Returns:
    #     str: ì œê±°ëœ í…ìŠ¤íŠ¸
    def remove_greeting_and_closing(self, text: str, lang: str = 'ko') -> str:
        # null ì²´í¬
        if not text:
            return ""
        
        if lang == 'ko':
            # í•œêµ­ì–´ ì¸ì‚¬ë§ ì œê±° íŒ¨í„´
            greeting_patterns = [
                r'^ì•ˆë…•í•˜ì„¸ìš”[^.]*\.\s*',
                r'^GOODTV\s+ë°”ì´ë¸”\s*ì• í”Œ[^.]*\.\s*',
                r'^ë°”ì´ë¸”\s*ì• í”Œ[^.]*\.\s*',
                r'^ì„±ë„ë‹˜[^.]*\.\s*',
                r'^ê³ ê°ë‹˜[^.]*\.\s*',
                r'^ê°ì‚¬í•©ë‹ˆë‹¤[^.]*\.\s*',
                r'^ê°ì‚¬ë“œë¦½ë‹ˆë‹¤[^.]*\.\s*',
                r'^ë°”ì´ë¸”\s*ì• í”Œì„\s*ì´ìš©í•´ì£¼ì…”ì„œ[^.]*\.\s*',
                r'^ë°”ì´ë¸”\s*ì• í”Œì„\s*ì• ìš©í•´\s*ì£¼ì…”ì„œ[^.]*\.\s*'
            ]
            
            # í•œêµ­ì–´ ëë§ºìŒë§ ì œê±° íŒ¨í„´ë“¤
            closing_patterns = [
                r'\s*ê°ì‚¬í•©ë‹ˆë‹¤[^.]*\.?\s*$',
                r'\s*ê°ì‚¬ë“œë¦½ë‹ˆë‹¤[^.]*\.?\s*$',
                r'\s*í‰ì•ˆí•˜ì„¸ìš”[^.]*\.?\s*$',
                r'\s*ì£¼ë‹˜\s*ì•ˆì—ì„œ[^.]*\.?\s*$',
                r'\s*í•¨ê»˜\s*ê¸°ë„í•˜ë©°[^.]*\.?\s*$',
                r'\s*í•­ìƒ[^.]*ë°”ì´ë¸”\s*ì• í”Œ[^.]*\.?\s*$',
                r'\s*í•­ìƒ\s*ì£¼ë‹˜\s*ì•ˆì—ì„œ[^.]*\.?\s*$',
                r'\s*ì£¼ë‹˜\s*ì•ˆì—ì„œ\s*í‰ì•ˆí•˜ì„¸ìš”[^.]*\.?\s*$',
                r'\s*ì£¼ë‹˜ì˜\s*ì€ì´ì´[^.]*\.?\s*$',
                r'\s*ê¸°ë„ë“œë¦¬ê² ìŠµë‹ˆë‹¤[^.]*\.?\s*$'
            ]

        else:  # ì˜ì–´ ì¸ì‚¬ë§ ì œê±° íŒ¨í„´
            greeting_patterns = [
                r'^Hello[^.]*\.\s*',
                r'^Hi[^.]*\.\s*',
                r'^Dear[^.]*\.\s*',
                r'^Thank you[^.]*\.\s*',
                r'^Thanks[^.]*\.\s*',
                r'^This is GOODTV Bible App[^.]*\.\s*',
            ]
            
            # ì˜ì–´ ëë§ºìŒë§ ì œê±° íŒ¨í„´
            closing_patterns = [
                r'\s*Thank you[^.]*\.?\s*$',
                r'\s*Thanks[^.]*\.?\s*$',
                r'\s*Best regards[^.]*\.?\s*$',
                r'\s*Sincerely[^.]*\.?\s*$',
                r'\s*God bless[^.]*\.?\s*$',
                r'\s*May God[^.]*\.?\s*$',
            ]
        
        # ì¸ì‚¬ë§ ì œê±°
        for pattern in greeting_patterns:
            text = re.sub(pattern, '', text, flags=re.IGNORECASE)
        
        # ëë§ºìŒë§ ì œê±°
        for pattern in closing_patterns:
            text = re.sub(pattern, '', text, flags=re.IGNORECASE)
        
        # ë¬¸ì¥ ëì˜ ëë§ºìŒë§ë“¤ë„ ì œê±°
        text = re.sub(r'[,.!?]\s*í•­ìƒ\s*ì£¼ë‹˜\s*ì•ˆì—ì„œ[^.]*\.?\s*$', '', text, flags=re.IGNORECASE)
        text = re.sub(r'[,.!?]\s*ê°ì‚¬í•©ë‹ˆë‹¤[^.]*\.?\s*$', '', text, flags=re.IGNORECASE)
        text = re.sub(r'[,.!?]\s*í‰ì•ˆí•˜ì„¸ìš”[^.]*\.?\s*$', '', text, flags=re.IGNORECASE)
        
        # ì•ë’¤ ê³µë°± ì •ë¦¬
        text = text.strip()
        
        return text

    # â˜† GPT ë‹µë³€ ìƒì„±ì„ ìœ„í•œ í–¥ìƒëœ ì»¨í…ìŠ¤íŠ¸ ìƒì„± ë©”ì„œë“œ
    # 
    # ì»¨í…ìŠ¤íŠ¸ ìƒì„± ì „ëµ:
    # 1. í’ˆì§ˆë³„ ë‹µë³€ ê·¸ë£¹í•‘ (ê³ /ì¤‘/ë‚®ì€ í’ˆì§ˆ)
    # 2. ê³ í’ˆì§ˆ ë‹µë³€ ìš°ì„  ì„ íƒ (ìµœëŒ€ 4ê°œ)
    # 3. ì¤‘í’ˆì§ˆ ë‹µë³€ìœ¼ë¡œ ë³´ì™„ (ìµœëŒ€ 3ê°œ)
    # 4. ìµœì†Œ ê°œìˆ˜ ë¯¸ë‹¬ì‹œ ì¤‘ê°„ í’ˆì§ˆ ë‹µë³€ ì¶”ê°€
    # 5. í…ìŠ¤íŠ¸ ì •ì œ ë° ê¸¸ì´ ì œí•œ (ì¸ì‚¬ë§/ëë§ºìŒë§ ì œê±°)
    # 
    # ì´ë ‡ê²Œ êµ¬ì„±ëœ ì»¨í…ìŠ¤íŠ¸ëŠ” GPTì—ê²Œ ì°¸ê³  ìë£Œë¡œ ì œê³µë˜ì–´
    # ì¼ê´€ëœ ìŠ¤íƒ€ì¼ê³¼ ì •í™•í•œ ì •ë³´ë¡œ ë‹µë³€ì„ ìƒì„±í•˜ê²Œ í•¨
    # 
    # Args:
    #     similar_answers (list): ê²€ìƒ‰ëœ ìœ ì‚¬ ë‹µë³€ ë¦¬ìŠ¤íŠ¸
    #     max_answers (int): í¬í•¨í•  ìµœëŒ€ ë‹µë³€ ê°œìˆ˜ (ê¸°ë³¸ê°’: 7)
    #     
    # Returns:
    #     str: GPTìš© ì»¨í…ìŠ¤íŠ¸ ë¬¸ìì—´
    def create_enhanced_context(self, similar_answers: list, max_answers: int = 7, target_lang: str = 'ko') -> str:
        if not similar_answers:
            return ""
        
        context_parts = [] # ì»¨í…ìŠ¤íŠ¸ ë¶€ë¶„ë“¤ì„ ì €ì¥í•  ë¦¬ìŠ¤íŠ¸
        used_answers = 0 # ì‚¬ìš©ëœ ë‹µë³€ ê°œìˆ˜
        
        # ìœ ì‚¬ë„ ì ìˆ˜ì— ë”°ë¥¸ ë‹µë³€ ê·¸ë£¹í•‘
        # ê³„ì¸µì  ê·¸ë£¹í•‘ìœ¼ë¡œ í’ˆì§ˆë³„ ë‹µë³€ì„ ë¶„ë¥˜: ì´ëŠ” ë‚˜ì¤‘ì— ìš°ì„ ìˆœìœ„ì— ë”°ë¼ ì„ íƒí•˜ê¸° ìœ„í•¨
        high_score = [ans for ans in similar_answers if ans['score'] >= 0.7]      # ê³ í’ˆì§ˆ (70% ì´ìƒ ìœ ì‚¬)
        medium_score = [ans for ans in similar_answers if 0.5 <= ans['score'] < 0.7]  # ì¤‘í’ˆì§ˆ (50-70%)
        medium_low_score = [ans for ans in similar_answers if 0.5 <= ans['score'] < 0.6] # ë‚®ì€ í’ˆì§ˆ (50-60%)

        # 1ë‹¨ê³„: ê³ í’ˆì§ˆ ë‹µë³€ ìš°ì„  í¬í•¨ (ìµœëŒ€ 4ê°œ)
        for ans in high_score[:4]:
            if used_answers >= max_answers:
                break
            # ì œì–´ ë¬¸ì(ì¤„ë°”ê¿ˆ, íƒ­, ê°œí–‰ ë“±) ë° HTML íƒœê·¸ ì œê±°
            clean_answer = re.sub(r'[\b\r\f\v\x00-\x08\x0B\x0C\x0E-\x1F\x7F]|<[^>]+>', '', ans['answer'])
            clean_answer = self.remove_greeting_and_closing(clean_answer, 'ko') # ì¸ì‚¬ë§ê³¼ ëë§ºìŒë§ ì œê±°í•˜ì—¬ ë³¸ë¬¸ë§Œ ì¶”ì¶œ
            
            # ì˜ì–´ ì§ˆë¬¸ì¸ ê²½ìš° ë‹µë³€ì„ ë²ˆì—­
            if target_lang == 'en' and ans.get('lang', 'ko') == 'ko':
                clean_answer = self.translate_text(clean_answer, 'ko', 'en')
            
            # ìœ íš¨ì„± ê²€ì‚¬ ë° ê¸¸ì´ ì œí•œ (ìµœì†Œ 20ì ì´ìƒí™•ì¸, 400ìë¡œ ì˜ë¼ì„œ ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´ ì œí•œ ë°©ì§€)
            if self.is_valid_text(clean_answer, target_lang) and len(clean_answer.strip()) > 20:
                context_parts.append(f"[ì°¸ê³ ë‹µë³€ {used_answers+1} - ì ìˆ˜: {ans['score']:.2f}]\n{clean_answer[:400]}")
                used_answers += 1
        
        # 2ë‹¨ê³„: ì¤‘í’ˆì§ˆ ë‹µë³€ìœ¼ë¡œ ë³´ì™„ (ìµœëŒ€ 3ê°œ)
        for ans in medium_score[:3]:
            if used_answers >= max_answers:
                break
            # ì œì–´ ë¬¸ì(ì¤„ë°”ê¿ˆ, íƒ­, ê°œí–‰ ë“±) ë° HTML íƒœê·¸ ì œê±°
            clean_answer = re.sub(r'[\b\r\f\v\x00-\x08\x0B\x0C\x0E-\x1F\x7F]|<[^>]+>', '', ans['answer'])
            clean_answer = self.remove_greeting_and_closing(clean_answer, 'ko') # ì¸ì‚¬ë§ê³¼ ëë§ºìŒë§ ì œê±°í•˜ì—¬ ë³¸ë¬¸ë§Œ ì¶”ì¶œ
            
            # ì˜ì–´ ì§ˆë¬¸ì¸ ê²½ìš° ë‹µë³€ì„ ë²ˆì—­
            if target_lang == 'en' and ans.get('lang', 'ko') == 'ko':
                clean_answer = self.translate_text(clean_answer, 'ko', 'en')
            
            if self.is_valid_text(clean_answer, target_lang) and len(clean_answer.strip()) > 20:
                context_parts.append(f"[ì°¸ê³ ë‹µë³€ {used_answers+1} - ì ìˆ˜: {ans['score']:.2f}]\n{clean_answer[:300]}")
                used_answers += 1

        # 3ë‹¨ê³„: ë‹µë³€ì´ ë¶€ì¡±í•œ ê²½ìš° ì¤‘ê°„ í’ˆì§ˆ ë‹µë³€ ì¶”ê°€ (50-60% êµ¬ê°„)
        if used_answers < 3:  # ìµœì†Œ 3ê°œ ì´ìƒ í™•ë³´í•˜ê¸° ìœ„í•¨
            for ans in medium_low_score[:2]:
                if used_answers >= max_answers:
                    break
                clean_answer = re.sub(r'[\b\r\f\v\x00-\x08\x0B\x0C\x0E-\x1F\x7F]|<[^>]+>', '', ans['answer'])
                clean_answer = self.remove_greeting_and_closing(clean_answer, 'ko')
                
                # ì˜ì–´ ì§ˆë¬¸ì¸ ê²½ìš° ë‹µë³€ì„ ë²ˆì—­
                if target_lang == 'en' and ans.get('lang', 'ko') == 'ko':
                    clean_answer = self.translate_text(clean_answer, 'ko', 'en')
                
                if self.is_valid_text(clean_answer, target_lang) and len(clean_answer.strip()) > 20:
                    context_parts.append(f"[ì°¸ê³ ë‹µë³€ {used_answers+1} - ì ìˆ˜: {ans['score']:.2f}]\n{clean_answer[:250]}")
                    used_answers += 1
        
        logging.info(f"ì»¨í…ìŠ¤íŠ¸ ìƒì„±: {used_answers}ê°œì˜ ë‹µë³€ í¬í•¨ (ì–¸ì–´: {target_lang})")
        
        # ìµœì¢… ì»¨í…ìŠ¤íŠ¸ ì¡°í•© (êµ¬ë¶„ì„ ìœ¼ë¡œ ë‹µë³€ë“¤ ë¶„ë¦¬)
        return "\n\n" + "="*50 + "\n\n".join(context_parts)

    # â˜† ì´ì „ ì•± ì´ë¦„ì„ ì œê±°í•˜ëŠ” ë©”ì„œë“œ (êµ¬ ë‹¤ë²ˆì—­ì„±ê²½ì°¬ì†¡ ë“±)
    # Args:
    #     text (str): ì œê±°í•  í…ìŠ¤íŠ¸
            
    # Returns:
    #     str: ì œê±°ëœ í…ìŠ¤íŠ¸
    def remove_old_app_name(self, text: str) -> str:
        patterns_to_remove = [
            r'\s*\(êµ¬\)\s*ë‹¤ë²ˆì—­ì„±ê²½ì°¬ì†¡',
            r'\s*\(êµ¬\)ë‹¤ë²ˆì—­ì„±ê²½ì°¬ì†¡',
            r'ë°”ì´ë¸”\s*ì• í”Œ\s*\(êµ¬\)\s*ë‹¤ë²ˆì—­ì„±ê²½ì°¬ì†¡',
            r'ë°”ì´ë¸”ì• í”Œ\s*\(êµ¬\)ë‹¤ë²ˆì—­ì„±ê²½ì°¬ì†¡',
        ]
        
        for pattern in patterns_to_remove:
            text = re.sub(pattern, '', text, flags=re.IGNORECASE)
        
        text = re.sub(r'(GOODTV\s+ë°”ì´ë¸”\s*ì• í”Œ)\s+', r'\1', text)
        
        return text

    # â˜† ë‹µë³€ í…ìŠ¤íŠ¸ë¥¼ HTML ë‹¨ë½ í˜•ì‹ìœ¼ë¡œ í¬ë§·íŒ…í•˜ëŠ” ë©”ì„œë“œ
    def format_answer_with_html_paragraphs(self, text: str, lang: str = 'ko') -> str:
        if not text:
            return ""
        
        text = self.remove_old_app_name(text)
        
        # ë¬¸ì¥ì„ ë§ˆì¹¨í‘œ, ëŠë‚Œí‘œ, ë¬¼ìŒí‘œë¡œ ë¶„ë¦¬
        sentences = re.split(r'(?<=[.!?])\s+', text)
        
        paragraphs = [] # ë‹¨ë½ë“¤ì„ ì €ì¥í•  ë¦¬ìŠ¤íŠ¸
        current_paragraph = [] # í˜„ì¬ ë‹¨ë½ì„ ì €ì¥í•  ë¦¬ìŠ¤íŠ¸
        
        # ë‹¨ë½ ë¶„ë¦¬ íŠ¸ë¦¬ê±° í‚¤ì›Œë“œë“¤ (ë” í¬ê´„ì ìœ¼ë¡œ í™•ì¥)
        # ì ‘ì†ì‚¬ë‚˜ ì¸ì‚¬ë§ë¡œ ì‹œì‘í•˜ëŠ” ë¬¸ì¥ì€ ë“¤ì—¬ì“°ê¸°ë¥¼ í†µí•´ ìƒˆ ë‹¨ë½ìœ¼ë¡œ ë¶„ë¦¬
        if lang == 'ko':
            paragraph_triggers = [
                # ì¸ì‚¬ ë° ê°ì‚¬
                'ì•ˆë…•í•˜ì„¸ìš”', 'ê°ì‚¬í•©ë‹ˆë‹¤', 'ê°ì‚¬ë“œë¦½ë‹ˆë‹¤', 'ë°”ì´ë¸” ì• í”Œì„',
                # ì ‘ì†ì–´ ë° ì—°ê²°ì–´
                'ë”°ë¼ì„œ', 'ê·¸ëŸ¬ë¯€ë¡œ', 'ë˜í•œ', 'ê·¸ë¦¬ê³ ', 'ë˜ëŠ”', 'í•˜ì§€ë§Œ', 'ê·¸ëŸ°ë°',
                'ì´ì™¸', 'ì´ì—', 'ì´ë¥¼', 'ì´ë¡œ', 'ì´ì™€', 'ì´ì—', 'ì´ìƒ', 'ì´í•˜',
                # ìƒí™© ì„¤ëª…
                'í˜„ì¬', 'ì§€ê¸ˆ', 'í˜„ì¬ë¡œ', 'í˜„ì¬ê¹Œì§€', 'í˜„ì¬ë¡œì„œëŠ”',
                'ë‚´ë¶€ì ìœ¼ë¡œ', 'ì™¸ë¶€ì ìœ¼ë¡œ', 'ê¸°ìˆ ì ìœ¼ë¡œ', 'ìš´ì˜ìƒ',
                # ì¡°ê±´ ë° ê°€ì •
                'ë§Œì•½', 'í˜¹ì‹œ', 'ë§Œì¼', 'ë§Œì•½ì—', 'ë§Œì•½ì˜',
                'í•´ë‹¹', 'ì´', 'ê·¸', 'ì €', 'ì´ëŸ°', 'ê·¸ëŸ°', 'ì €ëŸ°',
                # ìš”ì²­ ë° ì•ˆë‚´
                'ì„±ë„ë‹˜', 'ê³ ê°ë‹˜', 'ì´ìš©ì', 'ì‚¬ìš©ì',
                'ë²ˆê±°ë¡œìš°ì‹œ', 'ë¶ˆí¸í•˜ì‹œ', 'ì£„ì†¡í•˜ì§€ë§Œ', 'ì°¸ê³ ë¡œ',
                'ì–‘í•´ë¶€íƒë“œë¦½ë‹ˆë‹¤', 'ì–‘í•´í•´ì£¼ì‹œê¸°', 'ì´í•´í•´ì£¼ì‹œê¸°',
                # ì‹œê°„ ê´€ë ¨
                'í•­ìƒ', 'ëŠ˜', 'ì•ìœ¼ë¡œë„', 'ì§€ì†ì ìœ¼ë¡œ', 'ê³„ì†ì ìœ¼ë¡œ',
                'ì‹œê°„ì´', 'ì†Œìš”ë ', 'ê±¸ë¦´', 'í•„ìš”í•œ',
                # ê¸°ëŠ¥ ê´€ë ¨
                'ê¸°ëŠ¥', 'ê¸°ëŠ¥ì€', 'ê¸°ëŠ¥ì˜', 'ê¸°ëŠ¥ì´', 'ê¸°ëŠ¥ì„',
                'ìŠ¤í”¼ì»¤', 'ë²„íŠ¼', 'ë©”ë‰´', 'í™”ë©´', 'ì„¤ì •', 'ì˜µì…˜',
                # ì˜ê²¬ ë° ì „ë‹¬
                'ì˜ê²¬ì€', 'ì˜ê²¬ì„', 'ì „ë‹¬í• ', 'ì „ë‹¬í•˜ê² ìŠµë‹ˆë‹¤', 'ì „ë‹¬ë“œë¦¬ê² ìŠµë‹ˆë‹¤',
                'í† ì˜ê°€', 'ê²€í† ê°€', 'ê²€í† ë¥¼', 'ë…¼ì˜ê°€', 'ë…¼ì˜ë¥¼'
            ]
        else:  # ì˜ì–´
            paragraph_triggers = [
                # Greetings and appreciation
                'Hello', 'Hi', 'Dear', 'Thank', 'Thanks', 'Appreciate',
                'Grateful', 'Welcome', 'Greetings',
                
                # Conjunctions and transitions
                'Therefore', 'However', 'Additionally', 'Furthermore', 
                'Moreover', 'Nevertheless', 'Nonetheless', 'Meanwhile',
                'Subsequently', 'Consequently', 'Hence', 'Thus', 'Besides',
                'Although', 'Though', 'While', 'Whereas', 'Instead',
                
                # Situation descriptions
                'Currently', 'Presently', 'At the moment', 'Now',
                'At this time', 'As of now', 'Recently', 'Lately',
                'Technically', 'Internally', 'Externally', 'Generally',
                'Specifically', 'Basically', 'Essentially', 'Fundamentally',
                
                # Conditions and assumptions
                'If', 'When', 'Where', 'Whether', 'Unless', 'Provided',
                'Assuming', 'Suppose', 'In case', 'Should', 'Would',
                'Could', 'Might', 'May',
                
                # Requests and guidance
                'Please', 'Kindly', 'We recommend', 'We suggest',
                'You can', 'You may', 'You should', 'You might',
                'Try', 'Consider', 'Note that', 'Be aware',
                'Remember', 'Keep in mind', 'Important',
                
                # Apologies and understanding
                'Sorry', 'Apologize', 'Apologies', 'Unfortunately',
                'Regret', 'Understand', 'Realize', 'Acknowledge',
                'We know', 'We understand', 'We appreciate',
                
                # Time-related
                'Always', 'Usually', 'Often', 'Sometimes', 'Occasionally',
                'Frequently', 'Regularly', 'Continuously', 'Constantly',
                'Soon', 'Shortly', 'Eventually', 'Later', 'Previously',
                
                # Feature and function related
                'Feature', 'Function', 'Option', 'Setting', 'Button',
                'Menu', 'Screen', 'Tab', 'Page', 'Section', 'Tool',
                'Service', 'System', 'Application', 'Update', 'Version',
                
                # Problem-solving related
                'To fix', 'To solve', 'To resolve', 'To address',
                'Solution', 'Resolution', 'Workaround', 'Alternative',
                'Issue', 'Problem', 'Error', 'Bug', 'Trouble',
                
                # Feedback and communication
                'Your feedback', 'Your suggestion', 'Your opinion',
                'We will', 'We are', 'We have', 'Our team',
                'Will be', 'Has been', 'Have been', 'Working on',
                'Looking into', 'Reviewing', 'Considering', 'Planning',
                
                # Instructions and steps
                'First', 'Second', 'Third', 'Next', 'Then', 'After',
                'Before', 'Finally', 'Lastly', 'To begin', 'To start',
                'Step', 'Follow', 'Navigate', 'Click', 'Tap', 'Select',
                
                # Emphasis and clarification
                'Indeed', 'In fact', 'Actually', 'Certainly', 'Definitely',
                'Clearly', 'Obviously', 'Importantly', 'Notably',
                'Particularly', 'Especially', 'Specifically'
            ]
        
        for i, sentence in enumerate(sentences):
            sentence = sentence.strip()
            if not sentence:
                continue
            
            # ì²« ë²ˆì§¸ ë¬¸ì¥ (ì¸ì‚¬ë§)ì€ í•­ìƒ ë³„ë„ ë‹¨ë½
            if i == 0:
                if current_paragraph:
                    paragraphs.append(' '.join(current_paragraph))
                    current_paragraph = []
                paragraphs.append(sentence)
                continue
            
            should_break = False
            
            # íŠ¸ë¦¬ê±° í‚¤ì›Œë“œë¡œ ì‹œì‘í•˜ëŠ” ë¬¸ì¥ì€ ìƒˆ ë‹¨ë½
            for trigger in paragraph_triggers:
                if sentence.startswith(trigger):
                    should_break = True
                    break
            
            # í˜„ì¬ ë‹¨ë½ì— 2ê°œ ì´ìƒ ë¬¸ì¥ì´ ìˆìœ¼ë©´ ìƒˆ ë‹¨ë½
            if current_paragraph and len(current_paragraph) >= 2:
                should_break = True

            # ëë§ºìŒë§ì´ í¬í•¨ëœ ë¬¸ì¥ì€ ìƒˆ ë‹¨ë½
            if any(closing in sentence for closing in ['ê°ì‚¬í•©ë‹ˆë‹¤', 'ê°ì‚¬ë“œë¦½ë‹ˆë‹¤', 'í‰ì•ˆí•˜ì„¸ìš”', 'ì£¼ë‹˜ ì•ˆì—ì„œ']):
                should_break = True
            
            # ë¬¸ì¥ ê¸¸ì´ê°€ 50ì ì´ìƒì´ê³  í˜„ì¬ ë‹¨ë½ì´ ìˆìœ¼ë©´ ìƒˆ ë‹¨ë½
            if len(sentence) > 50 and current_paragraph:
                should_break = True
            
            # ìƒˆ ë‹¨ë½ ë¶„ë¦¬
            if should_break and current_paragraph:
                paragraphs.append(' '.join(current_paragraph))
                current_paragraph = [sentence]
            else:
                current_paragraph.append(sentence)
        
        # ë§ˆì§€ë§‰ ë‹¨ë½ ì¶”ê°€
        if current_paragraph:
            paragraphs.append(' '.join(current_paragraph))
        
        # Quill ì—ë””í„° í˜¸í™˜ì„ ìœ„í•œ HTML ë‹¨ë½ìœ¼ë¡œ ë³€í™˜
        # ê° ë‹¨ë½ì„ <p> íƒœê·¸ë¡œ ê°ì‹¸ê³ , ë‹¨ë½ ì‚¬ì´ì— <p><br></p> íƒœê·¸ë¡œ ë¹ˆ ì¤„ ì¶”ê°€
        html_paragraphs = []
        for i, paragraph in enumerate(paragraphs):
            html_paragraphs.append(f"<p>{paragraph}</p>")
            
            # ë‹¨ë½ ì‚¬ì´ì— ë¹ˆ ì¤„ ì¶”ê°€ (ë§ˆì§€ë§‰ ë‹¨ë½ ì œì™¸)
            if i < len(paragraphs) - 1:
                html_paragraphs.append("<p><br></p>")
        
        return ''.join(html_paragraphs)

    # â˜† ë‹µë³€ í…ìŠ¤íŠ¸ë¥¼ ì •ë¦¬í•˜ê³  í¬ë§·íŒ…í•˜ëŠ” ë©”ì„œë“œ (Quill ì—ë””í„°ìš©)
    def clean_answer_text(self, text: str) -> str:
        if not text:
            return ""
        
        # ì œì–´ ë¬¸ìë§Œ ì œê±°í•˜ê³  HTML íƒœê·¸ëŠ” ìœ ì§€
        text = re.sub(r'[\b\r\f\v]', '', text)
        text = re.sub(r'[\x00-\x08\x0B\x0C\x0E-\x1F\x7F]', '', text)

        # HTML íƒœê·¸ ì œê±°í•˜ì§€ ì•ŠìŒ (Quill ì—ë””í„°ìš©)
        text = re.sub(r'\*\*([^*]+)\*\*', r'\1', text)
        text = re.sub(r'\*([^*]+)\*', r'\1', text)
        
        # HTML íƒœê·¸ ë‚´ë¶€ì˜ ê³µë°±ë§Œ ì •ë¦¬ (íƒœê·¸ ìì²´ëŠ” ìœ ì§€)
        text = re.sub(r'>\s+<', '><', text)  # íƒœê·¸ ì‚¬ì´ ê³µë°± ì œê±°
        text = re.sub(r'<p>\s+', '<p>', text)  # <p> íƒœê·¸ ë‚´ë¶€ ì• ê³µë°± ì œê±°
        text = re.sub(r'\s+</p>', '</p>', text)  # </p> íƒœê·¸ ì• ê³µë°± ì œê±°
        
        text = self.remove_old_app_name(text)
        text = self.format_answer_with_html_paragraphs(text)
        
        return text

    # â˜† í…ìŠ¤íŠ¸ ìœ íš¨ì„± ê²€ì¦ ë©”ì„œë“œ
    def is_valid_text(self, text: str, lang: str = 'ko') -> bool:
        if not text or len(text.strip()) < 3:
            return False
        
        if lang == 'ko':
            return self.is_valid_korean_text(text)
        else:  # ì˜ì–´
            return self.is_valid_english_text(text)

    # â˜† í•œêµ­ì–´ í…ìŠ¤íŠ¸ì˜ ìœ íš¨ì„±ì„ ê²€ì¦í•˜ëŠ” ë©”ì„œë“œ
    def is_valid_korean_text(self, text: str) -> bool:
        if not text or len(text.strip()) < 3:
            logging.info(f"í•œêµ­ì–´ ê²€ì¦ ì‹¤íŒ¨: í…ìŠ¤íŠ¸ê°€ ë„ˆë¬´ ì§§ìŒ (ê¸¸ì´: {len(text.strip()) if text else 0})")
            return False
        
        korean_chars = len(re.findall(r'[ê°€-í£]', text))
        total_chars = len(re.sub(r'\s', '', text))
        
        if total_chars == 0:
            logging.info("í•œêµ­ì–´ ê²€ì¦ ì‹¤íŒ¨: ì´ ê¸€ì ìˆ˜ê°€ 0")
            return False
            
        korean_ratio = korean_chars / total_chars
        logging.info(f"í•œêµ­ì–´ ë¹„ìœ¨: {korean_ratio:.3f} (í•œêµ­ì–´: {korean_chars}, ì „ì²´: {total_chars})")
        
        # í•œêµ­ì–´ ë¹„ìœ¨ ê¸°ì¤€ì„ ì™„í™” (0.2 â†’ 0.1)
        if korean_ratio < 0.1:
            logging.info(f"í•œêµ­ì–´ ê²€ì¦ ì‹¤íŒ¨: í•œêµ­ì–´ ë¹„ìœ¨ ë¶€ì¡± ({korean_ratio:.3f} < 0.1)")
            return False
        
        # ë¬´ì˜ë¯¸í•œ íŒ¨í„´ ê°ì§€ (GPT í• ë£¨ì‹œë„¤ì´ì…˜ ë°©ì§€)
        meaningless_patterns = [
            r'^[a-z\s\.,;:\(\)\[\]\-_&\/\'"]+$', # ì˜ì–´
            r'^[A-Z\s\.,;:\(\)\[\]\-_&\/\'"]+$', # ì˜ì–´ ëŒ€ë¬¸ì
            r'^[\s\.,;:\(\)\[\]\-_&\/\'"]+$',    # ê³µë°±
            r'^[0-9\s\.,;:\(\)\[\]\-_&\/\'"]+$', # ìˆ«ì
            r'.*[Ğ°-Ñ].*',                        # ëŸ¬ì‹œì•„ì–´
            r'.*[Î±-Ï‰].*',                        # ê·¸ë¦¬ìŠ¤ì–´
        ]
        
        for pattern in meaningless_patterns:
            if re.match(pattern, text, re.IGNORECASE):
                logging.info(f"í•œêµ­ì–´ ê²€ì¦ ì‹¤íŒ¨: ë¬´ì˜ë¯¸í•œ íŒ¨í„´ ê°ì§€")
                return False
        
        # ë°˜ë³µ ë¬¸ì ê°ì§€: ê°™ì€ ë¬¸ìê°€ 5ë²ˆ ì´ìƒ ì—°ì†ìœ¼ë¡œ ë‚˜íƒ€ë‚˜ë©´ ë¹„ì •ìƒ í…ìŠ¤íŠ¸ë¡œ ê°„ì£¼
        if re.search(r'(.)\1{5,}', text):
            logging.info("í•œêµ­ì–´ ê²€ì¦ ì‹¤íŒ¨: ë°˜ë³µ ë¬¸ì ê°ì§€")
            return False
        
        # ì˜ì–´ ë¹„ìœ¨ ê²€ì‚¬ë¥¼ ì™„í™” (0.5 â†’ 0.7)
        random_pattern = r'[a-zA-Z]{8,}'
        if re.search(random_pattern, text) and korean_ratio < 0.3:
            logging.info(f"í•œêµ­ì–´ ê²€ì¦ ì‹¤íŒ¨: ê¸´ ì˜ì–´ ë‹¨ì–´ì™€ ë‚®ì€ í•œêµ­ì–´ ë¹„ìœ¨")
            return False
        
        logging.info("í•œêµ­ì–´ ê²€ì¦ ì„±ê³µ")
        return True

    # â˜† ì˜ì–´ í…ìŠ¤íŠ¸ì˜ ìœ íš¨ì„±ì„ ê²€ì¦í•˜ëŠ” ë©”ì„œë“œ
    def is_valid_english_text(self, text: str) -> bool:
        if not text or len(text.strip()) < 3:
            return False
        
        english_chars = len(re.findall(r'[a-zA-Z]', text))
        total_chars = len(re.sub(r'\s', '', text))
        
        if total_chars == 0:
            return False
            
        english_ratio = english_chars / total_chars
        
        if english_ratio < 0.7:  # ì˜ì–´ ë¹„ìœ¨ì´ 70% ë¯¸ë§Œì´ë©´ ë¬´íš¨
            return False
        
        # ë°˜ë³µ ë¬¸ì ê°ì§€
        if re.search(r'(.)\1{5,}', text):
            return False
        
        return True

    # â˜† ìƒì„±ëœ í…ìŠ¤íŠ¸ë¥¼ ì •ë¦¬í•˜ê³  ê²€ì¦í•˜ëŠ” ë©”ì„œë“œ
    def clean_generated_text(self, text: str) -> str:
        if not text:
            return ""
        # ì œì–´ ë¬¸ì ì œê±°
        text = re.sub(r'[\x00-\x08\x0B\x0C\x0E-\x1F\x7F]', '', text)
        text = re.sub(r'[\b\r\f\v]', '', text)

        # ì˜ì–´ ì•½ì–´ ì œê±°
        text = re.sub(r'\b[a-z]{1,2}\b(?:\s+[a-z]{1,2}\b)*', '', text, flags=re.IGNORECASE)
        text = re.sub(r'[Ğ°-Ñ]+', '', text)
        text = re.sub(r'[Î±-Ï‰]+', '', text)

        # í•œê¸€ ë¬¸ì ì œê±°
        text = re.sub(r'[^\w\sê°€-í£.,!?()"\'-]{3,}', '', text)
        text = re.sub(r'[.,;:!?]{3,}', '.', text)

        # ê³µë°± ì •ë¦¬
        text = re.sub(r'\s+', ' ', text)
        text = text.strip()
        
        return text

    # â˜† í†µì¼ëœ GPT í”„ë¡¬í”„íŠ¸ ìƒì„± ë©”ì„œë“œ (ëª¨ë“ˆí™”)
    def get_gpt_prompts(self, query: str, context: str, lang: str = 'ko') -> tuple:
        """ì–¸ì–´ë³„ GPT í”„ë¡¬í”„íŠ¸ ìƒì„±"""
        if lang == 'en': # ì˜ì–´
            system_prompt = """You are a GOODTV Bible App customer service representative.

Guidelines:
1. Follow the style and content of the provided reference answers faithfully
2. Find and apply solutions from similar situations in the reference answers
3. Adapt to the customer's specific situation while maintaining the tone and style of the reference answers

âš ï¸ Absolute Prohibitions:
- Do not guide non-existent features or menus
- Do not create specific settings methods or button locations
- If a feature is not in the reference answers, say "Sorry, this feature is currently not available"
- If uncertain, respond with "We will review this internally"

4. For feature requests or improvement suggestions, use:
   - "Thank you for your valuable feedback"
   - "We will discuss/review this internally"
   - "We will forward this as an improvement"

5. Address customers as 'Dear user' or similar polite forms
6. Use 'GOODTV Bible App' or 'Bible App' as the app name

ğŸš« Do NOT generate greetings or closings:
- Do not use "Hello", "Thank you", "Best regards", etc.
- Do not use "God bless", "In Christ", etc.
- Only write the main content

7. Do not use HTML tags, write in natural sentences"""

            user_prompt = f"""Customer inquiry: {query}

Reference answers (main content only, greetings and closings removed):
{context}

Based on the reference answers' solution methods and tone, write a specific answer to the customer's problem.
Important: Do not include greetings or closings. Only write the main content."""

        else:  # í•œêµ­ì–´
            system_prompt = """ë‹¹ì‹ ì€ GOODTV ë°”ì´ë¸” ì• í”Œ ê³ ê°ì„¼í„° ìƒë‹´ì›ì…ë‹ˆë‹¤.

ğŸ¯ í•µì‹¬ ì›ì¹™:
1. ê³ ê°ì˜ ì§ˆë¬¸ì„ ì •í™•íˆ ì´í•´í•˜ê³  ì§ˆë¬¸ ë‚´ìš©ì— ë§ëŠ” ë‹µë³€ë§Œ ì œê³µí•˜ì„¸ìš”
2. ì§ˆë¬¸ê³¼ ê´€ë ¨ ì—†ëŠ” ë‚´ìš©ì˜ ì°¸ê³  ë‹µë³€ì€ ë¬´ì‹œí•˜ê³ , ì§ˆë¬¸ì˜ ë³¸ì§ˆì— ì§‘ì¤‘í•˜ì„¸ìš”
3. ì°¸ê³  ë‹µë³€ì´ ì§ˆë¬¸ê³¼ ë§ì§€ ì•Šìœ¼ë©´ ìƒˆë¡œìš´ ì ì ˆí•œ ë‹µë³€ì„ ìƒì„±í•˜ì„¸ìš”

ğŸ“‹ ì§ˆë¬¸ ìœ í˜•ë³„ ëŒ€ì‘ ë°©ë²•:

ğŸ”¤ ì˜¤íƒˆì/ì˜¤ë¥˜ ë¬¸ì˜:
- ì˜¤íƒˆì ì œë³´ ê°ì‚¬ í‘œí˜„
- ì„±ì„œê³µíšŒ ì›ë¬¸ í™•ì¸ í›„ ìˆ˜ì • ì•ˆë‚´
- ì•± ì—…ë°ì´íŠ¸ë¥¼ í†µí•œ ë°˜ì˜ ì¼ì • ì•ˆë‚´

âš™ï¸ ê¸°ëŠ¥ ë¬¸ì˜:
- í•´ë‹¹ ê¸°ëŠ¥ì˜ ì‚¬ìš©ë²•ì„ êµ¬ì²´ì ìœ¼ë¡œ ì•ˆë‚´
- ë²„íŠ¼ ìœ„ì¹˜, ë©”ë‰´ ê²½ë¡œë¥¼ ëª…í™•íˆ ì„¤ëª…
- ê¸°ëŠ¥ì´ ì—†ë‹¤ë©´ "í˜„ì¬ ì œê³µë˜ì§€ ì•ŠëŠ” ê¸°ëŠ¥"ì´ë¼ê³  ëª…ì‹œ

ğŸ“± í™”ë©´ í‘œì‹œ ë¬¸ì œ:
- êµ¬ì²´ì ì¸ ìƒí™© íŒŒì•…ì„ ìœ„í•œ ì¶”ê°€ ì§ˆë¬¸
- ìŠ¤í¬ë¦°ìƒ· ìš”ì²­
- ì„ì‹œ í•´ê²°ë°©ë²• ì œì‹œ

ğŸ“š ì„±ê²½ ê²€ìƒ‰/ë²ˆì—­ë³¸:
- ê¸°ì¡´ ì§€ì› ê¸°ëŠ¥ ì •í™•íˆ ì•ˆë‚´
- ì§€ì›ë˜ëŠ” ë²ˆì—­ë³¸ ëª©ë¡ ì œì‹œ
- ë™ì‹œ ë¹„êµ ê¸°ëŠ¥ ë“± ì‹¤ì œ ê°€ëŠ¥í•œ ê¸°ëŠ¥ ì•ˆë‚´

âš ï¸ ì ˆëŒ€ ê¸ˆì§€ì‚¬í•­:
- ì§ˆë¬¸ê³¼ ì „í˜€ ë‹¤ë¥¸ ë‚´ìš©ì˜ ë‹µë³€ ê¸ˆì§€
- ì˜¤íƒˆì ë¬¸ì˜ì— í°íŠ¸ ì¡°ì ˆ ë‹µë³€ ê¸ˆì§€
- ê¸°ëŠ¥ ë¬¸ì˜ì— ì˜¤íƒˆì ê´€ë ¨ ë‹µë³€ ê¸ˆì§€
- êµ¬ì²´ì  ì§ˆë¬¸ì— "ë‚´ë¶€ ê²€í† " ë“± íšŒí”¼ì„± ë‹µë³€ ì§€ì–‘

ğŸš« ì¸ì‚¬ë§ ë° ëë§ºìŒë§ ìƒì„± ê¸ˆì§€:
- "ì•ˆë…•í•˜ì„¸ìš”", "ê°ì‚¬í•©ë‹ˆë‹¤", "í‰ì•ˆí•˜ì„¸ìš”" ë“±ì˜ ì¸ì‚¬ë§ì„ ì ˆëŒ€ ì‚¬ìš©í•˜ì§€ ë§ˆì„¸ìš”
- "ì£¼ë‹˜ ì•ˆì—ì„œ", "ê¸°ë„ë“œë¦¬ê² ìŠµë‹ˆë‹¤" ë“±ì˜ ëë§ºìŒë§ì„ ì ˆëŒ€ ì‚¬ìš©í•˜ì§€ ë§ˆì„¸ìš”
- ì˜¤ì§ ë³¸ë¬¸ ë‚´ìš©ë§Œ ì‘ì„±í•˜ì„¸ìš”

ğŸ’¡ ì°½ì˜ì  ì‚¬ê³ :
- ì°¸ê³  ë‹µë³€ì´ ë¶€ì ì ˆí•˜ë©´ ê³ ê° ì§ˆë¬¸ì— ë§ëŠ” ìƒˆë¡œìš´ ë‹µë³€ì„ ì°½ì˜ì ìœ¼ë¡œ ìƒì„±í•˜ì„¸ìš”
- ë°”ì´ë¸” ì• í”Œì˜ ì‹¤ì œ ê¸°ëŠ¥ê³¼ ì •ì±…ì— ë§ëŠ” í˜„ì‹¤ì ì¸ ë‹µë³€ì„ ì œê³µí•˜ì„¸ìš”"""

            user_prompt = f"""ê³ ê° ë¬¸ì˜: {query}

ì°¸ê³  ë‹µë³€ë“¤:
{context}

â— ì¤‘ìš” ì§€ì‹œì‚¬í•­:
ìœ„ ì°¸ê³  ë‹µë³€ë“¤ì´ ê³ ê°ì˜ ì§ˆë¬¸ê³¼ ê´€ë ¨ì´ ìˆëŠ”ì§€ ë¨¼ì € íŒë‹¨í•˜ì„¸ìš”.
- ê´€ë ¨ì´ ìˆë‹¤ë©´: ì°¸ê³  ë‹µë³€ì˜ í•´ê²° ë°©ì‹ì„ í™œìš©í•˜ì—¬ ë‹µë³€í•˜ì„¸ìš”
- ê´€ë ¨ì´ ì—†ë‹¤ë©´: ì°¸ê³  ë‹µë³€ì„ ì°¸ê³ í•˜ì—¬ ìƒˆë¡œìš´ ë‹µë³€ì„ ì‘ì„±í•˜ì„¸ìš”

ê³ ê°ì˜ êµ¬ì²´ì ì¸ ì§ˆë¬¸ì— ì •í™•íˆ ë§ëŠ” ë‹µë³€ë§Œ ì‘ì„±í•˜ì„¸ìš”.
ì¸ì‚¬ë§ì´ë‚˜ ëë§ºìŒë§ ì—†ì´ ë³¸ë¬¸ ë‚´ìš©ë§Œ ì‘ì„±í•˜ì„¸ìš”."""

        return system_prompt, user_prompt

    # â˜† í–¥ìƒëœ GPT ìƒì„± - í†µì¼ëœ í”„ë¡¬í”„íŠ¸ ì‚¬ìš©
    def generate_with_enhanced_gpt(self, query: str, similar_answers: list, context_analysis: dict, lang: str = 'ko') -> str:
        try:
            with memory_cleanup():
                approach = context_analysis['recommended_approach']
                context = self.create_enhanced_context(similar_answers, target_lang=lang)
                
                if not context:
                    logging.warning("ìœ íš¨í•œ ì»¨í…ìŠ¤íŠ¸ê°€ ì—†ì–´ GPT ìƒì„± ì¤‘ë‹¨")
                    return ""
                
                # í†µì¼ëœ í”„ë¡¬í”„íŠ¸ ìƒì„±
                system_prompt, user_prompt = self.get_gpt_prompts(query, context, lang)
                
                # ğŸ”¥ ì ‘ê·¼ ë°©ì‹ë³„ temperatureì™€ max_tokens ì„¤ì • ê°œì„ 
                if approach == 'gpt_with_strong_context':
                    # ê´€ë ¨ì„±ì´ ë†’ì€ ê²½ìš° ë” ì°½ì˜ì ìœ¼ë¡œ ì„¤ì •
                    temperature = 0.7 if context_analysis.get('context_relevance') == 'high' else 0.6
                    max_tokens = 700
                elif approach == 'gpt_with_weak_context':
                    # ê´€ë ¨ì„±ì´ ë‚®ì€ ê²½ìš° ë” ì°½ì˜ì ìœ¼ë¡œ ì„¤ì •í•˜ì—¬ ìƒˆë¡œìš´ ë‹µë³€ ìƒì„± ìœ ë„
                    temperature = 0.8
                    max_tokens = 650
                else: # fallbackì´ë‚˜ ê¸°íƒ€
                    return ""
                
                # GPT API í˜¸ì¶œ
                response = self.openai_client.chat.completions.create(
                    model=GPT_MODEL,
                    messages=[
                        {"role": "system", "content": system_prompt},
                        {"role": "user", "content": user_prompt}
                    ],
                    max_tokens=max_tokens,
                    temperature=temperature,
                    top_p=0.8,
                    frequency_penalty=0.1,
                    presence_penalty=0.1
                )
                
                generated = response.choices[0].message.content.strip()
                del response
                
                # ìƒì„±ëœ í…ìŠ¤íŠ¸ ì •ë¦¬
                generated = self.clean_generated_text(generated)
                
                # ğŸ”¥ ë‹µë³€ ê´€ë ¨ì„± ì¶”ê°€ ê²€ì¦ (AI ê¸°ë°˜)
                if not self.validate_answer_relevance_ai(generated, query, context_analysis.get('question_analysis', {})):
                    logging.warning(f"ìƒì„±ëœ ë‹µë³€ì´ ì§ˆë¬¸ê³¼ ê´€ë ¨ì„±ì´ ë‚®ìŒ: {generated[:50]}...")
                    return ""
                
                # í…ìŠ¤íŠ¸ ìœ íš¨ì„± ê²€ì¦ (ì™„í™”)
                if not self.is_valid_text(generated, lang):
                    logging.warning(f"GPTê°€ ë¬´íš¨í•œ í…ìŠ¤íŠ¸ ìƒì„±: {generated[:50]}...")
                    # ìœ íš¨ì„± ê²€ì¦ ì‹¤íŒ¨í•´ë„ ê´€ë ¨ì„±ì´ ë†’ìœ¼ë©´ ì‚¬ìš©
                    if context_analysis.get('context_relevance') == 'high':
                        logging.info("ê´€ë ¨ì„±ì´ ë†’ì•„ ìœ íš¨ì„± ê²€ì¦ ìš°íšŒ")
                    else:
                        return ""
                
                logging.info(f"GPT ìƒì„± ì„±ê³µ ({approach}, ì–¸ì–´: {lang}): {len(generated)}ì")
                return generated[:650]
                
        except Exception as e:
            logging.error(f"í–¥ìƒëœ GPT ìƒì„± ì‹¤íŒ¨: {e}")
            return ""

    # â˜† AI ê¸°ë°˜ ë‹µë³€ ê´€ë ¨ì„± ê²€ì¦ ë©”ì„œë“œ (ê°œì„ ëœ ë²„ì „)
    def validate_answer_relevance_ai(self, answer: str, query: str, question_analysis: dict) -> bool:
        """AIë¥¼ ì´ìš©í•´ ìƒì„±ëœ ë‹µë³€ì´ ì§ˆë¬¸ê³¼ ê´€ë ¨ì„±ì´ ìˆëŠ”ì§€ ê²€ì¦"""
        
        try:
            with memory_cleanup():
                system_prompt = """ë‹¹ì‹ ì€ ë‹µë³€ í’ˆì§ˆ ê²€ì¦ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
ìƒì„±ëœ ë‹µë³€ì´ ê³ ê°ì˜ ì§ˆë¬¸ì— ì ì ˆíˆ ëŒ€ì‘í•˜ëŠ”ì§€ í‰ê°€í•˜ì„¸ìš”.

í‰ê°€ ê¸°ì¤€:
1. ë‹µë³€ì´ ì§ˆë¬¸ì˜ í•µì‹¬ ë‚´ìš©ì„ ë‹¤ë£¨ê³  ìˆëŠ”ê°€?
2. ë‹µë³€ì´ ê³ ê°ì˜ ë¬¸ì œë¥¼ í•´ê²°í•˜ëŠ”ë° ë„ì›€ì´ ë˜ëŠ”ê°€?
3. ë‹µë³€ê³¼ ì§ˆë¬¸ì˜ ì£¼ì œê°€ ì¼ì¹˜í•˜ëŠ”ê°€?

ê²°ê³¼: "relevant" ë˜ëŠ” "irrelevant" ì¤‘ í•˜ë‚˜ë§Œ ë°˜í™˜í•˜ì„¸ìš”."""

                user_prompt = f"""ì§ˆë¬¸ ë¶„ì„:
ì˜ë„: {question_analysis.get('intent_type', 'N/A')}
ì£¼ì œ: {question_analysis.get('main_topic', 'N/A')}
ìš”ì²­ì‚¬í•­: {question_analysis.get('specific_request', 'N/A')}

ì›ë³¸ ì§ˆë¬¸: {query}

ìƒì„±ëœ ë‹µë³€: {answer}

ì´ ë‹µë³€ì´ ì§ˆë¬¸ì— ì ì ˆí•œì§€ í‰ê°€í•´ì£¼ì„¸ìš”."""

                response = self.openai_client.chat.completions.create(
                    model='gpt-3.5-turbo',
                    messages=[
                        {"role": "system", "content": system_prompt},
                        {"role": "user", "content": user_prompt}
                    ],
                    max_tokens=30,
                    temperature=0.1
                )
                
                result = response.choices[0].message.content.strip().lower()
                
                is_relevant = 'relevant' in result and 'irrelevant' not in result
                
                logging.info(f"AI ë‹µë³€ ê´€ë ¨ì„± ê²€ì¦: {result} -> {is_relevant}")
                
                return is_relevant
                
        except Exception as e:
            logging.error(f"AI ë‹µë³€ ê´€ë ¨ì„± ê²€ì¦ ì‹¤íŒ¨: {e}")
            # í´ë°±: ê¸°ë³¸ì ì¸ í‚¤ì›Œë“œ ë§¤ì¹­
            query_keywords = set(self.extract_keywords(query.lower()))
            answer_keywords = set(self.extract_keywords(answer.lower()))
            
            keyword_overlap = len(query_keywords & answer_keywords)
            keyword_relevance = keyword_overlap / max(len(query_keywords), 1)
            
            return keyword_relevance >= 0.2  # 20% ì´ìƒ í‚¤ì›Œë“œ ì¼ì¹˜ì‹œ ê´€ë ¨ì„± ìˆìŒìœ¼ë¡œ íŒë‹¨

    # â˜† ìµœì ì˜ í´ë°± ë‹µë³€ ì„ íƒ ë©”ì„œë“œ (ì§ì ‘ ì‚¬ìš© ë‹µë³€ í¬í•¨)
    def get_best_fallback_answer(self, similar_answers: list, lang: str = 'ko') -> str:
        logging.info(f"=== get_best_fallback_answer ì‹œì‘ ===")
        logging.info(f"ì…ë ¥ëœ similar_answers ê°œìˆ˜: {len(similar_answers)}")
        
        if not similar_answers:
            logging.warning("similar_answersê°€ ë¹„ì–´ìˆìŒ")
            return ""
        
        # ì…ë ¥ëœ ë‹µë³€ë“¤ ë¯¸ë¦¬ë³´ê¸°
        for i, ans in enumerate(similar_answers[:3]):
            logging.info(f"ë‹µë³€ #{i+1}: ì ìˆ˜={ans['score']:.3f}, ê¸¸ì´={len(ans.get('answer', ''))}, ë‚´ìš©ë¯¸ë¦¬ë³´ê¸°={ans.get('answer', '')[:50]}...")
        
        # ì ìˆ˜ì™€ í…ìŠ¤íŠ¸ í’ˆì§ˆì„ ì¢…í•© í‰ê°€
        best_answer = ""
        best_score = 0
        
        for i, ans in enumerate(similar_answers[:3]): # ìƒìœ„ 3ê°œë§Œ ê²€í†  (í’ˆì§ˆ í–¥ìƒ)
            logging.info(f"--- ë‹µë³€ #{i+1} ì²˜ë¦¬ ì‹œì‘ ---")
            score = ans['score']
            answer_text = ans['answer']  # ì›ë³¸ ë‹µë³€ í…ìŠ¤íŠ¸ ì‚¬ìš©
            logging.info(f"ì›ë³¸ ë‹µë³€ ê¸¸ì´: {len(answer_text)}, ë‚´ìš©: {answer_text[:100]}...")
            
            # ğŸ”¥ ê¸´ê¸‰ ìˆ˜ì •: 0.9 ì´ìƒ ì ìˆ˜ë©´ ì „ì²˜ë¦¬ ì—†ì´ ë°”ë¡œ ë°˜í™˜
            if score >= 0.9:
                logging.info(f"ğŸ”¥ ë§¤ìš° ë†’ì€ ìœ ì‚¬ë„({score:.3f}) - ì „ì²˜ë¦¬ ì—†ì´ ì›ë³¸ ë‹µë³€ ë°”ë¡œ ë°˜í™˜")
                print(f"ğŸ”¥ ë§¤ìš° ë†’ì€ ìœ ì‚¬ë„({score:.3f}) - ì „ì²˜ë¦¬ ì—†ì´ ì›ë³¸ ë‹µë³€ ë°”ë¡œ ë°˜í™˜")
                # ìµœì†Œí•œì˜ ì •ë¦¬ë§Œ
                clean_answer = answer_text.strip()
                if clean_answer:
                    logging.info(f"ğŸ”¥ ì›ë³¸ ë‹µë³€ ì§ì ‘ ë°˜í™˜: ê¸¸ì´={len(clean_answer)}")
                    return clean_answer
            
            # ê¸°ë³¸ ì •ë¦¬ë§Œ ìˆ˜í–‰
            answer_text = self.preprocess_text(answer_text)
            logging.info(f"ì „ì²˜ë¦¬ í›„ ê¸¸ì´: {len(answer_text)}, ë‚´ìš©: {answer_text[:100]}...")
            
            # ì˜ì–´ ì§ˆë¬¸ì¸ ê²½ìš° ë‹µë³€ì„ ë²ˆì—­
            if lang == 'en' and ans.get('lang', 'ko') == 'ko':
                answer_text = self.translate_text(answer_text, 'ko', 'en')
                logging.info(f"ë²ˆì—­ í›„ ê¸¸ì´: {len(answer_text)}")
            
            # ìœ íš¨ì„± ê²€ì‚¬ (ì„ì‹œë¡œ ìš°íšŒ - ë””ë²„ê¹…ìš©)
            is_valid = self.is_valid_text(answer_text, lang)
            logging.info(f"ìœ íš¨ì„± ê²€ì‚¬ ê²°ê³¼: {is_valid}")
            
            # ì„ì‹œë¡œ ìœ íš¨ì„± ê²€ì‚¬ ë¬´ì‹œí•˜ê³  ì§„í–‰
            if not is_valid:
                logging.warning(f"âš ï¸ ë‹µë³€ #{i+1} ìœ íš¨ì„± ê²€ì‚¬ ì‹¤íŒ¨í–ˆì§€ë§Œ ê°•ì œë¡œ ì§„í–‰")
                # continueë¥¼ ì£¼ì„ ì²˜ë¦¬í•˜ì—¬ ìœ íš¨ì„± ê²€ì‚¬ ì‹¤íŒ¨í•´ë„ ê³„ì† ì§„í–‰
            
            # ë†’ì€ ìœ ì‚¬ë„(0.8+)ì¸ ê²½ìš° ê°„ë‹¨í•˜ê²Œ ì²« ë²ˆì§¸ ë‹µë³€ ì„ íƒ
            if score >= 0.8:
                logging.info(f"ë†’ì€ ìœ ì‚¬ë„({score:.3f})ë¡œ ë‹µë³€ #{i+1} ì§ì ‘ ì„ íƒ")
                logging.info(f"ì„ íƒëœ ë‹µë³€ ìµœì¢… ê¸¸ì´: {len(answer_text)}")
                # ğŸ”¥ ìœ íš¨ì„± ê²€ì‚¬ ì‹¤íŒ¨í•´ë„ ê°•ì œë¡œ ë°˜í™˜
                if answer_text and len(answer_text.strip()) > 0:
                    return answer_text
                else:
                    logging.error(f"ğŸ”¥ ì „ì²˜ë¦¬ í›„ ë‹µë³€ì´ ë¹„ì–´ìˆìŒ! ì›ë³¸ìœ¼ë¡œ í´ë°±")
                    return ans['answer'].strip()
            
            # ì¢…í•© ì ìˆ˜ ê³„ì‚° (ìœ ì‚¬ë„ + í…ìŠ¤íŠ¸ ê¸¸ì´ + ì™„ì„±ë„)
            length_score = min(len(answer_text) / 200, 1.0) # 200ì ê¸°ì¤€ ì •ê·œí™”
            completeness_score = 1.0 if answer_text.endswith(('.', '!', '?')) else 0.8
            
            total_score = score * 0.8 + length_score * 0.1 + completeness_score * 0.1  # ìœ ì‚¬ë„ ê°€ì¤‘ì¹˜ ì¦ê°€
            
            logging.info(f"ë‹µë³€ #{i+1} ì¢…í•© ì ìˆ˜: {total_score:.3f} (ìœ ì‚¬ë„:{score:.3f}, ê¸¸ì´:{length_score:.3f}, ì™„ì„±ë„:{completeness_score:.3f})")
            
            if total_score > best_score:
                best_score = total_score
                best_answer = answer_text
                logging.info(f"ìƒˆë¡œìš´ ìµœê³  ì ìˆ˜ ë‹µë³€ìœ¼ë¡œ ì„ íƒë¨")
        
        logging.info(f"=== get_best_fallback_answer ì™„ë£Œ ===")
        logging.info(f"ìµœì¢… ì„ íƒëœ ë‹µë³€ ì ìˆ˜: {best_score:.3f}")
        logging.info(f"ìµœì¢… ë‹µë³€ ê¸¸ì´: {len(best_answer)}")
        logging.info(f"ìµœì¢… ë‹µë³€ ë¯¸ë¦¬ë³´ê¸°: {best_answer[:100] if best_answer else 'None'}...")
        
        # ğŸ”¥ ê¸´ê¸‰ ì•ˆì „ì¥ì¹˜: ë‹µë³€ì´ ë¹„ì–´ìˆìœ¼ë©´ ì²« ë²ˆì§¸ ì›ë³¸ ë‹µë³€ ê°•ì œ ë°˜í™˜
        if not best_answer and similar_answers:
            logging.error("ğŸš¨ ìµœì¢… ë‹µë³€ì´ ë¹„ì–´ìˆìŒ! ì²« ë²ˆì§¸ ì›ë³¸ ë‹µë³€ ê°•ì œ ë°˜í™˜")
            print("ğŸš¨ ìµœì¢… ë‹µë³€ì´ ë¹„ì–´ìˆìŒ! ì²« ë²ˆì§¸ ì›ë³¸ ë‹µë³€ ê°•ì œ ë°˜í™˜")
            emergency_answer = similar_answers[0]['answer'].strip()
            logging.info(f"ğŸš¨ ê¸´ê¸‰ ë‹µë³€ ê¸¸ì´: {len(emergency_answer)}")
            return emergency_answer
        
        return best_answer

    # â˜† ë” ë³´ìˆ˜ì ì¸ GPT-3.5-turbo ìƒì„± ë©”ì„œë“œ (ê¸°ì¡´ ì½”ë“œì™€ì˜ í˜¸í™˜ì„± ìœ ì§€)
    # ë³´ìˆ˜ì ì´ê³  ì°¸ê³  ë‹µë³€ì— ì¶©ì‹¤í•œ GPT-3.5-turbo í…ìŠ¤íŠ¸ ìƒì„±
    @profile
    def generate_ai_answer(self, query: str, similar_answers: list, lang: str) -> str:
        
        # 1. ì–¸ì–´ ê°ì§€ (lang íŒŒë¼ë¯¸í„°ê°€ ì—†ê±°ë‚˜ 'auto'ì¸ ê²½ìš°)
        if not lang or lang == 'auto':
            detected_lang = self.detect_language(query)
            lang = detected_lang
            logging.info(f"ê°ì§€ëœ ì–¸ì–´: {lang}")
        
        # 2. ìœ ì‚¬ ë‹µë³€ì´ ì—†ëŠ” ê²½ìš°
        if not similar_answers:
            logging.error("ğŸš¨ ìœ ì‚¬ ë‹µë³€ì´ ì „í˜€ ì—†ìŒ - Pinecone ê²€ìƒ‰ ì‹¤íŒ¨ ë˜ëŠ” ë°ì´í„° ì—†ìŒ")
            print(f"ğŸš¨ CRITICAL: ìœ ì‚¬ ë‹µë³€ì´ ì „í˜€ ì—†ìŒ! query='{query[:50]}...', lang='{lang}'")
            if lang == 'en':
                default_msg = "<p>We need more detailed information to provide an accurate answer to your inquiry.</p><p><br></p><p>Please contact our customer service center for prompt assistance.</p>"
            else:
                default_msg = "<p>ì•ˆë…•í•˜ì„¸ìš”, GOODTV ë°”ì´ë¸” ì• í”Œì…ë‹ˆë‹¤.</p><p><br></p><p>ë°”ì´ë¸” ì• í”Œì„ ì´ìš©í•´ ì£¼ì…”ì„œ ì§„ì‹¬ìœ¼ë¡œ ê°ì‚¬ë“œë¦½ë‹ˆë‹¤.</p><p><br></p><p>ë‚¨ê²¨ì£¼ì‹  ë¬¸ì˜ëŠ” í˜„ì¬ ë‹´ë‹¹ìê°€ ì§ì ‘ í™•ì¸í•˜ê³  ìˆìŠµë‹ˆë‹¤.</p><p><br></p><p>ì„±ë„ë‹˜ê»˜ ë„ì›€ì´ ë  ìˆ˜ ìˆë„ë¡ ë‚´ìš©ì„ ê¼¼ê¼¼íˆ ì‚´í´</p><p><br></p><p>ì •í™•í•˜ê³  êµ¬ì²´ì ì¸ ë‹µë³€ì„ ì¤€ë¹„í•˜ê² ìŠµë‹ˆë‹¤.</p><p><br></p><p>ë‹µë³€ì€ ìµœëŒ€ í•˜ë£¨ ì´ë‚´ì— ë“œë¦´ ì˜ˆì •ì´ì˜¤ë‹ˆ</p><p><br></p><p>ì¡°ê¸ˆë§Œ ê¸°ë‹¤ë ¤ ì£¼ì‹œë©´ ê°ì‚¬í•˜ê² ìŠµë‹ˆë‹¤.</p><p><br></p><p>í•­ìƒ ì£¼ë‹˜ ì•ˆì—ì„œ í‰ì•ˆí•˜ì„¸ìš”, ê°ì‚¬í•©ë‹ˆë‹¤.</p>"
            return default_msg
        
        # 3. ì»¨í…ìŠ¤íŠ¸ ë¶„ì„
        context_analysis = self.analyze_context_quality(similar_answers, query)
        
        # 4. ê²€ìƒ‰ ê²°ê³¼ ìƒì„¸ ë¡œê¹…
        logging.info(f"âœ… ê²€ìƒ‰ëœ ìœ ì‚¬ ë‹µë³€ ê°œìˆ˜: {len(similar_answers)}")
        print(f"âœ… ê²€ìƒ‰ëœ ìœ ì‚¬ ë‹µë³€ ê°œìˆ˜: {len(similar_answers)}")
        
        if similar_answers:
            for i, ans in enumerate(similar_answers[:3]):
                log_msg = f"ğŸ“ ë‹µë³€ #{i+1}: ì ìˆ˜={ans['score']:.3f}, ì¹´í…Œê³ ë¦¬={ans['category']}"
                logging.info(log_msg)
                print(log_msg)
        
        # 5. ë‹µë³€ì´ ì „í˜€ ì—†ì„ ë•Œë§Œ ê¸°ë³¸ ë©”ì‹œì§€ ë°˜í™˜ (ì¤‘ë³µ ì²´í¬ ì œê±°)
        # ì´ë¯¸ ìœ„ì—ì„œ ì²´í¬í–ˆìœ¼ë¯€ë¡œ ì´ ë¶€ë¶„ì€ ì‹¤í–‰ë˜ì§€ ì•Šì•„ì•¼ í•¨
        

        try:
            approach = context_analysis['recommended_approach']
            logging.info(f"=== ì ‘ê·¼ ë°©ì‹ ê²°ì • ===")
            logging.info(f"ğŸ¯ ì„ íƒëœ ì ‘ê·¼ ë°©ì‹: {approach}, ì–¸ì–´: {lang}")
            logging.info(f"ğŸ“Š ì»¨í…ìŠ¤íŠ¸ ë¶„ì„: {context_analysis}")
            
            # ì½˜ì†”ì—ë„ ì¶œë ¥
            print(f"ğŸ¯ ì„ íƒëœ ì ‘ê·¼ ë°©ì‹: {approach}, ì–¸ì–´: {lang}")
            print(f"ğŸ“Š ì»¨í…ìŠ¤íŠ¸ ë¶„ì„: {context_analysis}")
            
            base_answer = ""
            
            if approach == 'direct_use':
                logging.info("=== ì§ì ‘ ì‚¬ìš© ë°©ì‹ ì‹œì‘ ===")
                base_answer = self.get_best_fallback_answer(similar_answers[:3], lang)
                logging.info(f"ì§ì ‘ ì‚¬ìš© ê²°ê³¼ ê¸¸ì´: {len(base_answer) if base_answer else 0}")
                logging.info(f"ì§ì ‘ ì‚¬ìš© ê²°ê³¼ ë¯¸ë¦¬ë³´ê¸°: {base_answer[:100] if base_answer else 'None'}...")
                
            elif approach in ['gpt_with_strong_context', 'gpt_with_weak_context']:
                logging.info(f"=== GPT ìƒì„± ë°©ì‹ ì‹œì‘: {approach} ===")
                base_answer = self.generate_with_enhanced_gpt(query, similar_answers, context_analysis, lang)
                logging.info(f"GPT ìƒì„± ê²°ê³¼ ê¸¸ì´: {len(base_answer) if base_answer else 0}")
                
                if not base_answer or not self.is_valid_text(base_answer, lang):
                    logging.warning("GPT ìƒì„± ì‹¤íŒ¨, í´ë°± ë‹µë³€ ì‚¬ìš©")
                    base_answer = self.get_best_fallback_answer(similar_answers, lang)
                    logging.info(f"í´ë°± ë‹µë³€ ê¸¸ì´: {len(base_answer) if base_answer else 0}")
                    
            else:
                logging.info("=== í´ë°± ë°©ì‹ ì‚¬ìš© ===")
                base_answer = self.get_best_fallback_answer(similar_answers, lang)
                logging.info(f"í´ë°± ë‹µë³€ ê¸¸ì´: {len(base_answer) if base_answer else 0}")
            
            # ìµœì¢… ê²€ì¦ ì „ ìƒì„¸ ë¡œê¹…
            logging.info(f"=== ìµœì¢… ê²€ì¦ ì‹œì‘ ===")
            logging.info(f"base_answer ì¡´ì¬ ì—¬ë¶€: {base_answer is not None and base_answer != ''}")
            if base_answer:
                logging.info(f"base_answer ê¸¸ì´: {len(base_answer)}")
                logging.info(f"base_answer ë¯¸ë¦¬ë³´ê¸°: {base_answer[:200]}...")
                is_valid = self.is_valid_text(base_answer, lang)
                logging.info(f"is_valid_text ê²°ê³¼: {is_valid}")
                if not is_valid:
                    logging.warning(f"ìœ íš¨ì„± ê²€ì‚¬ ì‹¤íŒ¨ ì‚¬ìœ  ë¶„ì„ ì¤‘...")
                    # í•œêµ­ì–´ í…ìŠ¤íŠ¸ ë¶„ì„
                    if lang == 'ko':
                        korean_chars = len(re.findall(r'[ê°€-í£]', base_answer))
                        total_chars = len(re.sub(r'\s', '', base_answer))
                        korean_ratio = korean_chars / total_chars if total_chars > 0 else 0
                        logging.warning(f"í•œêµ­ì–´ ë¹„ìœ¨: {korean_ratio:.2f} (ê¸°ì¤€: 0.2 ì´ìƒ)")
                        logging.warning(f"í•œêµ­ì–´ ê¸€ì ìˆ˜: {korean_chars}, ì „ì²´ ê¸€ì ìˆ˜: {total_chars}")
            else:
                logging.error("base_answerê°€ ë¹„ì–´ìˆìŒ")
            
            # ğŸ”¥ ê¸´ê¸‰ ìˆ˜ì •: is_valid_text ê²€ì¦ ì™„ì „íˆ ìš°íšŒ
            if not base_answer:
                logging.error("=== base_answerê°€ ë¹„ì–´ìˆìŒ ===")
                logging.error(f"similar_answers ê°œìˆ˜: {len(similar_answers)}")
                if similar_answers:
                    logging.error(f"ì²« ë²ˆì§¸ ë‹µë³€ ì ìˆ˜: {similar_answers[0]['score']}")
                    logging.error(f"ì²« ë²ˆì§¸ ë‹µë³€ ë‚´ìš©: {similar_answers[0]['answer'][:100]}...")
                
                if lang == 'en':
                    return "<p>We need more detailed information to provide an accurate answer to your inquiry.</p><p><br></p><p>Please contact our customer service center for prompt assistance.</p>"
                else:
                    return "<p>ì•ˆë…•í•˜ì„¸ìš”, GOODTV ë°”ì´ë¸” ì• í”Œì…ë‹ˆë‹¤.</p><p><br></p><p>ë°”ì´ë¸” ì• í”Œì„ ì´ìš©í•´ ì£¼ì…”ì„œ ì§„ì‹¬ìœ¼ë¡œ ê°ì‚¬ë“œë¦½ë‹ˆë‹¤.</p><p><br></p><p>ë‚¨ê²¨ì£¼ì‹  ë¬¸ì˜ëŠ” í˜„ì¬ ë‹´ë‹¹ìê°€ ì§ì ‘ í™•ì¸í•˜ê³  ìˆìŠµë‹ˆë‹¤.</p><p><br></p><p>ì„±ë„ë‹˜ê»˜ ë„ì›€ì´ ë  ìˆ˜ ìˆë„ë¡ ë‚´ìš©ì„ ê¼¼ê¼¼íˆ ì‚´í´</p><p><br></p><p>ì •í™•í•˜ê³  êµ¬ì²´ì ì¸ ë‹µë³€ì„ ì¤€ë¹„í•˜ê² ìŠµë‹ˆë‹¤.</p><p><br></p><p>ë‹µë³€ì€ ìµœëŒ€ í•˜ë£¨ ì´ë‚´ì— ë“œë¦´ ì˜ˆì •ì´ì˜¤ë‹ˆ</p><p><br></p><p>ì¡°ê¸ˆë§Œ ê¸°ë‹¤ë ¤ ì£¼ì‹œë©´ ê°ì‚¬í•˜ê² ìŠµë‹ˆë‹¤.</p><p><br></p><p>í•­ìƒ ì£¼ë‹˜ ì•ˆì—ì„œ í‰ì•ˆí•˜ì„¸ìš”, ê°ì‚¬í•©ë‹ˆë‹¤.</p>"
            
            # ğŸ”¥ is_valid_text ê²€ì¦ì„ ì„ì‹œë¡œ ì£¼ì„ ì²˜ë¦¬
            # elif not self.is_valid_text(base_answer, lang):
            elif False:  # í•­ìƒ Falseê°€ ë˜ì–´ ì´ ë¸”ë¡ì€ ì‹¤í–‰ë˜ì§€ ì•ŠìŒ
                logging.warning(f"ìœ íš¨ì„± ê²€ì‚¬ ì‹¤íŒ¨í–ˆì§€ë§Œ ë‹µë³€ ì¡´ì¬í•¨ - ê°•ì œ ì§„í–‰")
                # ì´ ë¸”ë¡ì€ ì‹¤í–‰ë˜ì§€ ì•ŠìŒ
            
            # ğŸ”¥ ì„±ê³µ ë¡œê·¸ ì¶”ê°€
            logging.info("ğŸ‰ ìœ íš¨ì„± ê²€ì‚¬ ìš°íšŒ ì„±ê³µ - ë‹µë³€ í¬ë§·íŒ… ì‹œì‘")
            print("ğŸ‰ ìœ íš¨ì„± ê²€ì‚¬ ìš°íšŒ ì„±ê³µ - ë‹µë³€ í¬ë§·íŒ… ì‹œì‘")
            
            # ì–¸ì–´ë³„ í¬ë§·íŒ…
            if lang == 'en':
                # ì˜ì–´ ë‹µë³€ í¬ë§·íŒ…
                base_answer = self.remove_old_app_name(base_answer)
                
                # ê¸°ì¡´ ì¸ì‚¬ë§/ëë§ºìŒë§ ì œê±°
                base_answer = re.sub(r'^Hello[^.]*\.\s*', '', base_answer, flags=re.IGNORECASE)
                base_answer = re.sub(r'^This is GOODTV Bible App[^.]*\.\s*', '', base_answer, flags=re.IGNORECASE)
                base_answer = re.sub(r'\s*Thank you[^.]*\.?\s*$', '', base_answer, flags=re.IGNORECASE)
                base_answer = re.sub(r'\s*Best regards[^.]*\.?\s*$', '', base_answer, flags=re.IGNORECASE)
                base_answer = re.sub(r'\s*God bless[^.]*\.?\s*$', '', base_answer, flags=re.IGNORECASE)
                
                formatted_body = self.format_answer_with_html_paragraphs(base_answer.strip(), 'en')
                
                # ì˜ì–´ ê³ ì • ì¸ì‚¬ë§ê³¼ ëë§ºìŒë§
                final_answer = "<p>Hello, this is GOODTV Bible Apple App customer service team.</p><p><br></p><p>Thank you very much for using our app and for taking the time to contact us.</p><p><br></p>"
                final_answer += formatted_body
                final_answer += "<p><br></p><p>Thank you once again for sharing your thoughts with us!</p><p><br></p><p>May God's peace and grace always be with you.</p>"
                
            else:  # í•œêµ­ì–´
                # í•œêµ­ì–´ ë‹µë³€ ìµœì¢… í¬ë§·íŒ… (Quill ì—ë””í„°ìš© HTML í˜•ì‹ ìœ ì§€)
                # ì•± ì´ë¦„ ì •ë¦¬ ë° ê³ ê°ë‹˜ â†’ ì„±ë„ë‹˜ ë³€ê²½
                base_answer = self.remove_old_app_name(base_answer)
                base_answer = re.sub(r'ê³ ê°ë‹˜', 'ì„±ë„ë‹˜', base_answer)
                
                # ê¸°ì¡´ ì¸ì‚¬ë§/ëë§ºìŒë§ ì œê±° (ì¼ë°˜ í…ìŠ¤íŠ¸ì—ì„œ)
                # ì¸ì‚¬ë§ ì œê±°
                base_answer = re.sub(r'^ì•ˆë…•í•˜ì„¸ìš”[^.]*\.\s*', '', base_answer, flags=re.IGNORECASE)
                base_answer = re.sub(r'^GOODTV\s+ë°”ì´ë¸”\s*ì• í”Œ[^.]*\.\s*', '', base_answer, flags=re.IGNORECASE)
                base_answer = re.sub(r'^ë°”ì´ë¸”\s*ì• í”Œ[^.]*\.\s*', '', base_answer, flags=re.IGNORECASE)
                base_answer = re.sub(r'ê³ ê°ì„¼í„°[^.]*\.\s*', '', base_answer, flags=re.IGNORECASE)
                
                # ëë§ºìŒë§ ì œê±° (ë” ê°•í™”ëœ íŒ¨í„´) - 'í•­ìƒ' ì¤‘ë³µ ì œê±° í¬í•¨
                base_answer = re.sub(r'\s*ê°ì‚¬í•©ë‹ˆë‹¤[^.]*\.?\s*$', '', base_answer, flags=re.IGNORECASE)
                base_answer = re.sub(r'\s*í‰ì•ˆí•˜ì„¸ìš”[^.]*\.?\s*$', '', base_answer, flags=re.IGNORECASE)
                base_answer = re.sub(r'\s*ì£¼ë‹˜\s*ì•ˆì—ì„œ[^.]*\.?\s*$', '', base_answer, flags=re.IGNORECASE)
                base_answer = re.sub(r'\s*í•¨ê»˜\s*ê¸°ë„í•˜ë©°[^.]*\.?\s*$', '', base_answer, flags=re.IGNORECASE)
                base_answer = re.sub(r'\s*í•­ìƒ[^.]*ë°”ì´ë¸”\s*ì• í”Œ[^.]*\.?\s*$', '', base_answer, flags=re.IGNORECASE)
                
                # ì¶”ê°€ ëë§ºìŒë§ íŒ¨í„´ë“¤ (ë” í¬ê´„ì ìœ¼ë¡œ)
                base_answer = re.sub(r'\s*í•­ìƒ\s*ì£¼ë‹˜\s*ì•ˆì—ì„œ[^.]*\.?\s*$', '', base_answer, flags=re.IGNORECASE)
                base_answer = re.sub(r'\s*ì£¼ë‹˜\s*ì•ˆì—ì„œ\s*í‰ì•ˆí•˜ì„¸ìš”[^.]*\.?\s*$', '', base_answer, flags=re.IGNORECASE)
                base_answer = re.sub(r'\s*í‰ì•ˆí•˜ì„¸ìš”[^.]*\.?\s*$', '', base_answer, flags=re.IGNORECASE)
                
                # ë¬¸ì¥ ëì˜ ëë§ºìŒë§ë“¤ë„ ì œê±°
                base_answer = re.sub(r'[,.!?]\s*í•­ìƒ\s*ì£¼ë‹˜\s*ì•ˆì—ì„œ[^.]*\.?\s*$', '', base_answer, flags=re.IGNORECASE)
                base_answer = re.sub(r'[,.!?]\s*ê°ì‚¬í•©ë‹ˆë‹¤[^.]*\.?\s*$', '', base_answer, flags=re.IGNORECASE)
                base_answer = re.sub(r'[,.!?]\s*í‰ì•ˆí•˜ì„¸ìš”[^.]*\.?\s*$', '', base_answer, flags=re.IGNORECASE)
                
                # ğŸ”¥ êµ¬ ì•± ì´ë¦„ì„ ë°”ì´ë¸” ì• í”Œë¡œ ì™„ì „ êµì²´ (ì¤‘ë³µ ë°©ì§€)
                # ì¤‘ë³µ ë°©ì§€ë¥¼ ìœ„í•´ ìˆœì„œë¥¼ ì¡°ì •: ì „ì²´ íŒ¨í„´ë¶€í„° ì²˜ë¦¬
                base_answer = re.sub(r'ë°”ì´ë¸”\s*ì• í”Œ\s*\(êµ¬\)\s*ë‹¤ë²ˆì—­\s*ì„±ê²½\s*ì°¬ì†¡', 'ë°”ì´ë¸” ì• í”Œ', base_answer, flags=re.IGNORECASE)
                base_answer = re.sub(r'ë°”ì´ë¸”\s*ì• í”Œ\s*\(êµ¬\)\s*ë‹¤ë²ˆì—­ì„±ê²½ì°¬ì†¡', 'ë°”ì´ë¸” ì• í”Œ', base_answer, flags=re.IGNORECASE)
                base_answer = re.sub(r'\(êµ¬\)\s*ë‹¤ë²ˆì—­\s*ì„±ê²½\s*ì°¬ì†¡', 'ë°”ì´ë¸” ì• í”Œ', base_answer, flags=re.IGNORECASE)
                base_answer = re.sub(r'\(êµ¬\)\s*ë‹¤ë²ˆì—­ì„±ê²½ì°¬ì†¡', 'ë°”ì´ë¸” ì• í”Œ', base_answer, flags=re.IGNORECASE)
                base_answer = re.sub(r'ë‹¤ë²ˆì—­\s*ì„±ê²½\s*ì°¬ì†¡', 'ë°”ì´ë¸” ì• í”Œ', base_answer, flags=re.IGNORECASE)
                base_answer = re.sub(r'ë‹¤ë²ˆì—­ì„±ê²½ì°¬ì†¡', 'ë°”ì´ë¸” ì• í”Œ', base_answer, flags=re.IGNORECASE)
                
                # ğŸ”¥ ì™„ì „íˆ ê°•í™”ëœ ì¤‘ë³µ ëë§ºìŒë§ ì œê±° ì‹œìŠ¤í…œ
                # 1ë‹¨ê³„: ëª¨ë“  í˜•íƒœì˜ "í•­ìƒ ì„±ë„ë‹˜ê»˜..." íŒ¨í„´ ì œê±°
                base_answer = re.sub(r'í•­ìƒ\s*ì„±ë„ë‹˜ë“¤?ê»˜\s*ì¢‹ì€\s*(ì„œë¹„ìŠ¤|ì„±ê²½ì•±)ì„?\s*ì œê³µí•˜ê¸°\s*ìœ„í•´\s*ë…¸ë ¥í•˜ëŠ”\s*ë°”ì´ë¸”\s*ì• í”Œì´\s*ë˜ê² ìŠµë‹ˆë‹¤\.?\s*', 
                                   '', base_answer, flags=re.IGNORECASE)
                
                # 2ë‹¨ê³„: ê°ì‚¬í•©ë‹ˆë‹¤ íŒ¨í„´ ì™„ì „ ì œê±°
                base_answer = re.sub(r'ê°ì‚¬í•©ë‹ˆë‹¤\.?\s*(ì£¼ë‹˜\s*ì•ˆì—ì„œ\s*í‰ì•ˆí•˜ì„¸ìš”\.?)?\s*', 
                                   '', base_answer, flags=re.IGNORECASE)
                
                # 3ë‹¨ê³„: ë¶ˆì™„ì „í•œ ë¬¸ì¥ë“¤ ì œê±°
                base_answer = re.sub(r'ì˜¤ëŠ˜ë„\s*$', '', base_answer, flags=re.IGNORECASE)
                base_answer = re.sub(r'ì˜¤ëŠ˜ë„\s*\n', '\n', base_answer, flags=re.IGNORECASE)
                base_answer = re.sub(r'í•­ìƒ\s*$', '', base_answer, flags=re.IGNORECASE)
                
                # ğŸ”¥ 'í•­ìƒ' ë‹¨ë…ìœ¼ë¡œ ë‚¨ì€ ê²½ìš° ì œê±° (ì¤‘ë³µ ë¬¸ì œ í•´ê²°)
                base_answer = re.sub(r'\s*í•­ìƒ\s*$', '', base_answer, flags=re.IGNORECASE)
                base_answer = re.sub(r'\n\s*í•­ìƒ\s*\n', '\n', base_answer, flags=re.IGNORECASE)
                base_answer = re.sub(r'<p>\s*í•­ìƒ\s*</p>', '', base_answer, flags=re.IGNORECASE)
                base_answer = re.sub(r'<p><br></p>\s*<p>\s*í•­ìƒ\s*</p>', '', base_answer, flags=re.IGNORECASE)
                
                # ë³¸ë¬¸ì„ HTML ë‹¨ë½ í˜•ì‹ìœ¼ë¡œ í¬ë§·íŒ…
                formatted_body = self.format_answer_with_html_paragraphs(base_answer.strip(), 'ko')
                
                # í•œêµ­ì–´ ê³ ì • ì¸ì‚¬ë§ (HTML í˜•ì‹ìœ¼ë¡œ)
                final_answer = "<p>ì•ˆë…•í•˜ì„¸ìš”. GOODTV ë°”ì´ë¸” ì• í”Œì…ë‹ˆë‹¤.</p><p><br></p><p>ë°”ì´ë¸” ì• í”Œì„ ì´ìš©í•´ì£¼ì…”ì„œ ê°ì‚¬ë“œë¦½ë‹ˆë‹¤.</p><p><br></p>"
                
                # í¬ë§·íŒ…ëœ ë³¸ë¬¸ ì¶”ê°€ ì „ ìµœì¢… ì •ë¦¬
                # ğŸ”¥ HTML í¬ë§·íŒ… í›„ ì™„ì „í•œ ì •ë¦¬ ì‘ì—…
                # ì¤‘ë³µëœ ëë§ºìŒë§ HTML íƒœê·¸ ì œê±°
                formatted_body = re.sub(r'<p>\s*í•­ìƒ\s*ì„±ë„ë‹˜ë“¤?ê»˜\s*ì¢‹ì€\s*(ì„œë¹„ìŠ¤|ì„±ê²½ì•±)ì„?\s*ì œê³µí•˜ê¸°\s*ìœ„í•´\s*ë…¸ë ¥í•˜ëŠ”\s*ë°”ì´ë¸”\s*ì• í”Œì´\s*ë˜ê² ìŠµë‹ˆë‹¤\.?\s*</p>', '', formatted_body, flags=re.IGNORECASE)
                formatted_body = re.sub(r'<p>\s*ê°ì‚¬í•©ë‹ˆë‹¤\.?\s*(ì£¼ë‹˜\s*ì•ˆì—ì„œ\s*í‰ì•ˆí•˜ì„¸ìš”\.?)?\s*</p>', '', formatted_body, flags=re.IGNORECASE)
                
                # ë¶ˆì™„ì „í•œ ë¬¸ì¥ë“¤ ì œê±°
                formatted_body = re.sub(r'<p>\s*í•­ìƒ\s*</p>', '', formatted_body, flags=re.IGNORECASE)
                formatted_body = re.sub(r'<p>\s*ì˜¤ëŠ˜ë„\s*</p>', '', formatted_body, flags=re.IGNORECASE)
                formatted_body = re.sub(r'<p><br></p>\s*<p>\s*(í•­ìƒ|ì˜¤ëŠ˜ë„)\s*</p>', '', formatted_body, flags=re.IGNORECASE)
                formatted_body = re.sub(r'<p>\s*(í•­ìƒ|ì˜¤ëŠ˜ë„)\s*<br></p>', '', formatted_body, flags=re.IGNORECASE)
                
                # ì—°ì†ëœ ë¹ˆ íƒœê·¸ë“¤ ì •ë¦¬
                formatted_body = re.sub(r'(<p><br></p>\s*){3,}', '<p><br></p><p><br></p>', formatted_body)
                formatted_body = re.sub(r'(<p><br></p>\s*)+$', '', formatted_body)  # ëì˜ ë¹ˆ íƒœê·¸ë“¤ ì œê±°
                
                final_answer += formatted_body
                
                # ê³ ì •ëœ ëë§ºìŒë§ (HTML í˜•ì‹ìœ¼ë¡œ)
                final_answer += "<p><br></p><p>í•­ìƒ ì„±ë„ë‹˜ê»˜ ì¢‹ì€ ì„±ê²½ì•±ì„ ì œê³µí•˜ê¸° ìœ„í•´ ë…¸ë ¥í•˜ëŠ” ë°”ì´ë¸” ì• í”Œì´ ë˜ê² ìŠµë‹ˆë‹¤.</p><p><br></p><p>ê°ì‚¬í•©ë‹ˆë‹¤. ì£¼ë‹˜ ì•ˆì—ì„œ í‰ì•ˆí•˜ì„¸ìš”.</p>"
            
            logging.info(f"ìµœì¢… ë‹µë³€ ìƒì„± ì™„ë£Œ: {len(final_answer)}ì, ì ‘ê·¼ë°©ì‹: {approach}, ì–¸ì–´: {lang}")
            return final_answer
            
        except Exception as e:
            logging.error(f"ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜: {e}")
            if lang == 'en':
                return "<p>Sorry, we cannot generate an answer at this moment.</p><p><br></p><p>Please contact our customer service center.</p>"
            else:
                return "<p>ì•ˆë…•í•˜ì„¸ìš”, GOODTV ë°”ì´ë¸” ì• í”Œì…ë‹ˆë‹¤.</p><p><br></p><p>ë°”ì´ë¸” ì• í”Œì„ ì´ìš©í•´ ì£¼ì…”ì„œ ì§„ì‹¬ìœ¼ë¡œ ê°ì‚¬ë“œë¦½ë‹ˆë‹¤.</p><p><br></p><p>ë‚¨ê²¨ì£¼ì‹  ë¬¸ì˜ëŠ” í˜„ì¬ ë‹´ë‹¹ìê°€ ì§ì ‘ í™•ì¸í•˜ê³  ìˆìŠµë‹ˆë‹¤.</p><p><br></p><p>ì„±ë„ë‹˜ê»˜ ë„ì›€ì´ ë  ìˆ˜ ìˆë„ë¡ ë‚´ìš©ì„ ê¼¼ê¼¼íˆ ì‚´í´</p><p><br></p><p>ì •í™•í•˜ê³  êµ¬ì²´ì ì¸ ë‹µë³€ì„ ì¤€ë¹„í•˜ê² ìŠµë‹ˆë‹¤.</p><p><br></p><p>ë‹µë³€ì€ ìµœëŒ€ í•˜ë£¨ ì´ë‚´ì— ë“œë¦´ ì˜ˆì •ì´ì˜¤ë‹ˆ</p><p><br></p><p>ì¡°ê¸ˆë§Œ ê¸°ë‹¤ë ¤ ì£¼ì‹œë©´ ê°ì‚¬í•˜ê² ìŠµë‹ˆë‹¤.</p><p><br></p><p>í•­ìƒ ì£¼ë‹˜ ì•ˆì—ì„œ í‰ì•ˆí•˜ì„¸ìš”, ê°ì‚¬í•©ë‹ˆë‹¤.</p>"

    # â˜† ë©”ëª¨ë¦¬ ìµœì í™”ëœ ë©”ì¸ ì²˜ë¦¬ ë©”ì„œë“œ
    def process(self, seq: int, question: str, lang: str) -> dict:
        try:
            with memory_cleanup():
                # 1. ì „ì²˜ë¦¬
                processed_question = self.preprocess_text(question)

                # 2. ì˜¤íƒ€ ìˆ˜ì • ì¶”ê°€ (Pinecone ì €ì¥ ì‹œì™€ ë™ì¼í•˜ê²Œ!)
                if lang == 'ko' or lang == 'auto':
                    processed_question = self.fix_korean_typos_with_ai(processed_question)
                    logging.info(f"ì˜¤íƒ€ ìˆ˜ì • ì ìš©: {question[:50]} â†’ {processed_question[:50]}")
                
                if not processed_question:
                    return {"success": False, "error": "ì§ˆë¬¸ì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤."}
                
                # ì–¸ì–´ ìë™ ê°ì§€
                if not lang or lang == 'auto':
                    lang = self.detect_language(processed_question)
                    logging.info(f"ìë™ ê°ì§€ëœ ì–¸ì–´: {lang}")
                
                logging.info(f"ì²˜ë¦¬ ì‹œì‘ - SEQ: {seq}, ì–¸ì–´: {lang}, ì§ˆë¬¸: {processed_question[:50]}...")
                
                # 3. ìœ ì‚¬ ë‹µë³€ ê²€ìƒ‰ (ì´ì œ ì˜¤íƒ€ ìˆ˜ì •ëœ ì§ˆë¬¸ìœ¼ë¡œ ê²€ìƒ‰)
                similar_answers = self.search_similar_answers(processed_question, lang=lang)
                
                # AI ë‹µë³€ ìƒì„± (ì–¸ì–´ íŒŒë¼ë¯¸í„° ì „ë‹¬)
                ai_answer = self.generate_ai_answer(processed_question, similar_answers, lang)
                
                # íŠ¹ìˆ˜ë¬¸ì ì •ë¦¬
                ai_answer = ai_answer.replace('"', '"').replace('"', '"')
                ai_answer = ai_answer.replace(''', "'").replace(''', "'")
                
                result = {
                    "success": True,
                    "answer": ai_answer,
                    "similar_count": len(similar_answers),
                    "embedding_model": "text-embedding-3-small",
                    "generation_model": "gpt-3.5-turbo",
                    "detected_language": lang
                }
                
                logging.info(f"ì²˜ë¦¬ ì™„ë£Œ - SEQ: {seq}, ì–¸ì–´: {lang}, HTML ë‹µë³€ ìƒì„±ë¨")
                return result
                
        except Exception as e:
            logging.error(f"ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ - SEQ: {seq}, ì˜¤ë¥˜: {str(e)}")
            return {"success": False, "error": str(e)}

# ==================================================
# 8. Pinecone ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ë™ê¸°í™” í´ë˜ìŠ¤
# ==================================================
# MSSQL ìš´ì˜ ë°ì´í„°ë² ì´ìŠ¤ì™€ Pinecone ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ê°„ì˜ ë™ê¸°í™”ë¥¼ ë‹´ë‹¹í•˜ëŠ” í´ë˜ìŠ¤
# 
# ì£¼ìš” ê¸°ëŠ¥:
# 1. MSSQLì—ì„œ ìƒˆë¡œìš´ Q&A ë°ì´í„° ì¡°íšŒ
# 2. AIë¥¼ ì´ìš©í•œ í•œêµ­ì–´ ì˜¤íƒ€ ìˆ˜ì •
# 3. OpenAIë¡œ ì„ë² ë”© ë²¡í„° ìƒì„±
# 4. Pineconeì— ë²¡í„° ë°ì´í„° ì €ì¥/ìˆ˜ì •/ì‚­ì œ
# 
# ìš´ì˜ ì‹œë‚˜ë¦¬ì˜¤:
# - ìƒˆë¡œìš´ ê³ ê° ë¬¸ì˜ ë‹µë³€ì´ MSSQLì— ì €ì¥ë˜ë©´
# - ì´ í´ë˜ìŠ¤ë¥¼ í†µí•´ Pineconeì— ë™ê¸°í™”í•˜ì—¬
# - í–¥í›„ ìœ ì‚¬ ì§ˆë¬¸ ê²€ìƒ‰ì´ ê°€ëŠ¥í•˜ê²Œ í•¨
class PineconeSyncManager:
    
    # ë™ê¸°í™” ë§¤ë‹ˆì € ì´ˆê¸°í™”
    # ì™¸ë¶€ì—ì„œ ìƒì„±ëœ Pinecone ì¸ë±ìŠ¤ì™€ OpenAI í´ë¼ì´ì–¸íŠ¸ ì°¸ì¡°
    def __init__(self):
        self.index = index                    # Pinecone ë²¡í„° ì¸ë±ìŠ¤
        self.openai_client = openai_client    # OpenAI API í´ë¼ì´ì–¸íŠ¸
    
    # â˜† AIë¥¼ ì´ìš©í•œ í•œêµ­ì–´ ì˜¤íƒ€ ìˆ˜ì • ë©”ì„œë“œ
    def fix_korean_typos_with_ai(self, text: str) -> str:
        if not text or len(text.strip()) < 3:
            return text
        
        # ë„ˆë¬´ ê¸´ í…ìŠ¤íŠ¸ëŠ” ì²˜ë¦¬í•˜ì§€ ì•ŠìŒ (ë¹„ìš© ì ˆì•½)
        if len(text) > 500:
            logging.warning(f"í…ìŠ¤íŠ¸ê°€ ë„ˆë¬´ ê¸¸ì–´ ì˜¤íƒ€ ìˆ˜ì • ê±´ë„ˆëœ€: {len(text)}ì")
            return text
        
        try:
            with memory_cleanup():
                system_prompt = """ë‹¹ì‹ ì€ í•œêµ­ì–´ ë§ì¶¤ë²• ë° ì˜¤íƒ€ êµì • ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

ì§€ì¹¨:
1. ì…ë ¥ëœ í•œêµ­ì–´ í…ìŠ¤íŠ¸ì˜ ë§ì¶¤ë²•ê³¼ ì˜¤íƒ€ë§Œ ìˆ˜ì •í•˜ì„¸ìš”
2. ì›ë¬¸ì˜ ì˜ë¯¸ì™€ ì–´ì¡°ëŠ” ì ˆëŒ€ ë³€ê²½í•˜ì§€ ë§ˆì„¸ìš”
3. ë„ì–´ì“°ê¸°, ë§ì¶¤ë²•, ì¡°ì‚¬ ì‚¬ìš©ë²•ì„ ì •í™•íˆ êµì •í•˜ì„¸ìš”
4. ì•±/ì–´í”Œë¦¬ì¼€ì´ì…˜ ê´€ë ¨ ê¸°ìˆ  ìš©ì–´ëŠ” í‘œì¤€ ìš©ì–´ë¡œ í†µì¼í•˜ì„¸ìš”
5. ìˆ˜ì •ì´ í•„ìš”ì—†ë‹¤ë©´ ì›ë¬¸ ê·¸ëŒ€ë¡œ ë°˜í™˜í•˜ì„¸ìš”
6. ìˆ˜ì •ëœ í…ìŠ¤íŠ¸ë§Œ ë°˜í™˜í•˜ê³  ì¶”ê°€ ì„¤ëª…ì€ í•˜ì§€ ë§ˆì„¸ìš”

ì˜ˆì‹œ:
- "ì–´í”Œì´ ì•ˆë€ë‹¤" â†’ "ì•±ì´ ì•ˆ ë¼ìš”"
- "ë‹¤ìš´ë°›ê¸°ê°€ ì•ˆë˜ìš”" â†’ "ë‹¤ìš´ë¡œë“œê°€ ì•ˆ ë¼ìš”"
- "ì‚­ì¬í•˜ê³ ì‹¶ì–´ìš”" â†’ "ì‚­ì œí•˜ê³  ì‹¶ì–´ìš”"
- "ì—…ë°ì´ë“œí•´ì£¼ì„¸ìš”" â†’ "ì—…ë°ì´íŠ¸í•´ì£¼ì„¸ìš”"
"""

                user_prompt = f"ë‹¤ìŒ í…ìŠ¤íŠ¸ì˜ ë§ì¶¤ë²•ê³¼ ì˜¤íƒ€ë¥¼ ìˆ˜ì •í•´ì£¼ì„¸ìš”:\n\n{text}"

                response = self.openai_client.chat.completions.create(
                    model='gpt-3.5-turbo',
                    messages=[
                        {"role": "system", "content": system_prompt},
                        {"role": "user", "content": user_prompt}
                    ],
                    max_tokens=600,
                    temperature=0.1,  # ë§¤ìš° ë³´ìˆ˜ì ìœ¼ë¡œ ì„¤ì •
                    top_p=0.8,
                    frequency_penalty=0.0,
                    presence_penalty=0.0
                )
                
                corrected_text = response.choices[0].message.content.strip()
                del response # ë©”ëª¨ë¦¬ í•´ì œ
                
                # ê²°ê³¼ ê²€ì¦
                if not corrected_text or len(corrected_text) == 0:
                    logging.warning("AI ì˜¤íƒ€ ìˆ˜ì • ê²°ê³¼ê°€ ë¹„ì–´ìˆìŒ, ì›ë¬¸ ë°˜í™˜")
                    return text
                
                # ë„ˆë¬´ ë§ì´ ë³€ê²½ëœ ê²½ìš° ì˜ì‹¬ìŠ¤ëŸ¬ìš°ë¯€ë¡œ ì›ë¬¸ ë°˜í™˜
                if len(corrected_text) > len(text) * 2:
                    logging.warning("AI ì˜¤íƒ€ ìˆ˜ì • ê²°ê³¼ê°€ ì›ë¬¸ë³´ë‹¤ ë„ˆë¬´ ê¸¸ì–´ì§, ì›ë¬¸ ë°˜í™˜")
                    return text
                
                # ìˆ˜ì • ë‚´ìš©ì´ ìˆìœ¼ë©´ ë¡œê·¸ ê¸°ë¡
                if corrected_text != text:
                    logging.info(f"AI ì˜¤íƒ€ ìˆ˜ì •: '{text[:50]}...' â†’ '{corrected_text[:50]}...'")
                
                return corrected_text
                
        except Exception as e:
            logging.error(f"AI ì˜¤íƒ€ ìˆ˜ì • ì‹¤íŒ¨: {e}")
            # AI ì‹¤íŒ¨ ì‹œ ì›ë¬¸ ê·¸ëŒ€ë¡œ ë°˜í™˜
            return text
        
    # â˜† í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ ë©”ì„œë“œ
    def preprocess_text(self, text: str, for_metadata: bool = False) -> str:
        if not text or text == 'None':
            return ""
        
        text = str(text)
        text = html.unescape(text)
        
        # HTML íƒœê·¸ ì œê±°
        text = re.sub(r'<br\s*/?>', '\n', text, flags=re.IGNORECASE)
        text = re.sub(r'</p>', '\n', text, flags=re.IGNORECASE)
        text = re.sub(r'<p[^>]*>', '', text, flags=re.IGNORECASE)
        text = re.sub(r'<[^>]+>', '', text)
        
        # ìœ ë‹ˆì½”ë“œ ì •ê·œí™”
        text = unicodedata.normalize('NFC', text)
        
        # ê³µë°± ì •ë¦¬
        if for_metadata:
            text = re.sub(r'\n{3,}', '\n\n', text)
            text = re.sub(r'[ \t]+', ' ', text)
        else:
            text = re.sub(r'\s+', ' ', text)
        
        text = text.strip()
        
        # ê¸¸ì´ ì œí•œ
        max_length = 1000 if for_metadata else MAX_TEXT_LENGTH
        if len(text) > max_length:
            text = text[:max_length-3] + "..."
        
        return text
    
    # â˜† OpenAIë¡œ ì„ë² ë”© ìƒì„±í•˜ëŠ” ë©”ì„œë“œ
    def create_embedding(self, text: str) -> Optional[list]:
        try:
            if not text or not text.strip():
                return None
            
            with memory_cleanup():
                response = openai_client.embeddings.create(
                    model=MODEL_NAME,
                    input=text
                )
                return response.data[0].embedding
            
        except Exception as e:
            logging.error(f"ì„ë² ë”© ìƒì„± ì‹¤íŒ¨: {e}")
            return None
    
    # â˜† ì¹´í…Œê³ ë¦¬ ì¸ë±ìŠ¤ë¥¼ ì´ë¦„ìœ¼ë¡œ ë³€í™˜í•˜ëŠ” ë©”ì„œë“œ
    def get_category_name(self, cate_idx: str) -> str:
        return CATEGORY_MAPPING.get(str(cate_idx), 'ì‚¬ìš© ë¬¸ì˜(ê¸°íƒ€)')
    
    # â˜† MSSQLì—ì„œ ë°ì´í„° ì¡°íšŒí•˜ëŠ” ë©”ì„œë“œ
    # íŒŒë¼ë¯¸í„°í™”ëœ ì¿¼ë¦¬ë¡œ SQL ì¸ì ì…˜ ë°©ì§€, ? í”Œë ˆì´ìŠ¤í™€ë”ë¥¼ ì‚¬ìš©í•˜ì—¬ ì•ˆì „í•˜ê²Œ ê°’ì„ ë°”ì¸ë”©
    def get_mssql_data(self, seq: int) -> Optional[Dict]:
        try:
            with memory_cleanup():
                conn = pyodbc.connect(connection_string)
                cursor = conn.cursor()
                
                query = """
                SELECT seq, contents, reply_contents, cate_idx, name, 
                       CONVERT(varchar, regdate, 120) as regdate
                FROM mobile.dbo.bible_inquiry
                WHERE seq = ? AND answer_YN = 'Y'
                """
                
                cursor.execute(query, seq)
                row = cursor.fetchone()
                
                if row:
                    data = {
                        'seq': row[0],
                        'contents': row[1],
                        'reply_contents': row[2],
                        'cate_idx': row[3],
                        'name': row[4],
                        'regdate': row[5]
                    }
                    return data
                
                cursor.close()
                conn.close()
                return None
            
        except Exception as e:
            logging.error(f"MSSQL ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return None
    
    # â˜† MSSQL ë°ì´í„°ë¥¼ Pineconeì— ë™ê¸°í™”í•˜ëŠ” ë©”ì„œë“œ
    def sync_to_pinecone(self, seq: int, mode: str = 'upsert') -> Dict[str, Any]:
        try:
            with memory_cleanup():
                # ì‚­ì œ ëª¨ë“œ
                if mode == 'delete':
                    vector_id = f"qa_bible_{seq}"
                    self.index.delete(ids=[vector_id])
                    logging.info(f"Pineconeì—ì„œ ì‚­ì œ ì™„ë£Œ: {vector_id}")
                    return {"success": True, "message": "ì‚­ì œ ì™„ë£Œ", "seq": seq}
                
                # MSSQLì—ì„œ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
                data = self.get_mssql_data(seq)
                if not data:
                    return {"success": False, "error": "ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤"}
                
                # í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ (ì§ˆë¬¸ì— AI ì˜¤íƒ€ ìˆ˜ì • ì ìš©)
                raw_question = self.preprocess_text(data['contents'])
                question = self.fix_korean_typos_with_ai(raw_question)
                answer = self.preprocess_text(data['reply_contents'])
                
                # ì„ë² ë”© ìƒì„± (ì§ˆë¬¸ ê¸°ë°˜)
                embedding = self.create_embedding(question)
                if not embedding:
                    return {"success": False, "error": "ì„ë² ë”© ìƒì„± ì‹¤íŒ¨"}
                
                # ì¹´í…Œê³ ë¦¬ ì´ë¦„ ê°€ì ¸ì˜¤ê¸°
                category = self.get_category_name(data['cate_idx'])
                
                # ë©”íƒ€ë°ì´í„° êµ¬ì„± (ì§ˆë¬¸ì€ ì˜¤íƒ€ ìˆ˜ì •ëœ ë²„ì „ ì‚¬ìš©)
                metadata = {
                    "seq": int(data['seq']),
                    "question": question,
                    "answer": self.preprocess_text(data['reply_contents'], for_metadata=True),
                    "category": category,
                    "name": data['name'] if data['name'] else "ìµëª…",
                    "regdate": data['regdate'],
                    "source": "bible_inquiry_mssql",
                    "updated_at": datetime.now().strftime("%Y-%m-%d %H:%M:%S")
                }
                
                # Pineconeì— upsert
                vector_id = f"qa_bible_{seq}"
                
                # ê¸°ì¡´ ë²¡í„° í™•ì¸
                existing = self.index.fetch(ids=[vector_id])
                is_update = vector_id in existing['vectors']
                
                # ë²¡í„° ë°ì´í„° êµ¬ì„±
                vector_data = {
                    "id": vector_id,
                    "values": embedding,
                    "metadata": metadata
                }
                
                # Pineconeì— ì €ì¥
                self.index.upsert(vectors=[vector_data])
                
                action = "ìˆ˜ì •" if is_update else "ìƒì„±"
                logging.info(f"Pinecone {action} ì™„ë£Œ: {vector_id}")
                
                return {
                    "success": True,
                    "message": f"Pinecone {action} ì™„ë£Œ",
                    "seq": seq,
                    "vector_id": vector_id,
                    "is_update": is_update
                }
            
        except Exception as e:
            logging.error(f"Pinecone ë™ê¸°í™” ì‹¤íŒ¨: {str(e)}")
            return {"success": False, "error": str(e)}

# ==================================================
# 9. ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤ ìƒì„± (ì „ì—­ ê°ì²´)
# ==================================================
# ì• í”Œë¦¬ì¼€ì´ì…˜ ì „ì²´ì—ì„œ ì‚¬ìš©í•  ë‹¨ì¼ ì¸ìŠ¤í„´ìŠ¤ë“¤
# ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±ê³¼ ìƒíƒœ ì¼ê´€ì„±ì„ ìœ„í•´ ì‹±ê¸€í†¤ íŒ¨í„´ ì ìš©
generator = AIAnswerGenerator()      # AI ë‹µë³€ ìƒì„±ê¸°
sync_manager = PineconeSyncManager() # Pinecone ë™ê¸°í™” ë§¤ë‹ˆì €

# ==================================================
# 10. Flask RESTful API ì—”ë“œí¬ì¸íŠ¸ ì •ì˜
# ==================================================
# â˜… AI ë‹µë³€ ìƒì„± API ì—”ë“œí¬ì¸íŠ¸ (ë©”ì¸ ê¸°ëŠ¥)
# ASP Classicì—ì„œ í˜¸ì¶œí•˜ëŠ” ì£¼ìš” APIë¡œ, ê³ ê° ì§ˆë¬¸ì— ëŒ€í•œ AI ë‹µë³€ì„ ìƒì„±
# ì²˜ë¦¬ ê³¼ì •:
# 1. ì§ˆë¬¸ ì „ì²˜ë¦¬ ë° ê²€ì¦
# 2. Pineconeì—ì„œ ìœ ì‚¬ ë‹µë³€ ê²€ìƒ‰
# 3. ì»¨í…ìŠ¤íŠ¸ í’ˆì§ˆ ë¶„ì„
# 4. GPTë¥¼ ì´ìš©í•œ ë§ì¶¤ ë‹µë³€ ìƒì„±
# 5. ìµœì¢… í¬ë§·íŒ… ë° ë°˜í™˜
@app.route('/generate_answer', methods=['POST'])
def generate_answer():
    try:
        with memory_cleanup():
            data = request.get_json()
            seq = data.get('seq', 0)
            question = data.get('question', '')
            lang = data.get('lang', 'auto')  # ê¸°ë³¸ê°’ì„ 'auto'ë¡œ ë³€ê²½ (ìë™ ê°ì§€)
            
            if not question:
                return jsonify({"success": False, "error": "ì§ˆë¬¸ì´ í•„ìš”í•©ë‹ˆë‹¤."}), 400
            
            result = generator.process(seq, question, lang)
            
            response = jsonify(result)
            response.headers['Content-Type'] = 'application/json; charset=utf-8'

            # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ëª¨ë‹ˆí„°ë§
            snapshot = tracemalloc.take_snapshot() # ê° ìš”ì²­ í›„ ë©”ëª¨ë¦¬ ìŠ¤ëƒ…ìƒ· ì´¬ì˜
            top_stats = snapshot.statistics('lineno') # ê° ìš”ì²­ í›„ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ í†µê³„
            memory_usage = sum(stat.size for stat in top_stats) / 1024 / 1024  # MBë¡œ ë°˜í™˜
            logging.info(f"í˜„ì¬ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {memory_usage:.2f}MB")
            
            if memory_usage > 500: # 500MB ì´ˆê³¼ì‹œ ê²½ê³  ë° ê°€ë¹„ì§€ ì»¬ë ‰ì…˜ ê°•ì œ ì‹¤í–‰
                logging.warning(f"ë†’ì€ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ê°ì§€: {memory_usage:.2f}MB")
                gc.collect()

            return response
        
    except Exception as e:
        logging.error(f"API í˜¸ì¶œ ì˜¤ë¥˜: {str(e)}")
        return jsonify({"success": False, "error": str(e)}), 500

# â˜… MSSQL ë°ì´í„°ë¥¼ Pineconeì— ë™ê¸°í™”í•˜ëŠ” API ì—”ë“œí¬ì¸íŠ¸
# ìš´ì˜ ì‹œìŠ¤í…œì—ì„œ ìƒˆë¡œìš´ Q&A ë°ì´í„°ê°€ ìƒì„±ë˜ê±°ë‚˜ ìˆ˜ì •ë  ë•Œ í˜¸ì¶œ
# ì²˜ë¦¬ ê³¼ì •:
# 1. MSSQLì—ì„œ í•´ë‹¹ seq ë°ì´í„° ì¡°íšŒ
# 2. AIë¡œ ì§ˆë¬¸ ì˜¤íƒ€ ìˆ˜ì •
# 3. OpenAIë¡œ ì„ë² ë”© ë²¡í„° ìƒì„±
# 4. Pineconeì— ë²¡í„° ì €ì¥/ìˆ˜ì •/ì‚­ì œ
@app.route('/sync_to_pinecone', methods=['POST'])
def sync_to_pinecone():
    try:
        data = request.get_json()
        seq = data.get('seq')
        mode = data.get('mode', 'upsert')

        logging.info(f"ë™ê¸°í™” ìš”ì²­ ìˆ˜ì‹ : seq={seq}, mode={mode}")
        
        if not seq:
            logging.warning("seq ëˆ„ë½")
            return jsonify({"success": False, "error": "seqê°€ í•„ìš”í•©ë‹ˆë‹¤"}), 400
        
        if not isinstance(seq, int):
            seq = int(seq)
        
        result = sync_manager.sync_to_pinecone(seq, mode)

        logging.info(f"ë™ê¸°í™” ê²°ê³¼: {result}")
        
        status_code = 200 if result["success"] else 500
        return jsonify(result), status_code
        
    except ValueError as e:
        logging.error(f"ì˜ëª»ëœ seq ê°’: {str(e)}")
        return jsonify({"success": False, "error": f"ì˜ëª»ëœ seq ê°’: {str(e)}"}), 400
    except Exception as e:
        logging.error(f"Pinecone ë™ê¸°í™” API ì˜¤ë¥˜: {str(e)}")
        return jsonify({"success": False, "error": str(e)}), 500

# â˜… ì‹œìŠ¤í…œ ìƒíƒœ í™•ì¸ì„ ìœ„í•œ í—¬ìŠ¤ì²´í¬ API ì—”ë“œí¬ì¸íŠ¸
# ë¡œë“œë°¸ëŸ°ì„œë‚˜ ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œì—ì„œ í˜¸ì¶œí•˜ì—¬ ì„œë²„ ìƒíƒœ í™•ì¸
@app.route('/health', methods=['GET'])
def health_check():
    try:
        stats = index.describe_index_stats()
        
        return jsonify({
            "status": "healthy",
            "pinecone_vectors": stats.get('total_vector_count', 0),
            "timestamp": datetime.now().isoformat(),
            "services": {
                "ai_answer": "active",
                "pinecone_sync": "active",
                "multilingual_support": "active"  # ë‹¤êµ­ì–´ ì§€ì› ìƒíƒœ ì¶”ê°€
            }
        }), 200
    except Exception as e:
        return jsonify({
            "status": "unhealthy",
            "error": str(e)
        }), 500

# ==================================================
# 11. ë©”ì¸ ì‹¤í–‰ ë¶€ë¶„
# ==================================================
# Flask ì›¹ ì„œë²„ ì‹œì‘ì 
# 
# ì´ ë¶€ë¶„ì€ ì´ íŒŒì¼(ìŠ¤í¬ë¦½íŠ¸)ê°€ ì§ì ‘ ì‹¤í–‰ë  ë•Œë§Œ ì‹¤í–‰ë˜ëŠ” ì ˆì°¨ì  ì½”ë“œ
# ë‹¤ë¥¸ ëª¨ë“ˆì—ì„œ importí•  ë•ŒëŠ” ì‹¤í–‰ë˜ì§€ ì•ŠìŒ
# 
# ì„¤ì •:
# - í¬íŠ¸: í™˜ê²½ë³€ìˆ˜ FLASK_PORT ë˜ëŠ” ê¸°ë³¸ê°’ 8000
# - í˜¸ìŠ¤íŠ¸: 0.0.0.0 (ëª¨ë“  IPì—ì„œ ì ‘ê·¼ ê°€ëŠ¥)
# - ë””ë²„ê·¸: False (ìš´ì˜ ëª¨ë“œ)
# - ìŠ¤ë ˆë“œ: True (ë©€í‹°ìŠ¤ë ˆë”© ì§€ì›)
if __name__ == "__main__":
    
    # í™˜ê²½ë³€ìˆ˜ì—ì„œ í¬íŠ¸ ì„¤ì • ë¡œë“œ (ê¸°ë³¸ê°’: 8000)
    port = int(os.getenv('FLASK_PORT', 8000))
    
    # ì‹œì‘ ë©”ì‹œì§€ ì¶œë ¥
    print("="*60)
    print("ğŸš€ GOODTV ë°”ì´ë¸” ì• í”Œ AI ë‹µë³€ ìƒì„± ì„œë²„ ì‹œì‘")
    print("="*60)
    print(f"ğŸ“¡ ì„œë²„ í¬íŠ¸: {port}")
    print(f"ğŸ¤– AI ëª¨ë¸: {GPT_MODEL} (Enhanced Context Mode)")
    print(f"ğŸ” ì„ë² ë”© ëª¨ë¸: {MODEL_NAME}")
    print(f"ğŸ—ƒï¸  ë²¡í„° DB: Pinecone ({INDEX_NAME})")
    print(f"ğŸŒ ë‹¤êµ­ì–´ ì§€ì›: í•œêµ­ì–´(ko), ì˜ì–´(en)")
    print("ğŸ”§ ì œê³µ ì„œë¹„ìŠ¤:")
    print("   â”œâ”€â”€ AI ë‹µë³€ ìƒì„± (/generate_answer)")
    print("   â”œâ”€â”€ Pinecone ë™ê¸°í™” (/sync_to_pinecone)")
    print("   â””â”€â”€ í—¬ìŠ¤ì²´í¬ (/health)")
    print("="*60)
    
    # Flask ì›¹ ì„œë²„ ì‹œì‘
    # host='0.0.0.0': ëª¨ë“  ë„¤íŠ¸ì›Œí¬ ì¸í„°í˜ì´ìŠ¤ì—ì„œ ì ‘ê·¼ í—ˆìš©
    # debug=False: ìš´ì˜ ëª¨ë“œ (ë³´ì•ˆìƒ ì¤‘ìš”)
    # threaded=True: ë©€í‹°ìŠ¤ë ˆë”© í™œì„±í™”, ë™ì‹œ ìš”ì²­ ì²˜ë¦¬ ê°€ëŠ¥
    app.run(host='0.0.0.0', port=port, debug=False, threaded=True)