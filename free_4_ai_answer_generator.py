#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
=== AI ë‹µë³€ ìƒì„± Flask API ì„œë²„ (ë‹¤êµ­ì–´ ì§€ì›) ===
íŒŒì¼ëª…: free_4_ai_answer_generator.py
ëª©ì : ASP Classicì—ì„œ í˜¸ì¶œí•˜ëŠ” AI ë‹µë³€ ìƒì„± API + Pinecone ë²¡í„°DB ë™ê¸°í™”
ì£¼ìš” ê¸°ëŠ¥:
1. OpenAI GPT-3.5-turboë¥¼ ì´ìš©í•œ ìì—°ì–´ ë‹µë³€ ìƒì„± (í•œêµ­ì–´/ì˜ì–´)
2. Pinecone ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ìœ ì‚¬ ë‹µë³€ ê²€ìƒ‰
3. MSSQL ë°ì´í„°ë² ì´ìŠ¤ì™€ Pinecone ë™ê¸°í™”
4. ë©”ëª¨ë¦¬ ìµœì í™” ë° ëª¨ë‹ˆí„°ë§
5. ë‹¤êµ­ì–´ ì§€ì› (í•œêµ­ì–´, ì˜ì–´)
"""

# ==================================================
# 1. í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸ êµ¬ê°„
# ==================================================
# ê¸°ë³¸ Python ëª¨ë“ˆë“¤
import os                   # í™˜ê²½ë³€ìˆ˜ ë° íŒŒì¼ ì‹œìŠ¤í…œ ì‘ì—…
import sys                  # ì‹œìŠ¤í…œ ê´€ë ¨ ê¸°ëŠ¥
import json                 # JSON ë°ì´í„° ì²˜ë¦¬
import json as json_module  # JSON ëª¨ë“ˆì˜ ë³„ì¹­ (ì¼ë¦¬ì•„ìŠ¤, ì½”ë“œ ë‚´ ì¤‘ë³µ ë°©ì§€)
import re                   # ì •ê·œí‘œí˜„ì‹ íŒ¨í„´ ë§¤ì¹­
import html                 # HTML ì—”í‹°í‹° ì²˜ë¦¬
import unicodedata          # ìœ ë‹ˆì½”ë“œ ë¬¸ì ì •ê·œí™”
import logging              # ë¡œê·¸ ê¸°ë¡ ì‹œìŠ¤í…œ
import gc                   # ê°€ë¹„ì§€ ì»¬ë ‰ì…˜ (ë©”ëª¨ë¦¬ ê´€ë¦¬)

# ì›¹ í”„ë ˆì„ì›Œí¬ ê´€ë ¨
from flask import Flask, request, jsonify  # Flask ì›¹ í”„ë ˆì„ì›Œí¬
from flask_cors import CORS                 # CORS(Cross-Origin Resource Sharing) ì²˜ë¦¬

# AI ë° ë°ì´í„°ë² ì´ìŠ¤ ê´€ë ¨
from pinecone import Pinecone      # Pinecone ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤
import openai                      # OpenAI API í´ë¼ì´ì–¸íŠ¸
import pyodbc                      # MSSQL ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²°

# í™˜ê²½ì„¤ì • ë° ìœ í‹¸ë¦¬í‹°
from dotenv import load_dotenv     # .env íŒŒì¼ì—ì„œ í™˜ê²½ë³€ìˆ˜ ë¡œë“œ
from datetime import datetime      # ë‚ ì§œ/ì‹œê°„ ì²˜ë¦¬
from typing import Optional, Dict, Any, List  # íƒ€ì… íŒíŒ…
import re                                     # ì •ê·œí‘œí˜„ì‹

# ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ê´€ë ¨
from memory_profiler import profile                  # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ í”„ë¡œíŒŒì¼ë§
import tracemalloc                                   # ë©”ëª¨ë¦¬ ì¶”ì 
import threading                                     # ë©€í‹°ìŠ¤ë ˆë”©
from contextlib import contextmanager                # ì»¨í…ìŠ¤íŠ¸ ë§¤ë‹ˆì € (withë¬¸ ì‚¬ìš©)
from langdetect import detect, LangDetectException   # ì–¸ì–´ ê°ì§€

# ==================================================
# 2. ì‹œìŠ¤í…œ ì´ˆê¸°í™” ë° ì„¤ì •
# ==================================================
# ë©”ëª¨ë¦¬ ì¶”ì  ì‹œì‘ - ë©”ëª¨ë¦¬ ëˆ„ìˆ˜ ë° ì‚¬ìš©ëŸ‰ì„ ëª¨ë‹ˆí„°ë§í•˜ê¸° ìœ„í•¨ (ì´ ì‹œì ë¶€í„° ëª¨ë“  ë©”ëª¨ë¦¬ í• ë‹¹ì´ ê¸°ë¡ë˜ì–´ ë‚˜ì¤‘ì— ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ë¶„ì„ì´ ê°€ëŠ¥í•´ì§)
tracemalloc.start() 

# Flask ì›¹ ì• í”Œë¦¬ì¼€ì´ì…˜ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
# __name__: í˜„ì¬ ëª¨ë“ˆëª…ì„ ì „ë‹¬í•˜ì—¬ Flaskê°€ ë¦¬ì†ŒìŠ¤ ìœ„ì¹˜ë¥¼ ì°¾ì„ ìˆ˜ ìˆê²Œ í•¨
app = Flask(__name__)

# CORS ì„¤ì • - ëª¨ë“  ì—”ë“œí¬ì¸íŠ¸ì—ì„œ cross-origin ìš”ì²­ì„ í—ˆìš© (ASP Classicì—ì„œ í˜¸ì¶œí•˜ê¸° ìœ„í•¨)
CORS(app)

# ==================================================
# 3. ë¡œê¹… ì‹œìŠ¤í…œ ì„¤ì • (ì½˜ì†” + íŒŒì¼)
# ==================================================
# ë¡œê±° ìƒì„±
logger = logging.getLogger()
logger.setLevel(logging.INFO)

# í¬ë§·í„° ìƒì„±
formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')

# íŒŒì¼ í•¸ë“¤ëŸ¬ (ê¸°ì¡´)
try:
    # ë¡œê·¸ ë””ë ‰í† ë¦¬ ìƒì„±
    os.makedirs('/home/ec2-user/python/logs', exist_ok=True)
    file_handler = logging.FileHandler('/home/ec2-user/python/logs/ai_generator.log', encoding='utf-8')
    file_handler.setLevel(logging.INFO)
    file_handler.setFormatter(formatter)
    logger.addHandler(file_handler)
except Exception as e:
    print(f"ë¡œê·¸ íŒŒì¼ í•¸ë“¤ëŸ¬ ìƒì„± ì‹¤íŒ¨: {e}")

# ì½˜ì†” í•¸ë“¤ëŸ¬ ì¶”ê°€ (ì‹¤ì‹œê°„ ë””ë²„ê¹…ìš©)
console_handler = logging.StreamHandler()
console_handler.setLevel(logging.INFO)
console_handler.setFormatter(formatter)
logger.addHandler(console_handler)

# ==================================================
# 4. í™˜ê²½ë³€ìˆ˜ ë¡œë“œ ë° ì‹œìŠ¤í…œ ìƒìˆ˜ ì •ì˜
# ==================================================
# .env íŒŒì¼ì—ì„œ API í‚¤ ë° ë°ì´í„°ë² ì´ìŠ¤ ì„¤ì • ë¡œë“œ
load_dotenv()

# AI ì„ë² ë”© ëª¨ë¸ ë° ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ì„¤ì • ìƒìˆ˜ë“¤
MODEL_NAME = 'text-embedding-3-small'          # OpenAI ì„ë² ë”© ëª¨ë¸ëª…
INDEX_NAME = "bible-app-support-1536-openai"   # Pinecone ì¸ë±ìŠ¤ëª…
EMBEDDING_DIMENSION = 1536                      # ì„ë² ë”© ë²¡í„° ì°¨ì›ìˆ˜
MAX_TEXT_LENGTH = 8000                          # í…ìŠ¤íŠ¸ ìµœëŒ€ ê¸¸ì´ ì œí•œ

# GPT ìì—°ì–´ ëª¨ë¸ ì„¤ì • - ë³´ìˆ˜ì  ì„¤ì •ìœ¼ë¡œ ì¼ê´€ì„± ìˆëŠ” ë‹µë³€ ìƒì„±
GPT_MODEL = 'gpt-3.5-turbo'     # ì‚¬ìš©í•  GPT ëª¨ë¸
MAX_TOKENS = 600                 # ìƒì„±í•  ìµœëŒ€ í† í° ìˆ˜ (ë‹µë³€ ê¸¸ì´ ì œí•œ)
TEMPERATURE = 0.5                # ì°½ì˜ì„± ìˆ˜ì¤€ (ë‚®ì„ìˆ˜ë¡ ì¼ê´€ëœ ë‹µë³€)

# ê³ ê° ë¬¸ì˜ ì¹´í…Œê³ ë¦¬ ë§¤í•‘ í…Œì´ë¸”
# ì¹´í…Œê³ ë¦¬ ë§¤í•‘ ë”•ì…”ë„ˆë¦¬ëŠ” MSSQLì˜ ìˆ«ì ì¸ë±ìŠ¤ë¥¼ ì‚¬ëŒì´ ì½ì„ ìˆ˜ ìˆëŠ” í•œê¸€ ì¹´í…Œê³ ë¦¬ëª…ìœ¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤. 
# ì´ëŠ” Pinecone ë©”íƒ€ë°ì´í„°ì— ì €ì¥ë˜ì–´ ê²€ìƒ‰ ê²°ê³¼ì˜ ê°€ë…ì„±ì„ ë†’ì…ë‹ˆë‹¤.
CATEGORY_MAPPING = {
    '1': 'í›„ì›/í•´ì§€',                   
    '2': 'ì„±ê²½ í†µë…(ì½ê¸°,ë“£ê¸°,ë…¹ìŒ)',   
    '3': 'ì„±ê²½ë‚­ë… ë ˆì´ìŠ¤',             
    '4': 'ê°œì„ /ì œì•ˆ',                   
    '5': 'ì˜¤ë¥˜/ì¥ì• ',               
    '6': 'ë¶ˆë§Œ',                        
    '7': 'ì˜¤íƒˆìì œë³´',                   
    '0': 'ì‚¬ìš© ë¬¸ì˜(ê¸°íƒ€)'               
}

# ì˜ì–´ ì¹´í…Œê³ ë¦¬ ë§¤í•‘ ì¶”ê°€
CATEGORY_MAPPING_EN = {
    '1': 'Sponsorship/Cancellation',
    '2': 'Bible Reading(Read,Listen,Record)',
    '3': 'Bible Reading Race',
    '4': 'Improvement/Suggestion',
    '5': 'Error/Failure',
    '6': 'Complaint',
    '7': 'Typo Report',
    '0': 'Usage Inquiry(Other)'
}

# ==================================================
# 5. ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ ì •ì˜
# ==================================================
# ë©”ëª¨ë¦¬ ì •ë¦¬ë¥¼ ìœ„í•œ ì»¨í…ìŠ¤íŠ¸ ë§¤ë‹ˆì €
# withë¬¸ ì§„ì…ì‹œ try ë¸”ë¡ ì‹¤í–‰
# yieldì—ì„œ ì¼ì‹œì •ì§€í•˜ê³  with ë¸”ë¡ ë‚´ë¶€ ì½”ë“œ ì‹¤í–‰
# yield í‚¤ì›Œë“œê°€ ë“¤ì–´ê°„ ì´ í•¨ìˆ˜ëŠ” ì œë„ˆë ˆì´í„°ì´ë©´ì„œ ë™ì‹œì— ì»¨í…ìŠ¤íŠ¸ ë§¤ë‹ˆì €ë¡œ ë™ì‘
# ì œë„ˆë ˆì´í„° í•¨ìˆ˜ëŠ” ê°’ì„ ë°˜í™˜í•˜ëŠ” ëŒ€ì‹  ê°’ì„ í•˜ë‚˜ì”© ìƒì„±í•˜ì—¬ ë°˜í™˜í•©ë‹ˆë‹¤. (íŒŒì´ì¬ì—ì„œ ëŒ€ìš©ëŸ‰ ë°ì´í„° ì²˜ë¦¬ë¥¼ ìœ„í•´ ì‚¬ìš©)
# with ë¸”ë¡ ì¢…ë£Œì‹œ finallyì—ì„œ gc.collect() ì‹¤í–‰

# ì´ë ‡ê²Œ í•˜ë©´ ëŒ€ìš©ëŸ‰ ë°ì´í„° ì²˜ë¦¬ í›„ ìë™ìœ¼ë¡œ ê°€ë¹„ì§€ ì»¬ë ‰ì…˜ì´ ì‹¤í–‰ë©ë‹ˆë‹¤.
@contextmanager
def memory_cleanup():
    try:
        yield  # with ë¸”ë¡ ë‚´ë¶€ ì½”ë“œ ì‹¤í–‰
    finally:
        gc.collect()  # ê°€ë¹„ì§€ ì»¬ë ‰ì…˜ ê°•ì œ ì‹¤í–‰ìœ¼ë¡œ ë©”ëª¨ë¦¬ ì •ë¦¬

# ==================================================
# 6. ì™¸ë¶€ ì„œë¹„ìŠ¤ ì—°ê²° ë° ì´ˆê¸°í™”
# ==================================================
try:
    # Pinecone ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì„¤ì •
    # ìœ ì‚¬ ë‹µë³€ ê²€ìƒ‰ì„ ìœ„í•œ ë²¡í„°DB - ì„ë² ë”©ëœ ì§ˆë¬¸/ë‹µë³€ ì €ì¥ì†Œ
    pc = Pinecone(api_key=os.getenv('PINECONE_API_KEY'))
    index = pc.Index(INDEX_NAME)
    
    # OpenAI API í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
    # GPT ëª¨ë¸ ë° ì„ë² ë”© ìƒì„±ì„ ìœ„í•œ í´ë¼ì´ì–¸íŠ¸
    # openai.api_key = ... ë°©ì‹ë³´ë‹¤ ê°ì²´ì§€í–¥ì ìœ¼ë¡œ ì„¤ê³„
    openai_client = openai.OpenAI(api_key=os.getenv('OPENAI_API_KEY'))
    
    # MSSQL ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì„¤ì •
    # ê¸°ì¡´ ê³ ê° ë¬¸ì˜ ë°ì´í„°ê°€ ì €ì¥ëœ ìš´ì˜ DB ì—°ê²°ì„ ìœ„í•œ ì„¤ì •
    mssql_config = {
        'server': os.getenv('MSSQL_SERVER'),       # DB ì„œë²„ ì£¼ì†Œ
        'database': os.getenv('MSSQL_DATABASE'),   # ë°ì´í„°ë² ì´ìŠ¤ëª…
        'username': os.getenv('MSSQL_USERNAME'),   # DB ì‚¬ìš©ìëª…
        'password': os.getenv('MSSQL_PASSWORD')    # DB ë¹„ë°€ë²ˆí˜¸
    }
    
    # MSSQL Server ì—°ê²° ë¬¸ìì—´ êµ¬ì„±
    # MSSQL Server í‘œì¤€ ODBC ë“œë¼ì´ë²„ë¥¼ ì‚¬ìš©í•œ ì—°ê²° ë¬¸ìì—´
    connection_string = (
            f"DRIVER={{ODBC Driver 17 for SQL Server}};"  # ODBC ë“œë¼ì´ë²„ ë²„ì „
            f"SERVER={mssql_config['server']},1433;"      # ì„œë²„ ì£¼ì†Œì™€ í¬íŠ¸
            f"DATABASE={mssql_config['database']};"       # ë°ì´í„°ë² ì´ìŠ¤ëª…
            f"UID={mssql_config['username']};"            # ì‚¬ìš©ì ID
            f"PWD={mssql_config['password']};"            # ë¹„ë°€ë²ˆí˜¸
            f"TrustServerCertificate=yes;"                # SSL ì¸ì¦ì„œ ì‹ ë¢°
            f"Connection Timeout=30;"                     # ì—°ê²° íƒ€ì„ì•„ì›ƒ (30ì´ˆ)
    )

except Exception as e:
    # ì´ˆê¸°í™” ì‹¤íŒ¨ ì‹œ ë¡œê·¸ ê¸°ë¡ í›„ í”„ë¡œê·¸ë¨ ì¢…ë£Œ
    logging.error(f"ëª¨ë“ˆ ë¡œë“œ ì‹¤íŒ¨: {str(e)}")
    app.logger.error(f"ëª¨ë“ˆ ë¡œë“œ ì‹¤íŒ¨: {str(e)}")
    raise  # ì˜ˆì™¸ë¥¼ ë‹¤ì‹œ ë°œìƒì‹œì¼œ í”„ë¡œê·¸ë¨ ì¤‘ë‹¨

# ==================================================
# 7. AI ë‹µë³€ ìƒì„± ë©”ì¸ í´ë˜ìŠ¤ (ê°ì²´ ì§€í–¥ í”„ë¡œê·¸ë˜ë°, ë‹¤êµ­ì–´ ì§€ì› ì¶”ê°€)
# ==================================================
    # AI ë‹µë³€ ìƒì„±ì„ ë‹´ë‹¹í•˜ëŠ” ë©”ì¸ í´ë˜ìŠ¤
    # ê°ì²´ì§€í–¥ ì„¤ê³„ë¡œ ê´€ë ¨ ê¸°ëŠ¥ë“¤ì„ í•˜ë‚˜ì˜ í´ë˜ìŠ¤ì— ìº¡ìŠí™”
    
    # ì£¼ìš” ê¸°ëŠ¥:
    # 1. í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ ë° ì •ì œ
    # 2. OpenAIë¥¼ ì´ìš©í•œ ì„ë² ë”© ìƒì„±
    # 3. Pineconeì—ì„œ ìœ ì‚¬ ë‹µë³€ ê²€ìƒ‰
    # 4. GPTë¥¼ ì´ìš©í•œ ë§ì¶¤í˜• ë‹µë³€ ìƒì„±
    # 5. í•œêµ­ì–´ í…ìŠ¤íŠ¸ ê²€ì¦ ë° í¬ë§·íŒ…

class AIAnswerGenerator:
    
    # í´ë˜ìŠ¤ ì´ˆê¸°í™” ë©”ì„œë“œ
    # OpenAI í´ë¼ì´ì–¸íŠ¸ë¥¼ ì¸ìŠ¤í„´ìŠ¤ ë³€ìˆ˜ë¡œ ì„¤ì •. ì´ëŠ” ì˜ì¡´ì„± ì£¼ì… íŒ¨í„´ì˜ ê°„ì†Œí™”ëœ í˜•íƒœ
    def __init__(self):
        self.openai_client = openai_client

    # â˜† í•œêµ­ì–´ ì˜¤íƒ€ ìˆ˜ì • ë©”ì„œë“œ
    def fix_korean_typos_with_ai(self, text: str) -> str:
        if not text or len(text.strip()) < 3:
            return text
        
        # ë„ˆë¬´ ê¸´ í…ìŠ¤íŠ¸ëŠ” ì²˜ë¦¬í•˜ì§€ ì•ŠìŒ (ë¹„ìš© ì ˆì•½)
        if len(text) > 500:
            logging.warning(f"í…ìŠ¤íŠ¸ê°€ ë„ˆë¬´ ê¸¸ì–´ ì˜¤íƒ€ ìˆ˜ì • ê±´ë„ˆëœ€: {len(text)}ì")
            return text
        
        try:
            with memory_cleanup():
                system_prompt = """ë‹¹ì‹ ì€ í•œêµ­ì–´ ë§ì¶¤ë²• ë° ì˜¤íƒ€ êµì • ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

ì§€ì¹¨:
1. ì…ë ¥ëœ í•œêµ­ì–´ í…ìŠ¤íŠ¸ì˜ ë§ì¶¤ë²•ê³¼ ì˜¤íƒ€ë§Œ ìˆ˜ì •í•˜ì„¸ìš”
2. ì›ë¬¸ì˜ ì˜ë¯¸ì™€ ì–´ì¡°ëŠ” ì ˆëŒ€ ë³€ê²½í•˜ì§€ ë§ˆì„¸ìš”
3. ë„ì–´ì“°ê¸°, ë§ì¶¤ë²•, ì¡°ì‚¬ ì‚¬ìš©ë²•ì„ ì •í™•íˆ êµì •í•˜ì„¸ìš”
4. ì•±/ì–´í”Œë¦¬ì¼€ì´ì…˜ ê´€ë ¨ ê¸°ìˆ  ìš©ì–´ëŠ” í‘œì¤€ ìš©ì–´ë¡œ í†µì¼í•˜ì„¸ìš”
5. ìˆ˜ì •ì´ í•„ìš”ì—†ë‹¤ë©´ ì›ë¬¸ ê·¸ëŒ€ë¡œ ë°˜í™˜í•˜ì„¸ìš”
6. ìˆ˜ì •ëœ í…ìŠ¤íŠ¸ë§Œ ë°˜í™˜í•˜ê³  ì¶”ê°€ ì„¤ëª…ì€ í•˜ì§€ ë§ˆì„¸ìš”

ì˜ˆì‹œ:
- "ì–´í”Œì´ ì•ˆë€ë‹¤" â†’ "ì•±ì´ ì•ˆ ë¼ìš”"
- "ë‹¤ìš´ë°›ê¸°ê°€ ì•ˆë˜ìš”" â†’ "ë‹¤ìš´ë¡œë“œê°€ ì•ˆ ë¼ìš”"
- "ì‚­ì¬í•˜ê³ ì‹¶ì–´ìš”" â†’ "ì‚­ì œí•˜ê³  ì‹¶ì–´ìš”"
- "ì—…ë°ì´ë“œí•´ì£¼ì„¸ìš”" â†’ "ì—…ë°ì´íŠ¸í•´ì£¼ì„¸ìš”"
"""

                user_prompt = f"ë‹¤ìŒ í…ìŠ¤íŠ¸ì˜ ë§ì¶¤ë²•ê³¼ ì˜¤íƒ€ë¥¼ ìˆ˜ì •í•´ì£¼ì„¸ìš”:\n\n{text}"

                response = self.openai_client.chat.completions.create(
                    model='gpt-3.5-turbo',
                    messages=[
                        {"role": "system", "content": system_prompt},
                        {"role": "user", "content": user_prompt}
                    ],
                    max_tokens=600,
                    temperature=0.1,  # ë§¤ìš° ë³´ìˆ˜ì ìœ¼ë¡œ ì„¤ì •
                    top_p=0.8,
                    frequency_penalty=0.0,
                    presence_penalty=0.0
                )
                
                corrected_text = response.choices[0].message.content.strip()
                del response # ë©”ëª¨ë¦¬ í•´ì œ
                
                # ê²°ê³¼ ê²€ì¦
                if not corrected_text or len(corrected_text) == 0:
                    logging.warning("AI ì˜¤íƒ€ ìˆ˜ì • ê²°ê³¼ê°€ ë¹„ì–´ìˆìŒ, ì›ë¬¸ ë°˜í™˜")
                    return text
                
                # ë„ˆë¬´ ë§ì´ ë³€ê²½ëœ ê²½ìš° ì˜ì‹¬ìŠ¤ëŸ¬ìš°ë¯€ë¡œ ì›ë¬¸ ë°˜í™˜
                if len(corrected_text) > len(text) * 2:
                    logging.warning("AI ì˜¤íƒ€ ìˆ˜ì • ê²°ê³¼ê°€ ì›ë¬¸ë³´ë‹¤ ë„ˆë¬´ ê¸¸ì–´ì§, ì›ë¬¸ ë°˜í™˜")
                    return text
                
                # ìˆ˜ì • ë‚´ìš©ì´ ìˆìœ¼ë©´ ë¡œê·¸ ê¸°ë¡
                if corrected_text != text:
                    logging.info(f"AI ì˜¤íƒ€ ìˆ˜ì •: '{text[:50]}...' â†’ '{corrected_text[:50]}...'")
                
                return corrected_text
                
        except Exception as e:
            logging.error(f"AI ì˜¤íƒ€ ìˆ˜ì • ì‹¤íŒ¨: {e}")
            # AI ì‹¤íŒ¨ ì‹œ ì›ë¬¸ ê·¸ëŒ€ë¡œ ë°˜í™˜
            return text
    
    # â˜† í…ìŠ¤íŠ¸ì˜ ì–¸ì–´ë¥¼ ê°ì§€í•˜ëŠ” ë©”ì„œë“œ
    def detect_language(self, text: str) -> str:
        try:
            # langdetect ë¼ì´ë¸ŒëŸ¬ë¦¬ ì‚¬ìš©
            detected = detect(text)
            
            # ì˜ì–´ì™€ í•œêµ­ì–´ë§Œ ì§€ì›
            if detected == 'en':
                return 'en'
            elif detected == 'ko':
                return 'ko'
            else:
                # ê¸°ë³¸ê°’ì€ í•œêµ­ì–´
                return 'ko'
        except LangDetectException:
            # ê°ì§€ ì‹¤íŒ¨ì‹œ í…ìŠ¤íŠ¸ ë‚´ í•œê¸€ ë¹„ìœ¨ë¡œ íŒë‹¨
            korean_chars = len(re.findall(r'[ê°€-í£]', text))
            english_chars = len(re.findall(r'[a-zA-Z]', text))
            
            if korean_chars > english_chars:
                return 'ko'
            else:
                return 'en'

    # â˜† ì…ë ¥ í…ìŠ¤íŠ¸ë¥¼ AI ì²˜ë¦¬ì— ì í•©í•˜ê²Œ ì „ì²˜ë¦¬í•˜ëŠ” ë©”ì„œë“œ (ì›ë³¸ í…ìŠ¤íŠ¸ -> ì •ì œëœ í…ìŠ¤íŠ¸)
    def preprocess_text(self, text: str) -> str:
        logging.info(f"ì „ì²˜ë¦¬ ì‹œì‘: ì…ë ¥ ê¸¸ì´={len(text) if text else 0}")
        logging.info(f"ì „ì²˜ë¦¬ ì…ë ¥ ë¯¸ë¦¬ë³´ê¸°: {text[:100] if text else 'None'}...")

        # null ì²´í¬
        if not text:
            logging.info("ì „ì²˜ë¦¬: ë¹ˆ í…ìŠ¤íŠ¸ ì…ë ¥")
            return ""
        
        # ë¬¸ìì—´ë¡œ ë³€í™˜ ë° HTML ì—”í‹°í‹° ë””ì½”ë”©
        text = str(text)
        text = html.unescape(text)  # &amp; â†’ &, &lt; â†’ < ë“±
        logging.info(f"HTML ë””ì½”ë”© í›„ ê¸¸ì´: {len(text)}")
        
        # HTML íƒœê·¸ ì œê±° ë°ë° í…ìŠ¤íŠ¸ í˜•íƒœë¡œ ë³€í™˜ (êµ¬ì¡° ìœ ì§€)
        text = re.sub(r'<br\s*/?>', '\n', text, flags=re.IGNORECASE)      # <br> â†’ ì¤„ë°”ê¿ˆ
        text = re.sub(r'</p>', '\n\n', text, flags=re.IGNORECASE)         # </p> â†’ ë‹¨ë½ êµ¬ë¶„
        text = re.sub(r'<p[^>]*>', '\n', text, flags=re.IGNORECASE)       # <p> â†’ ì¤„ë°”ê¿ˆ
        text = re.sub(r'<li[^>]*>', '\nâ€¢ ', text, flags=re.IGNORECASE)    # <li> â†’ ë¶ˆë¦¿í¬ì¸íŠ¸
        text = re.sub(r'</li>', '', text, flags=re.IGNORECASE)            # </li> ì œê±°
        text = re.sub(r'<[^>]+>', '', text)                               # ë‚˜ë¨¸ì§€ HTML íƒœê·¸ ëª¨ë‘ ì œê±°
        logging.info(f"HTML íƒœê·¸ ì œê±° í›„ ê¸¸ì´: {len(text)}")
        
        # ğŸ”¥ êµ¬ ì•± ì´ë¦„ì„ ë°”ì´ë¸” ì• í”Œë¡œ êµì²´ (ì „ì²˜ë¦¬ ë‹¨ê³„ì—ì„œ)
        # ì¤‘ë³µ ë°©ì§€ë¥¼ ìœ„í•´ ìˆœì„œë¥¼ ì¡°ì •: ì „ì²´ íŒ¨í„´ë¶€í„° ì²˜ë¦¬
        text = re.sub(r'ë°”ì´ë¸”\s*ì• í”Œ\s*\(êµ¬\)\s*ë‹¤ë²ˆì—­\s*ì„±ê²½\s*ì°¬ì†¡', 'ë°”ì´ë¸” ì• í”Œ', text, flags=re.IGNORECASE)
        text = re.sub(r'ë°”ì´ë¸”\s*ì• í”Œ\s*\(êµ¬\)\s*ë‹¤ë²ˆì—­ì„±ê²½ì°¬ì†¡', 'ë°”ì´ë¸” ì• í”Œ', text, flags=re.IGNORECASE)
        text = re.sub(r'\(êµ¬\)\s*ë‹¤ë²ˆì—­\s*ì„±ê²½\s*ì°¬ì†¡', 'ë°”ì´ë¸” ì• í”Œ', text, flags=re.IGNORECASE)
        text = re.sub(r'\(êµ¬\)\s*ë‹¤ë²ˆì—­ì„±ê²½ì°¬ì†¡', 'ë°”ì´ë¸” ì• í”Œ', text, flags=re.IGNORECASE)
        text = re.sub(r'ë‹¤ë²ˆì—­\s*ì„±ê²½\s*ì°¬ì†¡', 'ë°”ì´ë¸” ì• í”Œ', text, flags=re.IGNORECASE)
        text = re.sub(r'ë‹¤ë²ˆì—­ì„±ê²½ì°¬ì†¡', 'ë°”ì´ë¸” ì• í”Œ', text, flags=re.IGNORECASE)
        
        # ê³µë°± ë° ì¤„ë°”ê¿ˆ ì •ê·œí™” - ì¼ê´€ëœ í˜•íƒœë¡œ ë³€í™˜
        text = re.sub(r'\n{3,}', '\n\n', text)    # 3ê°œ ì´ìƒ ì¤„ë°”ê¿ˆ â†’ 2ê°œë¡œ ì œí•œ
        text = re.sub(r'[ \t]+', ' ', text)       # ì—°ì† ê³µë°±/íƒ­ â†’ ë‹¨ì¼ ê³µë°±
        text = text.strip()                       # ì•ë’¤ ê³µë°± ì œê±°
        
        logging.info(f"ì „ì²˜ë¦¬ ì™„ë£Œ: ìµœì¢… ê¸¸ì´={len(text)}")
        logging.info(f"ì „ì²˜ë¦¬ ê²°ê³¼ ë¯¸ë¦¬ë³´ê¸°: {text[:100]}...")
        
        return text

    # â˜† JSON ë¬¸ìì—´ ì´ìŠ¤ì¼€ì´í”„ ì²˜ë¦¬ (íŠ¹ìˆ˜ë¬¸ìê°€ í¬í•¨ëœ í…ìŠ¤íŠ¸ë¥¼ JSONìœ¼ë¡œ ì•ˆì „í•˜ê²Œ ë³€í™˜)
    def escape_json_string(self, text: str) -> str:
        
        if not text:
            return ""
        escaped = json_module.dumps(text, ensure_ascii=False) # ensure_ascii=False: í•œê¸€ ê¹¨ì§ ë°©ì§€
        return escaped[1:-1]  # ì•ë’¤ ë”°ì˜´í‘œ ì œê±°

    # â˜† OpenAI APIë¥¼ ì‚¬ìš©í•˜ì—¬ í…ìŠ¤íŠ¸ë¥¼ ë²¡í„°ë¡œ ë³€í™˜í•˜ëŠ” ë©”ì„œë“œ
    # ë²¡í„° ì„ë² ë”©: í…ìŠ¤íŠ¸ì˜ ì˜ë¯¸ë¥¼ ìˆ˜ì¹˜ ë°°ì—´ë¡œ í‘œí˜„í•˜ì—¬ ìœ ì‚¬ë„ ê³„ì‚° ê°€ëŠ¥
    def create_embedding(self, text: str) -> Optional[list]:

        # ë¹ˆ ë¬¸ìì—´ë¿ë§Œ ì•„ë‹ˆë¼ ê³µë°±ë§Œ ìˆëŠ” ë¬¸ìì—´ë„ ê±¸ëŸ¬ëƒ„ (ì´ëŸ° ê²½ìš° JSON ë³€í™˜ ë¶ˆê°€)
        if not text or not text.strip():
            return None
            
        try:
            with memory_cleanup(): # ë©”ëª¨ë¦¬ ëˆ„ìˆ˜ ë°©ì§€ (ë¸”ë¡ ì¢…ë£Œ ì‹œ ê°€ë¹„ì§€ ì»¬ë ‰ì…˜)
                # OpenAI Embedding API í˜¸ì¶œ
                response = self.openai_client.embeddings.create(
                    model='text-embedding-3-small',    # ë²¡í„° ì„ë² ë”© ëª¨ë¸ëª…
                    input=text[:8000]                  # í…ìŠ¤íŠ¸ ê¸¸ì´ ì œí•œ (í† í° ì œí•œ ë°©ì§€ì§€)
                )
                
                # ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±ì„ ìœ„í•´ ë²¡í„°ë§Œ ë³µì‚¬ í›„ ì‘ë‹µ ê°ì²´ ì‚­ì œ
                embedding = response.data[0].embedding.copy() # ì„ë² ë”© ë²¡í„°ë§Œ ë³µì‚¬í•˜ì—¬ ë°˜í™˜ (ê¹Šì€ ë³µì‚¬ë¡œ ë…ë¦½ì ì¸ ë¦¬ìŠ¤íŠ¸ ìƒì„±)
                del response  # ì›ë³¸ ì‘ë‹µ ê°ì²´ ì¦‰ì‹œ ì‚­ì œ (ë©”ëª¨ë¦¬ í•´ì œ)
                return embedding
                
        except Exception as e:
            logging.error(f"ì„ë² ë”© ìƒì„± ì‹¤íŒ¨: {e}")
            return None

    # â˜† Pinecone ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ìœ ì‚¬í•œ ë‹µë³€ì„ ê²€ìƒ‰í•˜ëŠ” ë©”ì„œë“œ
    # Args:
    #     query (str): ê²€ìƒ‰í•  ì§ˆë¬¸
    #     top_k (int): ê²€ìƒ‰í•  ìµœëŒ€ ê°œìˆ˜ (ê¸°ë³¸ê°’: 5)
    #     similarity_threshold (float): ìœ ì‚¬ë„ ì„ê³„ê°’ (ê¸°ë³¸ê°’: 0.3)
            
    # Returns:
    #     list: ìœ ì‚¬ ë‹µë³€ ë¦¬ìŠ¤íŠ¸ [{'score': float, 'question': str, 'answer': str, ...}, ...]
    def search_similar_answers(self, query: str, top_k: int = 5, similarity_threshold: float = 0.7, lang: str = 'ko') -> list:
        try:
            with memory_cleanup():
                # â˜… ë””ë²„ê¹… ë¡œê·¸ ì¶”ê°€
                logging.info(f"=== ê²€ìƒ‰ ì‹œì‘ ===")
                logging.info(f"ì›ë³¸ ì§ˆë¬¸: {query[:100]}")
                
                # â˜… ì˜¤íƒ€ ìˆ˜ì • ì ìš© (ë™ê¸°í™”ì™€ ë™ì¼í•˜ê²Œ!)
                if lang == 'ko':
                    corrected_query = self.fix_korean_typos_with_ai(query)
                    logging.info(f"ì˜¤íƒ€ ìˆ˜ì • í›„: {corrected_query[:100]}")
                    query_to_embed = corrected_query
                else:
                    query_to_embed = query
                
                # ì„ë² ë”© ìƒì„±
                query_vector = self.create_embedding(query_to_embed)
                
                if query_vector is None:
                    logging.error("ì„ë² ë”© ìƒì„± ì‹¤íŒ¨")
                    return []
                
                # Pinecone ê²€ìƒ‰
                results = index.query(
                    vector=query_vector,
                    top_k=top_k * 2,  # ë” ë§ì´ ê²€ìƒ‰
                    include_metadata=True
                )
                
                logging.info(f"Pinecone ê²€ìƒ‰ ê²°ê³¼: {len(results['matches'])}ê°œ")
                
                # í•œêµ­ì–´ ë²¡í„°ë¡œ ì¶”ê°€ ê²€ìƒ‰ (ì˜ì–´ ì§ˆë¬¸ì¸ ê²½ìš°)
                korean_vector = None  # ì´ˆê¸°í™”í•˜ì—¬ NameError ë°©ì§€
                if lang == 'en':
                    # ì˜ì–´ ì¿¼ë¦¬ë¥¼ í•œêµ­ì–´ë¡œ ë²ˆì—­ í›„ ì„ë² ë”© ìƒì„± (ëˆ„ë½ëœ ë¡œì§ ì¶”ê°€)
                    korean_query = self.translate_text(query_to_embed, 'en', 'ko')
                    korean_vector = self.create_embedding(korean_query)
                    if korean_vector:
                        korean_results = index.query(
                            vector=korean_vector,       # ê²€ìƒ‰í•  ë²¡í„°
                            top_k=3,                    # ë³´ì¡° ê²€ìƒ‰ì€ ì ê²Œ (3ê°œ)
                            include_metadata=True       # ë©”íƒ€ë°ì´í„° í¬í•¨ (ì§ˆë¬¸, ë‹µë³€, ì¹´í…Œê³ ë¦¬ ë“±)
                        )
                        # ê²°ê³¼ ë³‘í•© (ì¤‘ë³µ ì œê±°)
                        seen_ids = set()
                        merged_matches = []
                        for match in results['matches'] + korean_results['matches']:
                            if match['id'] not in seen_ids:
                                seen_ids.add(match['id'])
                                merged_matches.append(match)
                        results['matches'] = sorted(merged_matches, key=lambda x: x['score'], reverse=True)[:top_k]
                
                # 3. ê²°ê³¼ í•„í„°ë§ ë° êµ¬ì¡°í™”
                filtered_results = []
                for i, match in enumerate(results['matches']): # enumerateë¡œ ìˆœìœ„(rank) ìƒì„±
                    score = match['score'] # ìœ ì‚¬ë„ ì ìˆ˜ (0~1, ë†’ì„ìˆ˜ë¡ ìœ ì‚¬)
                    question = match['metadata'].get('question', '')
                    answer = match['metadata'].get('answer', '')
                    category = match['metadata'].get('category', 'ì¼ë°˜')
                    
                    # â˜… ì„ê³„ê°’ ë¡œì§ ëŒ€í­ ì™„í™” - í•­ìƒ ìµœì†Œ 3ê°œëŠ” ë°˜í™˜í•˜ë„ë¡ ìˆ˜ì •
                    include_result = False
                    
                    if score >= similarity_threshold:
                        include_result = True
                        logging.info(f"ì„ê³„ê°’ í†µê³¼: {score:.3f} >= {similarity_threshold:.2f}")
                    elif i < 3:  # ìƒìœ„ 3ê°œëŠ” ë¬´ì¡°ê±´ í¬í•¨
                        include_result = True
                        logging.info(f"ìƒìœ„ {i+1}ë²ˆì§¸ ê²°ê³¼ë¡œ ê°•ì œ í¬í•¨: {score:.3f}")
                    elif score >= 0.3:  # ë§¤ìš° ë‚®ì€ ì„ê³„ê°’ìœ¼ë¡œ ì¶”ê°€ í¬í•¨
                        include_result = True
                        logging.info(f"ë‚®ì€ ì„ê³„ê°’ í†µê³¼: {score:.3f} >= 0.3")
                    
                    if include_result:
                        filtered_results.append({
                            'score': score,
                            'question': question,
                            'answer': answer,
                            'category': category,
                            'rank': i + 1,
                            'lang': 'ko'  # ì›ë³¸ ë°ì´í„°ëŠ” í•œêµ­ì–´
                        })
                        
                        # ë””ë²„ê¹…ì„ ìœ„í•œ ìƒì„¸ ë¡œê¹…
                        logging.info(f"â˜… í¬í•¨ëœ ìœ ì‚¬ ë‹µë³€ #{i+1}: ì ìˆ˜={score:.3f}, ì¹´í…Œê³ ë¦¬={category}, ì–¸ì–´={lang}")
                        logging.info(f"ì°¸ê³  ì§ˆë¬¸: {question[:50]}...")
                        logging.info(f"ì°¸ê³  ë‹µë³€: {answer[:100]}...")
                    else:
                        logging.info(f"Ã— ì œì™¸ëœ ë‹µë³€ #{i+1}: ì ìˆ˜={score:.3f} (ë„ˆë¬´ ë‚®ìŒ)")
                        
                # 4. ë©”ëª¨ë¦¬ ì •ë¦¬
                del results # ì›ë³¸ ì‘ë‹µ ê°ì²´ ì¦‰ì‹œ ì‚­ì œ (ë©”ëª¨ë¦¬ í•´ì œ)
                if korean_vector is not None:
                    del korean_vector # í•œêµ­ì–´ ë²¡í„° ì¦‰ì‹œ ì‚­ì œ (ë©”ëª¨ë¦¬ í•´ì œ)
                del query_vector # ê²€ìƒ‰ ë²¡í„° ì¦‰ì‹œ ì‚­ì œ (ë©”ëª¨ë¦¬ í•´ì œ)
                
                logging.info(f"ì´ {len(filtered_results)}ê°œì˜ ìœ ì‚¬ ë‹µë³€ ê²€ìƒ‰ ì™„ë£Œ (ì–¸ì–´: {lang})")
                return filtered_results
                
        except Exception as e:
            logging.error(f"Pinecone ê²€ìƒ‰ ì‹¤íŒ¨: {str(e)}, query: {query[:50]}..., lang: {lang}")
            return []

    # â˜† GPTë¥¼ ì‚¬ìš©í•œ ë²ˆì—­
    # Args:
    #     text (str): ë²ˆì—­í•  í…ìŠ¤íŠ¸
    #     source_lang (str): ì›ë³¸ ì–¸ì–´
    #     target_lang (str): ë²ˆì—­ ì–¸ì–´
            
    # Returns:
    #     str: ë²ˆì—­ëœ í…ìŠ¤íŠ¸
    def translate_text(self, text: str, source_lang: str, target_lang: str) -> str:
        try:
            # ì–¸ì–´ ë§¤í•‘
            lang_map = {
                'ko': 'Korean',
                'en': 'English'
            }
            
            system_prompt = f"You are a professional translator. Translate the following text from {lang_map[source_lang]} to {lang_map[target_lang]}. Keep the same tone and style. Only provide the translation without any explanation."
            
            response = self.openai_client.chat.completions.create(
                model='gpt-3.5-turbo',
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": text}
                ],
                max_tokens=600,
                temperature=0.5
            )
            
            return response.choices[0].message.content.strip()
            
        except Exception as e:
            logging.error(f"ë²ˆì—­ ì‹¤íŒ¨: {e}")
            return text

    # â˜† AI ê¸°ë°˜ ì§ˆë¬¸ ì˜ë„ ë¶„ì„ ë©”ì„œë“œ (ëŒ€í­ ê°•í™”ëœ ì •í™•ë„)
    def analyze_question_intent(self, query: str) -> dict:
        """AIë¥¼ ì´ìš©í•´ ì§ˆë¬¸ì˜ ì˜ë„ì™€ í•µì‹¬ ë‚´ìš©ì„ ë§¤ìš° ì •í™•í•˜ê²Œ ë¶„ì„"""
        try:
            with memory_cleanup():
                system_prompt = """ë‹¹ì‹ ì€ ë°”ì´ë¸” ì•± ê³ ê° ë¬¸ì˜ ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. 
ê³ ê°ì˜ ì§ˆë¬¸ì„ ë§¤ìš° ì •í™•í•˜ê²Œ ë¶„ì„í•˜ì—¬ ë‹¤ìŒ ì •ë³´ë¥¼ JSON í˜•íƒœë¡œ ë°˜í™˜í•˜ì„¸ìš”:

{
  "intent_type": "ë¬¸ì˜ ìœ í˜•",
  "main_topic": "í•µì‹¬ ì£¼ì œ",
  "sub_topic": "ì„¸ë¶€ ì£¼ì œ",
  "content_type": "ì½˜í…ì¸  ìœ í˜•",
  "language_preference": "ì–¸ì–´ ì„ í˜¸ë„",
  "specific_request": "êµ¬ì²´ì  ìš”ì²­ì‚¬í•­",
  "keywords": ["í•µì‹¬", "í‚¤ì›Œë“œ", "ëª©ë¡"],
  "urgency": "ê¸´ê¸‰ë„",
  "action_type": "ìš”ì²­ í–‰ë™"
}

ğŸ¯ ë°”ì´ë¸” ì•± ì „ìš© ë¶„ì„ ê¸°ì¤€:

ğŸ“š ì½˜í…ì¸  ìœ í˜• êµ¬ë¶„ (ë§¤ìš° ì¤‘ìš”!):
- "ì„±ê²½": ì„±ê²½ ë³¸ë¬¸, êµ¬ì ˆ, ì¥, ì ˆ ê´€ë ¨
- "ì°¬ì†¡": ì°¬ì†¡ê°€, ì°¬ì–‘, hymn, praise ê´€ë ¨  
- "ê¸°ë„": ê¸°ë„ë¬¸, ì£¼ê¸°ë„ë¬¸ ê´€ë ¨
- "ì„¤êµ": ì„¤êµ, ê°•ì˜, ê°•ì—° ê´€ë ¨
- "ì•±ê¸°ëŠ¥": ì•± ìì²´ ê¸°ëŠ¥, ì„¤ì •, ì‚¬ìš©ë²•

ğŸŒ ì–¸ì–´ êµ¬ë¶„ (ì •í™•íˆ!):
- "korean": í•œêµ­ì–´, í•œê¸€ ê´€ë ¨
- "english": ì˜ì–´, English ê´€ë ¨
- "multilingual": ë‹¤êµ­ì–´, ë²ˆì—­ ê´€ë ¨
- "none": ì–¸ì–´ ë¬´ê´€

âš ï¸ ë§¤ìš° ì¤‘ìš”í•œ í‚¤ì›Œë“œ êµ¬ë¶„:
1. "ì°¬ì†¡ê°€" â‰  "ì„±ê²½" (ì™„ì „íˆ ë‹¤ë¥¸ ì½˜í…ì¸ )
2. "ì˜ì–´ ì°¬ì†¡ê°€" = content_type: "ì°¬ì†¡", language_preference: "english"
3. "ì˜ì–´ ì„±ê²½" = content_type: "ì„±ê²½", language_preference: "english"  
4. "ë³µì‚¬" â‰  "ì¬ìƒ" (ì™„ì „íˆ ë‹¤ë¥¸ í–‰ë™)
5. "ê²€ìƒ‰" â‰  "ì„¤ì •" (ì™„ì „íˆ ë‹¤ë¥¸ ê¸°ëŠ¥)

ğŸ“‹ ë¶„ì„ ì˜ˆì‹œ:
- "ì˜ì–´ ì°¬ì†¡ê°€ë„ ìˆë‚˜ìš”?" â†’ content_type: "ì°¬ì†¡", language_preference: "english"
- "NIV ì„±ê²½ ìˆë‚˜ìš”?" â†’ content_type: "ì„±ê²½", language_preference: "english"
- "ë³µì‚¬í•´ì„œ ì›Œë“œë¡œ" â†’ action_type: "ë³µì‚¬", content_type: "ì„±ê²½"
- "ì°¬ì†¡ê°€ ì—°ì†ì¬ìƒ" â†’ action_type: "ì¬ìƒ", content_type: "ì°¬ì†¡"

ğŸš« ì ˆëŒ€ í˜¼ë™í•˜ì§€ ë§ ê²ƒ:
- ì°¬ì†¡ê°€ â‰  ì„±ê²½ë³¸ë¬¸
- ì˜ì–´ â‰  í•œêµ­ì–´  
- ë³µì‚¬ â‰  ì¬ìƒ
- ê²€ìƒ‰ â‰  ì„¤ì •ë³€ê²½"""

                user_prompt = f"ë‹¤ìŒ ë°”ì´ë¸” ì•± ê³ ê° ë¬¸ì˜ë¥¼ ë§¤ìš° ì •í™•í•˜ê²Œ ë¶„ì„í•´ì£¼ì„¸ìš”: {query}"

                response = self.openai_client.chat.completions.create(
                    model='gpt-3.5-turbo',
                    messages=[
                        {"role": "system", "content": system_prompt},
                        {"role": "user", "content": user_prompt}
                    ],
                    max_tokens=400,
                    temperature=0.2  # ë” ë³´ìˆ˜ì ìœ¼ë¡œ ì„¤ì •
                )
                
                result_text = response.choices[0].message.content.strip()
                
                # JSON íŒŒì‹± ì‹œë„
                try:
                    result = json.loads(result_text)
                    logging.info(f"ê°•í™”ëœ AI ì˜ë„ ë¶„ì„ ê²°ê³¼: {result}")
                    return result
                except json.JSONDecodeError:
                    logging.warning(f"JSON íŒŒì‹± ì‹¤íŒ¨, ê¸°ë³¸ê°’ ë°˜í™˜: {result_text}")
                    return {
                        "intent_type": "ì¼ë°˜ë¬¸ì˜",
                        "main_topic": "ê¸°íƒ€",
                        "sub_topic": "ê¸°íƒ€",
                        "content_type": "ì•±ê¸°ëŠ¥",
                        "language_preference": "none",
                        "specific_request": query[:100],
                        "keywords": [query[:20]],
                        "urgency": "medium",
                        "action_type": "ê¸°íƒ€"
                    }
                
        except Exception as e:
            logging.error(f"AI ì˜ë„ ë¶„ì„ ì‹¤íŒ¨: {e}")
            return {
                "intent_type": "ì¼ë°˜ë¬¸ì˜", 
                "main_topic": "ê¸°íƒ€",
                "sub_topic": "ê¸°íƒ€",
                "content_type": "ì•±ê¸°ëŠ¥",
                "language_preference": "none",
                "specific_request": query[:100],
                "keywords": [query[:20]],
                "urgency": "medium",
                "action_type": "ê¸°íƒ€"
            }

    # â˜† ê²€ìƒ‰ëœ ìœ ì‚¬ ë‹µë³€ë“¤ì˜ í’ˆì§ˆì„ ë¶„ì„í•˜ì—¬ ìµœì ì˜ ë‹µë³€ ìƒì„± ì „ëµì„ ê²°ì •í•˜ëŠ” ë©”ì„œë“œ

    # Args:
    #     similar_answers (list): ê²€ìƒ‰ëœ ìœ ì‚¬ ë‹µë³€ ë¦¬ìŠ¤íŠ¸
    #     query (str): ì›ë³¸ ì§ˆë¬¸
    #     
    # Returns:
    #     dict: ë¶„ì„ ê²°ê³¼ ë° ê¶Œì¥ ì ‘ê·¼ ë°©ì‹
    def analyze_context_quality(self, similar_answers: list, query: str) -> dict:
        # ìœ ì‚¬ ë‹µë³€ì´ ì—†ìœ¼ë©´ ê¸°ë³¸ê°’ ë°˜í™˜
        if not similar_answers:
            return {
                'has_good_context': False,
                'best_score': 0.0,
                'recommended_approach': 'fallback',
                'context_summary': 'ìœ ì‚¬ ë‹µë³€ì´ ì—†ìŠµë‹ˆë‹¤.',
                'question_type': 'ì¼ë°˜ë¬¸ì˜',
                'context_relevance': 'none'
            }
        
        # ğŸ”¥ AI ê¸°ë°˜ ì§ˆë¬¸ ì˜ë„ ë¶„ì„ ì¶”ê°€
        question_analysis = self.analyze_question_intent(query)
        question_type = question_analysis.get('intent_type', 'ì¼ë°˜ë¬¸ì˜')
        logging.info(f"AI ë¶„ì„ ê²°ê³¼: {question_analysis}")
        
        # í’ˆì§ˆ ì§€í‘œ ê³„ì‚°
        best_score = similar_answers[0]['score']
        high_quality_count = len([ans for ans in similar_answers if ans['score'] >= 0.7])
        medium_quality_count = len([ans for ans in similar_answers if 0.5 <= ans['score'] < 0.7])
        
        # ğŸ”¥ ì¹´í…Œê³ ë¦¬ ì¼ì¹˜ë„ ë¶„ì„ ì¶”ê°€
        categories = [ans['category'] for ans in similar_answers[:5]]
        category_distribution = {cat: categories.count(cat) for cat in set(categories)}
        
        # ğŸ”¥ ê°•í™”ëœ ë‹¤ë‹¨ê³„ ê´€ë ¨ì„± ê²€ì¦ ì ìš©
        context_relevance = self.check_context_relevance_ai(question_analysis, categories, query, similar_answers[:3])
        logging.info(f"ê°•í™”ëœ ì»¨í…ìŠ¤íŠ¸ ê´€ë ¨ì„±: {context_relevance}")
        
        # ğŸ”¥ ì˜ì‚¬ ê²°ì • íŠ¸ë¦¬ ëŒ€í­ ê°œì„  - ê´€ë ¨ì„±ì„ ìµœìš°ì„ ìœ¼ë¡œ ê³ ë ¤
        if context_relevance == 'irrelevant':
            # ê´€ë ¨ì„±ì´ ì—†ìœ¼ë©´ ë¬´ì¡°ê±´ íŠ¹ë³„ fallback ì²˜ë¦¬
            approach = 'smart_fallback'
            logging.warning(f"ì§ˆë¬¸ ìœ í˜•({question_type})ê³¼ ê²€ìƒ‰ëœ ë‹µë³€ì˜ ê´€ë ¨ì„±ì´ ì—†ì–´ ìŠ¤ë§ˆíŠ¸ í´ë°± ì²˜ë¦¬")
        elif context_relevance == 'high':
            # ê´€ë ¨ì„±ì´ ë†’ìœ¼ë©´ ì ìˆ˜ì— ë”°ë¼ ê²°ì •
            if best_score >= 0.9:
                approach = 'direct_use'
            elif best_score >= 0.7:
                approach = 'gpt_with_strong_context'
            elif best_score >= 0.5:
                approach = 'gpt_with_strong_context'
            else:
                approach = 'gpt_with_weak_context'
        elif context_relevance == 'medium':
            # ê´€ë ¨ì„±ì´ ì¤‘ê°„ì´ë©´ ë” ì‹ ì¤‘í•˜ê²Œ
            if best_score >= 0.85:
                approach = 'direct_use'
            elif best_score >= 0.6:
                approach = 'gpt_with_strong_context'
            else:
                approach = 'gpt_with_weak_context'
        elif context_relevance == 'low':
            # ê´€ë ¨ì„±ì´ ë‚®ìœ¼ë©´ ë§¤ìš° ì‹ ì¤‘í•˜ê²Œ
            if best_score >= 0.9:
                approach = 'gpt_with_weak_context'  # ì§ì ‘ ì‚¬ìš©í•˜ì§€ ì•ŠìŒ
            else:
                approach = 'smart_fallback'
        else:
            approach = 'smart_fallback'
        
        # ë¶„ì„ ê²°ê³¼ êµ¬ì¡°í™”
        analysis = {
            'has_good_context': context_relevance in ['high', 'medium'] and best_score >= 0.4,
            'best_score': best_score,
            'high_quality_count': high_quality_count,
            'medium_quality_count': medium_quality_count,
            'category_distribution': category_distribution,
            'recommended_approach': approach,
            'question_analysis': question_analysis,
            'question_type': question_type,
            'context_relevance': context_relevance,
            'context_summary': f"ì˜ë„: {question_type}, ì£¼ì œ: {question_analysis.get('main_topic', 'N/A')}, ê´€ë ¨ì„±: {context_relevance}, ìµœê³ ì ìˆ˜: {best_score:.3f}"
        }
        
        logging.info(f"í–¥ìƒëœ ì»¨í…ìŠ¤íŠ¸ ë¶„ì„ ê²°ê³¼: {analysis}")
        return analysis

    # â˜† ì§ˆë¬¸ê³¼ ë‹µë³€ì˜ í•µì‹¬ ì£¼ì œë¥¼ ì¶”ì¶œí•˜ëŠ” ë©”ì„œë“œ (ìƒˆë¡œ ì¶”ê°€)
    def extract_core_topic(self, text: str, text_type: str = "question") -> dict:
        """ì§ˆë¬¸ ë˜ëŠ” ë‹µë³€ì—ì„œ í•µì‹¬ ì£¼ì œë¥¼ ì¶”ì¶œ"""
        try:
            with memory_cleanup():
                system_prompt = """ë‹¹ì‹ ì€ ë°”ì´ë¸” ì•± ì½˜í…ì¸  ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
ì…ë ¥ëœ í…ìŠ¤íŠ¸ì—ì„œ í•µì‹¬ ì£¼ì œë¥¼ ì¶”ì¶œí•˜ì—¬ JSON í˜•íƒœë¡œ ë°˜í™˜í•˜ì„¸ìš”:

{
  "content_type": "ì½˜í…ì¸  ìœ í˜•",
  "language_type": "ì–¸ì–´ ìœ í˜•", 
  "action_type": "í–‰ë™ ìœ í˜•",
  "specific_topic": "êµ¬ì²´ì  ì£¼ì œ",
  "confidence": "ì‹ ë¢°ë„ (0.0-1.0)"
}

ğŸ“š ì½˜í…ì¸  ìœ í˜•:
- "bible": ì„±ê²½, ì„±ê²½ë³¸ë¬¸, êµ¬ì ˆ, ì¥ì ˆ
- "hymn": ì°¬ì†¡ê°€, ì°¬ì–‘, ì°¬ì†¡
- "prayer": ê¸°ë„, ê¸°ë„ë¬¸
- "sermon": ì„¤êµ, ê°•ì˜
- "app_function": ì•± ê¸°ëŠ¥, ì„¤ì •
- "other": ê¸°íƒ€

ğŸŒ ì–¸ì–´ ìœ í˜•:
- "korean": í•œêµ­ì–´ ê´€ë ¨
- "english": ì˜ì–´ ê´€ë ¨  
- "mixed": ë‹¤êµ­ì–´ ê´€ë ¨
- "neutral": ì–¸ì–´ ë¬´ê´€

âš¡ í–‰ë™ ìœ í˜•:
- "play": ì¬ìƒ, ë“£ê¸°
- "copy": ë³µì‚¬, ë¶™ì—¬ë„£ê¸°
- "search": ê²€ìƒ‰, ì°¾ê¸°
- "download": ë‹¤ìš´ë¡œë“œ
- "setting": ì„¤ì •, ë³€ê²½
- "inquiry": ë¬¸ì˜, ì§ˆë¬¸
- "other": ê¸°íƒ€

ì¤‘ìš”: ë§¤ìš° ì •í™•í•˜ê²Œ ë¶„ë¥˜í•˜ì„¸ìš”!"""

                user_prompt = f"ë‹¤ìŒ {text_type}ì—ì„œ í•µì‹¬ ì£¼ì œë¥¼ ì¶”ì¶œí•´ì£¼ì„¸ìš”:\n\n{text}"

                response = self.openai_client.chat.completions.create(
                    model='gpt-3.5-turbo',
                    messages=[
                        {"role": "system", "content": system_prompt},
                        {"role": "user", "content": user_prompt}
                    ],
                    max_tokens=300,
                    temperature=0.1
                )
                
                result_text = response.choices[0].message.content.strip()
                
                try:
                    result = json.loads(result_text)
                    logging.info(f"í•µì‹¬ ì£¼ì œ ì¶”ì¶œ ({text_type}): {result}")
                    return result
                except json.JSONDecodeError:
                    return {
                        "content_type": "other",
                        "language_type": "neutral", 
                        "action_type": "other",
                        "specific_topic": text[:50],
                        "confidence": 0.3
                    }
                    
        except Exception as e:
            logging.error(f"í•µì‹¬ ì£¼ì œ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return {
                "content_type": "other",
                "language_type": "neutral",
                "action_type": "other", 
                "specific_topic": text[:50],
                "confidence": 0.0
            }

    # â˜† ì˜ë¯¸ì  ìœ ì‚¬ì„±ì„ ê²€ì¦í•˜ëŠ” ë©”ì„œë“œ (ìƒˆë¡œ ì¶”ê°€)
    def validate_semantic_similarity(self, question_analysis: dict, answer_topics: list, query: str, answers: list) -> dict:
        """ì§ˆë¬¸ê³¼ ë‹µë³€ë“¤ ê°„ì˜ ì˜ë¯¸ì  ìœ ì‚¬ì„±ì„ ë‹¤ê°ë„ë¡œ ê²€ì¦"""
        try:
            # 1. ì½˜í…ì¸  ìœ í˜• ì¼ì¹˜ë„ ê²€ì‚¬
            question_content = question_analysis.get('content_type', 'other')
            question_language = question_analysis.get('language_preference', 'none')
            question_action = question_analysis.get('action_type', 'other')
            
            content_matches = 0
            language_matches = 0
            action_matches = 0
            total_answers = len(answers)
            
            for answer in answers[:5]:  # ìƒìœ„ 5ê°œë§Œ ê²€ì‚¬
                # ë‹µë³€ì˜ í•µì‹¬ ì£¼ì œ ì¶”ì¶œ
                answer_topic = self.extract_core_topic(answer.get('answer', ''), 'answer')
                
                # ì½˜í…ì¸  ìœ í˜• ì¼ì¹˜ ê²€ì‚¬
                if answer_topic['content_type'] == question_content:
                    content_matches += 1
                
                # ì–¸ì–´ ìœ í˜• ì¼ì¹˜ ê²€ì‚¬  
                if question_language != 'none':
                    if answer_topic['language_type'] == question_language or answer_topic['language_type'] == 'neutral':
                        language_matches += 1
                else:
                    language_matches += 1  # ì–¸ì–´ ë¬´ê´€ì¸ ê²½ìš° í•­ìƒ ì¼ì¹˜
                
                # í–‰ë™ ìœ í˜• ì¼ì¹˜ ê²€ì‚¬
                if answer_topic['action_type'] == question_action or answer_topic['action_type'] == 'other':
                    action_matches += 1
            
            # 2. ì¼ì¹˜ë„ ë¹„ìœ¨ ê³„ì‚°
            content_ratio = content_matches / max(total_answers, 1)
            language_ratio = language_matches / max(total_answers, 1)
            action_ratio = action_matches / max(total_answers, 1)
            
            # 3. ì¢…í•© ìœ ì‚¬ì„± ì ìˆ˜ ê³„ì‚° (ê°€ì¤‘ í‰ê· )
            similarity_score = (content_ratio * 0.5 + language_ratio * 0.3 + action_ratio * 0.2)
            
            # 4. íŠ¹ë³„í•œ ë¶ˆì¼ì¹˜ íŒ¨í„´ ê°ì§€
            critical_mismatch = False
            
            # ì°¬ì†¡ê°€ vs ì„±ê²½ ë¶ˆì¼ì¹˜
            if question_content == 'hymn' and content_matches == 0:
                critical_mismatch = True
                logging.warning("ì‹¬ê°í•œ ë¶ˆì¼ì¹˜: ì°¬ì†¡ê°€ ì§ˆë¬¸ì— ì„±ê²½ ë‹µë³€")
                
            # ì˜ì–´ vs í•œêµ­ì–´ ë¶ˆì¼ì¹˜  
            if question_language in ['english', 'korean'] and language_matches == 0:
                critical_mismatch = True
                logging.warning(f"ì‹¬ê°í•œ ë¶ˆì¼ì¹˜: {question_language} ì§ˆë¬¸ì— ë‹¤ë¥¸ ì–¸ì–´ ë‹µë³€")
            
            # ë³µì‚¬ vs ì¬ìƒ ë¶ˆì¼ì¹˜
            if question_action in ['copy', 'play'] and action_matches == 0:
                critical_mismatch = True
                logging.warning(f"ì‹¬ê°í•œ ë¶ˆì¼ì¹˜: {question_action} ìš”ì²­ì— ë‹¤ë¥¸ í–‰ë™ ë‹µë³€")
            
            # 5. ìµœì¢… íŒì •
            if critical_mismatch:
                final_similarity = 'critical_mismatch'
            elif similarity_score >= 0.8:
                final_similarity = 'high'
            elif similarity_score >= 0.6:
                final_similarity = 'medium'
            elif similarity_score >= 0.4:
                final_similarity = 'low'
            else:
                final_similarity = 'very_low'
            
            result = {
                'similarity_score': similarity_score,
                'content_match_ratio': content_ratio,
                'language_match_ratio': language_ratio,
                'action_match_ratio': action_ratio,
                'critical_mismatch': critical_mismatch,
                'final_similarity': final_similarity,
                'analysis_summary': f"ì½˜í…ì¸ ì¼ì¹˜:{content_ratio:.2f}, ì–¸ì–´ì¼ì¹˜:{language_ratio:.2f}, í–‰ë™ì¼ì¹˜:{action_ratio:.2f}"
            }
            
            logging.info(f"ì˜ë¯¸ì  ìœ ì‚¬ì„± ê²€ì¦: {result}")
            return result
            
        except Exception as e:
            logging.error(f"ì˜ë¯¸ì  ìœ ì‚¬ì„± ê²€ì¦ ì‹¤íŒ¨: {e}")
            return {
                'similarity_score': 0.0,
                'final_similarity': 'error',
                'critical_mismatch': True,
                'analysis_summary': f"ê²€ì¦ ì‹¤íŒ¨: {str(e)}"
            }

    # â˜† AI ê¸°ë°˜ ì»¨í…ìŠ¤íŠ¸ ê´€ë ¨ì„± ê²€ì‚¬ ë©”ì„œë“œ (ëŒ€í­ ê°•í™” ë²„ì „)
    def check_context_relevance_ai(self, question_analysis: dict, answer_categories: list, query: str, top_answers: list) -> str:
        """AIì™€ ì˜ë¯¸ì  ìœ ì‚¬ì„± ê²€ì¦ì„ ê²°í•©í•œ ê³ ë„í™”ëœ ê´€ë ¨ì„± ê²€ì‚¬"""
        
        try:
            # 1ë‹¨ê³„: ì˜ë¯¸ì  ìœ ì‚¬ì„± ê²€ì¦ (ìƒˆë¡œìš´ ë‹¤ë‹¨ê³„ ê²€ì¦)
            semantic_result = self.validate_semantic_similarity(question_analysis, answer_categories, query, top_answers)
            
            # ì‹¬ê°í•œ ë¶ˆì¼ì¹˜ê°€ ê°ì§€ëœ ê²½ìš° ì¦‰ì‹œ irrelevant ë°˜í™˜
            if semantic_result.get('critical_mismatch', False):
                logging.warning(f"ì‹¬ê°í•œ ì˜ë¯¸ì  ë¶ˆì¼ì¹˜ ê°ì§€: {semantic_result.get('analysis_summary', '')}")
                return 'irrelevant'
            
            # ì˜ë¯¸ì  ìœ ì‚¬ì„± ì ìˆ˜ê°€ ë§¤ìš° ë‚®ì€ ê²½ìš°
            similarity_score = semantic_result.get('similarity_score', 0.0)
            if similarity_score < 0.3:
                logging.warning(f"ì˜ë¯¸ì  ìœ ì‚¬ì„± ì ìˆ˜ ë„ˆë¬´ ë‚®ìŒ: {similarity_score:.3f}")
                return 'irrelevant'
            
            # 2ë‹¨ê³„: AI ê¸°ë°˜ ìƒì„¸ ë¶„ì„
            answer_summaries = []
            for i, answer in enumerate(top_answers[:3]):
                answer_text = answer.get('answer', '')[:200]
                answer_summaries.append(f"ë‹µë³€{i+1}: {answer_text}")
            
            combined_answers = "\n".join(answer_summaries)
            
            with memory_cleanup():
                system_prompt = """ë‹¹ì‹ ì€ ë°”ì´ë¸” ì•± ì „ë¬¸ ë¬¸ì˜-ë‹µë³€ ê´€ë ¨ì„± ë¶„ì„ê°€ì…ë‹ˆë‹¤.
ê³ ê°ì˜ ì§ˆë¬¸ ì˜ë„ì™€ ê²€ìƒ‰ëœ ë‹µë³€ë“¤ì˜ ê´€ë ¨ì„±ì„ ë§¤ìš° ì—„ê²©í•˜ê²Œ ë¶„ì„í•˜ì—¬ íŒì •í•˜ì„¸ìš”:

- "high": ë‹µë³€ì´ ì§ˆë¬¸ê³¼ ì§ì ‘ì ìœ¼ë¡œ ê´€ë ¨ë˜ê³  ì™„ë²½íˆ ë„ì›€ë¨
- "medium": ë‹µë³€ì´ ê´€ë ¨ì´ ìˆì§€ë§Œ ì¼ë¶€ ë¶ˆì¼ì¹˜ ìˆìŒ
- "low": ë‹µë³€ì´ ì•½ê°„ ê´€ë ¨ì´ ìˆì§€ë§Œ í•µì‹¬ê³¼ ê±°ë¦¬ ìˆìŒ
- "irrelevant": ë‹µë³€ì´ ì§ˆë¬¸ê³¼ ì „í˜€ ê´€ë ¨ì—†ìŒ

ğŸš« ë°”ì´ë¸” ì•± íŠ¹í™” ì—„ê²© ê¸°ì¤€:
1. ì°¬ì†¡ê°€ â‰  ì„±ê²½ (ì™„ì „íˆ ë‹¤ë¥¸ ì½˜í…ì¸ )
2. ì˜ì–´ â‰  í•œêµ­ì–´ (ì–¸ì–´ ë¶ˆì¼ì¹˜)
3. ë³µì‚¬ â‰  ì¬ìƒ (í–‰ë™ ë¶ˆì¼ì¹˜)
4. ê²€ìƒ‰ â‰  ì„¤ì • (ê¸°ëŠ¥ ë¶ˆì¼ì¹˜)

ğŸ“š íŠ¹ë³„ ì¼€ì´ìŠ¤:
- "ì˜ì–´ ì°¬ì†¡ê°€" ì§ˆë¬¸ + "ì˜ì–´ ì„±ê²½" ë‹µë³€ â†’ "irrelevant"
- "ë³µì‚¬ ê¸°ëŠ¥" ì§ˆë¬¸ + "ì¬ìƒ ê¸°ëŠ¥" ë‹µë³€ â†’ "irrelevant"
- "ê²€ìƒ‰ ë°©ë²•" ì§ˆë¬¸ + "ì„¤ì • ë³€ê²½" ë‹µë³€ â†’ "irrelevant"

ê²°ê³¼ëŠ” "high", "medium", "low", "irrelevant" ì¤‘ í•˜ë‚˜ë§Œ ë°˜í™˜í•˜ì„¸ìš”."""

                user_prompt = f"""ì§ˆë¬¸ ìƒì„¸ ë¶„ì„:
ì½˜í…ì¸ ìœ í˜•: {question_analysis.get('content_type', 'N/A')}
ì–¸ì–´ì„ í˜¸: {question_analysis.get('language_preference', 'N/A')}
í–‰ë™ìœ í˜•: {question_analysis.get('action_type', 'N/A')}
ì˜ë„: {question_analysis.get('intent_type', 'N/A')}

ì˜ë¯¸ì  ìœ ì‚¬ì„± ê²€ì¦ ê²°ê³¼:
- ìœ ì‚¬ì„± ì ìˆ˜: {similarity_score:.3f}
- ì½˜í…ì¸  ì¼ì¹˜ë„: {semantic_result.get('content_match_ratio', 0):.2f}
- ì–¸ì–´ ì¼ì¹˜ë„: {semantic_result.get('language_match_ratio', 0):.2f}
- í–‰ë™ ì¼ì¹˜ë„: {semantic_result.get('action_match_ratio', 0):.2f}

ì›ë³¸ ì§ˆë¬¸: {query}

ê²€ìƒ‰ëœ ë‹µë³€ë“¤:
{combined_answers}

âš ï¸ ì˜ë¯¸ì  ìœ ì‚¬ì„± ê²€ì¦ì—ì„œ ì´ë¯¸ ë¶ˆì¼ì¹˜ê°€ ê°ì§€ë˜ì—ˆë‹¤ë©´ ë”ìš± ì—„ê²©í•˜ê²Œ íŒì •í•˜ì„¸ìš”.
ìœ„ ëª¨ë“  ì •ë³´ë¥¼ ì¢…í•©í•˜ì—¬ ê´€ë ¨ì„±ì„ ë¶„ì„í•´ì£¼ì„¸ìš”."""

                response = self.openai_client.chat.completions.create(
                    model='gpt-3.5-turbo',
                    messages=[
                        {"role": "system", "content": system_prompt},
                        {"role": "user", "content": user_prompt}
                    ],
                    max_tokens=50,
                    temperature=0.1  # ë” ë³´ìˆ˜ì ìœ¼ë¡œ
                )
                
                ai_result = response.choices[0].message.content.strip().lower()
                
                # 3ë‹¨ê³„: ì˜ë¯¸ì  ìœ ì‚¬ì„±ê³¼ AI ë¶„ì„ ê²°ê³¼ë¥¼ ì¢…í•©
                final_similarity = semantic_result.get('final_similarity', 'medium')
                
                # AI ê²°ê³¼ ì •ê·œí™”
                if 'high' in ai_result:
                    ai_relevance = 'high'
                elif 'medium' in ai_result:
                    ai_relevance = 'medium'
                elif 'low' in ai_result:
                    ai_relevance = 'low'
                elif 'irrelevant' in ai_result:
                    ai_relevance = 'irrelevant'
                else:
                    ai_relevance = 'medium'
                
                # ìµœì¢… íŒì •: ë‘ ê²°ê³¼ ì¤‘ ë” ë³´ìˆ˜ì ì¸ ê²ƒ ì„ íƒ
                relevance_ranking = {'high': 4, 'medium': 3, 'low': 2, 'irrelevant': 1, 'critical_mismatch': 0, 'very_low': 1, 'error': 0}
                
                semantic_rank = relevance_ranking.get(final_similarity, 1)
                ai_rank = relevance_ranking.get(ai_relevance, 1)
                
                # ë” ë‚®ì€ (ë³´ìˆ˜ì ì¸) ì ìˆ˜ ì„ íƒ
                final_rank = min(semantic_rank, ai_rank)
                
                # ì ìˆ˜ë¥¼ ë‹¤ì‹œ ë¬¸ìì—´ë¡œ ë³€í™˜
                rank_to_relevance = {4: 'high', 3: 'medium', 2: 'low', 1: 'irrelevant', 0: 'irrelevant'}
                final_result = rank_to_relevance.get(final_rank, 'irrelevant')
                
                logging.info(f"ê´€ë ¨ì„± ìµœì¢… íŒì •: ì˜ë¯¸ì ={final_similarity}, AI={ai_relevance}, ìµœì¢…={final_result}")
                return final_result
                    
        except Exception as e:
            logging.error(f"ê°•í™”ëœ ê´€ë ¨ì„± ë¶„ì„ ì‹¤íŒ¨: {e}")
            # í´ë°±: ê¸°ë³¸ì ì¸ í‚¤ì›Œë“œ ë§¤ì¹­
            return self.fallback_relevance_check(query, top_answers)
    
    # â˜† í´ë°± ê´€ë ¨ì„± ê²€ì‚¬ ë©”ì„œë“œ (ì˜ë¯¸ë¡ ì  ë§¤ì¹­ ê°•í™”)
    def fallback_relevance_check(self, query: str, top_answers: list) -> str:
        """AI ë¶„ì„ ì‹¤íŒ¨ ì‹œ ì‚¬ìš©í•˜ëŠ” ì˜ë¯¸ë¡ ì  í‚¤ì›Œë“œ ë§¤ì¹­"""
        
        # ì˜ë¯¸ë¡ ì  í‚¤ì›Œë“œ ê·¸ë£¹ ì •ì˜
        semantic_groups = {
            'text_copy': ['ë³µì‚¬', 'ë¶™ì—¬ë„£ê¸°', 'ì›Œë“œ', 'í…ìŠ¤íŠ¸', 'ë³µì‚¬í•´ì„œ', 'ì˜®ê¸°', 'ë‚´ë³´ë‚´ê¸°', 'ì €ì¥'],
            'audio_play': ['ì¬ìƒ', 'ë“£ê¸°', 'ìŒì„±', 'ì†Œë¦¬', 'ì—°ì†ì¬ìƒ', 'ë°˜ë³µ', 'ë“¤ì„', 'ë“£ê³ '],
            'search_find': ['ê²€ìƒ‰', 'ì°¾ê¸°', 'ì°¾ì•„ì„œ', 'ê²€ìƒ‰í•´ì„œ', 'ì°¾ì„', 'ì°¾ëŠ”'],
            'download': ['ë‹¤ìš´ë¡œë“œ', 'ë‹¤ìš´ë°›ê¸°', 'ë°›ê¸°', 'ì €ì¥'],
            'error_report': ['ì˜¤ë¥˜', 'ì—ëŸ¬', 'ì•ˆë¨', 'ì•ˆë˜', 'ë¬¸ì œ', 'ê³ ì¥', 'ë²„ê·¸'],
            'setting_config': ['ì„¤ì •', 'ë³€ê²½', 'ì¡°ì •', 'ì˜µì…˜', 'í™˜ê²½ì„¤ì •']
        }
        
        # ì§ˆë¬¸ì—ì„œ ì˜ë¯¸ ê·¸ë£¹ ì‹ë³„
        query_lower = query.lower()
        query_semantic_groups = set()
        for group_name, keywords in semantic_groups.items():
            if any(keyword in query_lower for keyword in keywords):
                query_semantic_groups.add(group_name)
        
        max_relevance = 0
        for answer in top_answers:
            answer_lower = answer.get('answer', '').lower()
            answer_semantic_groups = set()
            
            # ë‹µë³€ì—ì„œ ì˜ë¯¸ ê·¸ë£¹ ì‹ë³„
            for group_name, keywords in semantic_groups.items():
                if any(keyword in answer_lower for keyword in keywords):
                    answer_semantic_groups.add(group_name)
            
            # ì˜ë¯¸ ê·¸ë£¹ ì¼ì¹˜ë„ ê³„ì‚°
            if query_semantic_groups and answer_semantic_groups:
                semantic_overlap = len(query_semantic_groups & answer_semantic_groups)
                semantic_total = len(query_semantic_groups | answer_semantic_groups)
                semantic_ratio = semantic_overlap / semantic_total if semantic_total > 0 else 0
            else:
                semantic_ratio = 0
            
            # í‚¤ì›Œë“œ ì¼ì¹˜ë„ë„ í•¨ê»˜ ê³ ë ¤
            query_words = set(self.extract_keywords(query_lower))
            answer_words = set(self.extract_keywords(answer_lower))
            keyword_overlap = len(query_words & answer_words)
            keyword_ratio = keyword_overlap / max(len(query_words), 1)
            
            # ì˜ë¯¸ë¡ ì  ë§¤ì¹­ê³¼ í‚¤ì›Œë“œ ë§¤ì¹­ì„ ì¡°í•© (ì˜ë¯¸ë¡ ì  ë§¤ì¹­ì— ë” ë†’ì€ ê°€ì¤‘ì¹˜)
            combined_relevance = semantic_ratio * 0.7 + keyword_ratio * 0.3
            max_relevance = max(max_relevance, combined_relevance)
        
        # ğŸš« ì˜ë¯¸ ê·¸ë£¹ì´ ì™„ì „íˆ ë‹¤ë¥¸ ê²½ìš° irrelevant ì²˜ë¦¬
        if query_semantic_groups and any(answer.get('answer', '') for answer in top_answers):
            all_answer_groups = set()
            for answer in top_answers:
                answer_lower = answer.get('answer', '').lower()
                for group_name, keywords in semantic_groups.items():
                    if any(keyword in answer_lower for keyword in keywords):
                        all_answer_groups.add(group_name)
            
            # í…ìŠ¤íŠ¸-ìŒì„±, ê²€ìƒ‰-ì„¤ì • ë“± ìƒë°˜ëœ ê·¸ë£¹ì¸ ê²½ìš°
            conflicting_pairs = [
                ('text_copy', 'audio_play'),
                ('search_find', 'setting_config'),
                ('error_report', 'search_find')
            ]
            
            for q_group in query_semantic_groups:
                for a_group in all_answer_groups:
                    if (q_group, a_group) in conflicting_pairs or (a_group, q_group) in conflicting_pairs:
                        return 'irrelevant'
        
        # ê´€ë ¨ì„± ì ìˆ˜ì— ë”°ë¥¸ íŒì •
        if max_relevance >= 0.6:
            return 'high'
        elif max_relevance >= 0.4:
            return 'medium'
        elif max_relevance >= 0.2:
            return 'low'
        else:
            return 'irrelevant'
    
    # â˜† í•µì‹¬ í‚¤ì›Œë“œ ì¶”ì¶œ ë©”ì„œë“œ
    def extract_keywords(self, text: str) -> list:
        """í…ìŠ¤íŠ¸ì—ì„œ í•µì‹¬ í‚¤ì›Œë“œ ì¶”ì¶œ"""
        # ë¶ˆìš©ì–´ ì œê±°ìš© ë¦¬ìŠ¤íŠ¸
        stop_words = {'ëŠ”', 'ì€', 'ì´', 'ê°€', 'ì„', 'ë¥¼', 'ì—', 'ì—ì„œ', 'ë¡œ', 'ìœ¼ë¡œ', 'ì™€', 'ê³¼', 'ì˜', 'ë„', 'ë§Œ', 'ê¹Œì§€', 'ë¶€í„°', 'ê»˜ì„œ', 'ì—ê²Œ', 'í•œí…Œ', 'ë¡œë¶€í„°', 'ìœ¼ë¡œë¶€í„°'}
        
        # íŠ¹ìˆ˜ë¬¸ì ì œê±° ë° ë‹¨ì–´ ë¶„ë¦¬
        
        words = re.findall(r'[ê°€-í£a-zA-Z0-9]+', text)
        
        # ë¶ˆìš©ì–´ ì œê±° ë° 2ê¸€ì ì´ìƒ ë‹¨ì–´ë§Œ ì„ íƒ
        keywords = [word for word in words if len(word) >= 2 and word not in stop_words]
        
        return keywords

    # â˜† ìŠ¤ë§ˆíŠ¸ í´ë°± ë‹µë³€ ìƒì„± ë©”ì„œë“œ (ìƒˆë¡œ ì¶”ê°€)
    def generate_smart_fallback_answer(self, query: str, question_analysis: dict, lang: str = 'ko') -> str:
        """ê´€ë ¨ì„±ì´ ë‚®ì„ ë•Œ ì§ˆë¬¸ ì˜ë„ì— ë§ëŠ” ì ì ˆí•œ í´ë°± ë‹µë³€ ìƒì„±"""
        try:
            with memory_cleanup():
                # ì§ˆë¬¸ ë¶„ì„ ê²°ê³¼ ì¶”ì¶œ
                content_type = question_analysis.get('content_type', 'other')
                language_preference = question_analysis.get('language_preference', 'none')
                action_type = question_analysis.get('action_type', 'other')
                intent_type = question_analysis.get('intent_type', 'ì¼ë°˜ë¬¸ì˜')
                
                if lang == 'en':
                    # ì˜ì–´ ìŠ¤ë§ˆíŠ¸ í´ë°± ë‹µë³€ ìƒì„±
                    if content_type == 'hymn':
                        if language_preference == 'english':
                            fallback_answer = "<p>Thank you for your inquiry about English hymns.</p><p><br></p><p>We are reviewing options to expand our hymn collection, including English hymns.</p><p><br></p><p>Currently, our app primarily focuses on Korean hymns and Bible content.</p><p><br></p><p>We will forward your request to our development team for future consideration.</p><p><br></p><p>Please feel free to contact us again if you have any other questions.</p>"
                        else:
                            fallback_answer = "<p>Thank you for your inquiry about hymns.</p><p><br></p><p>Our app currently provides a comprehensive collection of Korean hymns.</p><p><br></p><p>We are continuously working to improve our hymn features.</p><p><br></p><p>Please let us know if you have any specific requests or feedback.</p>"
                    elif content_type == 'bible':
                        if language_preference == 'english':
                            fallback_answer = "<p>Thank you for your inquiry about English Bible content.</p><p><br></p><p>Our app supports multiple Bible translations including English versions.</p><p><br></p><p>Please check the translation settings in the app menu to access English Bible versions.</p><p><br></p><p>If you need assistance with specific features, please contact our customer service.</p>"
                        else:
                            fallback_answer = "<p>Thank you for your Bible-related inquiry.</p><p><br></p><p>Our app provides comprehensive Bible reading features with multiple translations.</p><p><br></p><p>Please explore the Bible section for various reading and study tools.</p><p><br></p><p>Contact us if you need help with specific Bible features.</p>"
                    else:
                        fallback_answer = "<p>Thank you for contacting GOODTV Bible App support.</p><p><br></p><p>We are reviewing your inquiry to provide the most accurate information.</p><p><br></p><p>Our team will respond with detailed guidance soon.</p><p><br></p><p>Please contact us again if you have any urgent questions.</p>"
                else:
                    # í•œêµ­ì–´ ìŠ¤ë§ˆíŠ¸ í´ë°± ë‹µë³€ ìƒì„±
                    if content_type == 'hymn':
                        if language_preference == 'english':
                            fallback_answer = "<p>ì˜ì–´ ì°¬ì†¡ê°€ì— ëŒ€í•œ ë¬¸ì˜í•´ ì£¼ì…”ì„œ ê°ì‚¬í•©ë‹ˆë‹¤.</p><p><br></p><p>í˜„ì¬ ë°”ì´ë¸” ì• í”Œì€ í•œêµ­ì–´ ì°¬ì†¡ê°€ë¥¼ ì¤‘ì‹¬ìœ¼ë¡œ ì„œë¹„ìŠ¤ë¥¼ ì œê³µí•˜ê³  ìˆìŠµë‹ˆë‹¤.</p><p><br></p><p>ì˜ì–´ ì°¬ì†¡ê°€ ì¶”ê°€ì— ëŒ€í•œ ì„±ë„ë‹˜ì˜ ì˜ê²¬ì€ ê°œë°œíŒ€ì— ì „ë‹¬í•˜ì—¬</p><p><br></p><p>í–¥í›„ ì„œë¹„ìŠ¤ ê°œì„  ì‹œ ì ê·¹ ê²€í† í•˜ë„ë¡ í•˜ê² ìŠµë‹ˆë‹¤.</p><p><br></p><p>ë‹¤ë¥¸ ê¶ê¸ˆí•œ ì‚¬í•­ì´ ìˆìœ¼ì‹œë©´ ì–¸ì œë“  ë¬¸ì˜í•´ ì£¼ì„¸ìš”.</p>"
                        else:
                            fallback_answer = "<p>ì°¬ì†¡ê°€ ê´€ë ¨ ë¬¸ì˜í•´ ì£¼ì…”ì„œ ê°ì‚¬í•©ë‹ˆë‹¤.</p><p><br></p><p>ë°”ì´ë¸” ì• í”Œì—ì„œëŠ” ë‹¤ì–‘í•œ ì°¬ì†¡ê°€ ì„œë¹„ìŠ¤ë¥¼ ì œê³µí•˜ê³  ìˆìŠµë‹ˆë‹¤.</p><p><br></p><p>ì°¬ì†¡ê°€ ë©”ë‰´ì—ì„œ ì›í•˜ì‹œëŠ” ì°¬ì†¡ì„ ê²€ìƒ‰í•˜ê³  ì¬ìƒí•˜ì‹¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤.</p><p><br></p><p>êµ¬ì²´ì ì¸ ê¸°ëŠ¥ì´ë‚˜ ì‚¬ìš©ë²•ì— ëŒ€í•´ ê¶ê¸ˆí•˜ì‹  ì ì´ ìˆìœ¼ì‹œë©´</p><p><br></p><p>ì–¸ì œë“  ë‹¤ì‹œ ë¬¸ì˜í•´ ì£¼ì„¸ìš”.</p>"
                    elif content_type == 'bible':
                        if language_preference == 'english':
                            fallback_answer = "<p>ì˜ì–´ ì„±ê²½ì— ëŒ€í•œ ë¬¸ì˜í•´ ì£¼ì…”ì„œ ê°ì‚¬í•©ë‹ˆë‹¤.</p><p><br></p><p>ë°”ì´ë¸” ì• í”Œì—ì„œëŠ” NIV, ESV ë“± ë‹¤ì–‘í•œ ì˜ì–´ ì„±ê²½ ë²ˆì—­ë³¸ì„</p><p><br></p><p>ì œê³µí•˜ê³  ìˆìŠµë‹ˆë‹¤.</p><p><br></p><p>ì„¤ì • ë©”ë‰´ì—ì„œ ë²ˆì—­ë³¸ì„ ë³€ê²½í•˜ì—¬ ì˜ì–´ ì„±ê²½ì„ ì´ìš©í•˜ì‹¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤.</p><p><br></p><p>ìì„¸í•œ ì‚¬ìš©ë²•ì´ ê¶ê¸ˆí•˜ì‹œë©´ ë‹¤ì‹œ ë¬¸ì˜í•´ ì£¼ì„¸ìš”.</p>"
                        else:
                            fallback_answer = "<p>ì„±ê²½ ê´€ë ¨ ë¬¸ì˜í•´ ì£¼ì…”ì„œ ê°ì‚¬í•©ë‹ˆë‹¤.</p><p><br></p><p>ë°”ì´ë¸” ì• í”Œì—ì„œëŠ” ë‹¤ì–‘í•œ ì„±ê²½ ë²ˆì—­ë³¸ê³¼ ì½ê¸° ê¸°ëŠ¥ì„</p><p><br></p><p>ì œê³µí•˜ê³  ìˆìŠµë‹ˆë‹¤.</p><p><br></p><p>ì„±ê²½ ë©”ë‰´ì—ì„œ ì›í•˜ì‹œëŠ” ì„±ê²½ ë³¸ë¬¸ì„ ê²€ìƒ‰í•˜ê³  ì½ìœ¼ì‹¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤.</p><p><br></p><p>êµ¬ì²´ì ì¸ ì‚¬ìš©ë²•ì´ ê¶ê¸ˆí•˜ì‹œë©´ ì–¸ì œë“  ë¬¸ì˜í•´ ì£¼ì„¸ìš”.</p>"
                    elif action_type == 'copy':
                        fallback_answer = "<p>í…ìŠ¤íŠ¸ ë³µì‚¬ ê¸°ëŠ¥ì— ëŒ€í•œ ë¬¸ì˜í•´ ì£¼ì…”ì„œ ê°ì‚¬í•©ë‹ˆë‹¤.</p><p><br></p><p>ë°”ì´ë¸” ì• í”Œì—ì„œëŠ” ì„±ê²½ ë³¸ë¬¸ì„ ì„ íƒí•˜ì—¬ ë³µì‚¬í•˜ëŠ” ê¸°ëŠ¥ì„</p><p><br></p><p>ì œê³µí•˜ê³  ìˆìŠµë‹ˆë‹¤.</p><p><br></p><p>ì›í•˜ì‹œëŠ” êµ¬ì ˆì„ ê¸¸ê²Œ ëˆŒëŸ¬ ì„ íƒí•œ í›„ ë³µì‚¬ ë²„íŠ¼ì„ ì´ìš©í•´ ì£¼ì„¸ìš”.</p><p><br></p><p>ìì„¸í•œ ì‚¬ìš©ë²•ì´ ê¶ê¸ˆí•˜ì‹œë©´ ì–¸ì œë“  ë¬¸ì˜í•´ ì£¼ì„¸ìš”.</p>"
                    elif action_type == 'play':
                        fallback_answer = "<p>ìŒì„± ì¬ìƒ ê¸°ëŠ¥ì— ëŒ€í•œ ë¬¸ì˜í•´ ì£¼ì…”ì„œ ê°ì‚¬í•©ë‹ˆë‹¤.</p><p><br></p><p>ë°”ì´ë¸” ì• í”Œì—ì„œëŠ” ì„±ê²½ê³¼ ì°¬ì†¡ê°€ ìŒì„± ì¬ìƒ ê¸°ëŠ¥ì„</p><p><br></p><p>ì œê³µí•˜ê³  ìˆìŠµë‹ˆë‹¤.</p><p><br></p><p>ì¬ìƒ ë²„íŠ¼ì„ ëˆŒëŸ¬ ì›í•˜ì‹œëŠ” ì½˜í…ì¸ ë¥¼ ë“¤ìœ¼ì‹¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤.</p><p><br></p><p>êµ¬ì²´ì ì¸ ì¬ìƒ ë°©ë²•ì´ ê¶ê¸ˆí•˜ì‹œë©´ ë‹¤ì‹œ ë¬¸ì˜í•´ ì£¼ì„¸ìš”.</p>"
                    else:
                        fallback_answer = "<p>ë°”ì´ë¸” ì• í”Œ ê´€ë ¨ ë¬¸ì˜í•´ ì£¼ì…”ì„œ ê°ì‚¬í•©ë‹ˆë‹¤.</p><p><br></p><p>ì„±ë„ë‹˜ì˜ ë¬¸ì˜ ë‚´ìš©ì„ ì •í™•íˆ íŒŒì•…í•˜ì—¬</p><p><br></p><p>ê°€ì¥ ì ì ˆí•œ ë‹µë³€ì„ ë“œë¦¬ê¸° ìœ„í•´ ê²€í† í•˜ê³  ìˆìŠµë‹ˆë‹¤.</p><p><br></p><p>êµ¬ì²´ì ì¸ ë‹µë³€ì€ ë¹ ë¥¸ ì‹œì¼ ë‚´ì— ì „ë‹¬ë“œë¦¬ê² ìŠµë‹ˆë‹¤.</p><p><br></p><p>ê¸‰í•œ ë¬¸ì˜ì‚¬í•­ì´ ìˆìœ¼ì‹œë©´ ì–¸ì œë“  ë‹¤ì‹œ ì—°ë½í•´ ì£¼ì„¸ìš”.</p>"
                
                logging.info(f"ìŠ¤ë§ˆíŠ¸ í´ë°± ë‹µë³€ ìƒì„± ì™„ë£Œ: ì½˜í…ì¸ ={content_type}, ì–¸ì–´={language_preference}, í–‰ë™={action_type}")
                return fallback_answer
                
        except Exception as e:
            logging.error(f"ìŠ¤ë§ˆíŠ¸ í´ë°± ë‹µë³€ ìƒì„± ì‹¤íŒ¨: {e}")
            # ìµœì¢… ì•ˆì „ì¥ì¹˜
            if lang == 'en':
                return "<p>Thank you for contacting GOODTV Bible App.</p><p><br></p><p>We are reviewing your inquiry and will provide a detailed response soon.</p><p><br></p><p>Please contact us again if you have any urgent questions.</p>"
            else:
                return "<p>ì•ˆë…•í•˜ì„¸ìš”. GOODTV ë°”ì´ë¸” ì• í”Œì…ë‹ˆë‹¤.</p><p><br></p><p>ì„±ë„ë‹˜ì˜ ë¬¸ì˜ ë‚´ìš©ì„ ê²€í† í•˜ì—¬ ì •í™•í•œ ë‹µë³€ì„ ì¤€ë¹„í•˜ê² ìŠµë‹ˆë‹¤.</p><p><br></p><p>ë‹µë³€ì€ ë¹ ë¥¸ ì‹œì¼ ë‚´ì— ì „ë‹¬ë“œë¦¬ê² ìŠµë‹ˆë‹¤.</p><p><br></p><p>ê°ì‚¬í•©ë‹ˆë‹¤.</p>"

    # â˜† ì°¸ê³  ë‹µë³€ì—ì„œ ì¸ì‚¬ë§ê³¼ ëë§ºìŒë§ì„ ì œê±°í•˜ëŠ” ë©”ì„œë“œ
    # Args:
    #     text (str): ì œê±°í•  í…ìŠ¤íŠ¸
    #     lang (str): ì–¸ì–´ (ê¸°ë³¸ê°’: í•œêµ­ì–´)
            
    # Returns:
    #     str: ì œê±°ëœ í…ìŠ¤íŠ¸
    def remove_greeting_and_closing(self, text: str, lang: str = 'ko') -> str:
        # null ì²´í¬
        if not text:
            return ""
        
        if lang == 'ko':
            # í•œêµ­ì–´ ì¸ì‚¬ë§ ì œê±° íŒ¨í„´
            greeting_patterns = [
                r'^ì•ˆë…•í•˜ì„¸ìš”[^.]*\.\s*',
                r'^GOODTV\s+ë°”ì´ë¸”\s*ì• í”Œ[^.]*\.\s*',
                r'^ë°”ì´ë¸”\s*ì• í”Œ[^.]*\.\s*',
                r'^ì„±ë„ë‹˜[^.]*\.\s*',
                r'^ê³ ê°ë‹˜[^.]*\.\s*',
                r'^ê°ì‚¬í•©ë‹ˆë‹¤[^.]*\.\s*',
                r'^ê°ì‚¬ë“œë¦½ë‹ˆë‹¤[^.]*\.\s*',
                r'^ë°”ì´ë¸”\s*ì• í”Œì„\s*ì´ìš©í•´ì£¼ì…”ì„œ[^.]*\.\s*',
                r'^ë°”ì´ë¸”\s*ì• í”Œì„\s*ì• ìš©í•´\s*ì£¼ì…”ì„œ[^.]*\.\s*'
            ]
            
            # í•œêµ­ì–´ ëë§ºìŒë§ ì œê±° íŒ¨í„´ë“¤
            closing_patterns = [
                r'\s*ê°ì‚¬í•©ë‹ˆë‹¤[^.]*\.?\s*$',
                r'\s*ê°ì‚¬ë“œë¦½ë‹ˆë‹¤[^.]*\.?\s*$',
                r'\s*í‰ì•ˆí•˜ì„¸ìš”[^.]*\.?\s*$',
                r'\s*ì£¼ë‹˜\s*ì•ˆì—ì„œ[^.]*\.?\s*$',
                r'\s*í•¨ê»˜\s*ê¸°ë„í•˜ë©°[^.]*\.?\s*$',
                r'\s*í•­ìƒ[^.]*ë°”ì´ë¸”\s*ì• í”Œ[^.]*\.?\s*$',
                r'\s*í•­ìƒ\s*ì£¼ë‹˜\s*ì•ˆì—ì„œ[^.]*\.?\s*$',
                r'\s*ì£¼ë‹˜\s*ì•ˆì—ì„œ\s*í‰ì•ˆí•˜ì„¸ìš”[^.]*\.?\s*$',
                r'\s*ì£¼ë‹˜ì˜\s*ì€ì´ì´[^.]*\.?\s*$',
                r'\s*ê¸°ë„ë“œë¦¬ê² ìŠµë‹ˆë‹¤[^.]*\.?\s*$'
            ]

        else:  # ì˜ì–´ ì¸ì‚¬ë§ ì œê±° íŒ¨í„´
            greeting_patterns = [
                r'^Hello[^.]*\.\s*',
                r'^Hi[^.]*\.\s*',
                r'^Dear[^.]*\.\s*',
                r'^Thank you[^.]*\.\s*',
                r'^Thanks[^.]*\.\s*',
                r'^This is GOODTV Bible App[^.]*\.\s*',
            ]
            
            # ì˜ì–´ ëë§ºìŒë§ ì œê±° íŒ¨í„´
            closing_patterns = [
                r'\s*Thank you[^.]*\.?\s*$',
                r'\s*Thanks[^.]*\.?\s*$',
                r'\s*Best regards[^.]*\.?\s*$',
                r'\s*Sincerely[^.]*\.?\s*$',
                r'\s*God bless[^.]*\.?\s*$',
                r'\s*May God[^.]*\.?\s*$',
            ]
        
        # ì¸ì‚¬ë§ ì œê±°
        for pattern in greeting_patterns:
            text = re.sub(pattern, '', text, flags=re.IGNORECASE)
        
        # ëë§ºìŒë§ ì œê±°
        for pattern in closing_patterns:
            text = re.sub(pattern, '', text, flags=re.IGNORECASE)
        
        # ë¬¸ì¥ ëì˜ ëë§ºìŒë§ë“¤ë„ ì œê±°
        text = re.sub(r'[,.!?]\s*í•­ìƒ\s*ì£¼ë‹˜\s*ì•ˆì—ì„œ[^.]*\.?\s*$', '', text, flags=re.IGNORECASE)
        text = re.sub(r'[,.!?]\s*ê°ì‚¬í•©ë‹ˆë‹¤[^.]*\.?\s*$', '', text, flags=re.IGNORECASE)
        text = re.sub(r'[,.!?]\s*í‰ì•ˆí•˜ì„¸ìš”[^.]*\.?\s*$', '', text, flags=re.IGNORECASE)
        
        # ì•ë’¤ ê³µë°± ì •ë¦¬
        text = text.strip()
        
        return text

    # â˜† GPT ë‹µë³€ ìƒì„±ì„ ìœ„í•œ í–¥ìƒëœ ì»¨í…ìŠ¤íŠ¸ ìƒì„± ë©”ì„œë“œ
    # 
    # ì»¨í…ìŠ¤íŠ¸ ìƒì„± ì „ëµ:
    # 1. í’ˆì§ˆë³„ ë‹µë³€ ê·¸ë£¹í•‘ (ê³ /ì¤‘/ë‚®ì€ í’ˆì§ˆ)
    # 2. ê³ í’ˆì§ˆ ë‹µë³€ ìš°ì„  ì„ íƒ (ìµœëŒ€ 4ê°œ)
    # 3. ì¤‘í’ˆì§ˆ ë‹µë³€ìœ¼ë¡œ ë³´ì™„ (ìµœëŒ€ 3ê°œ)
    # 4. ìµœì†Œ ê°œìˆ˜ ë¯¸ë‹¬ì‹œ ì¤‘ê°„ í’ˆì§ˆ ë‹µë³€ ì¶”ê°€
    # 5. í…ìŠ¤íŠ¸ ì •ì œ ë° ê¸¸ì´ ì œí•œ (ì¸ì‚¬ë§/ëë§ºìŒë§ ì œê±°)
    # 
    # ì´ë ‡ê²Œ êµ¬ì„±ëœ ì»¨í…ìŠ¤íŠ¸ëŠ” GPTì—ê²Œ ì°¸ê³  ìë£Œë¡œ ì œê³µë˜ì–´
    # ì¼ê´€ëœ ìŠ¤íƒ€ì¼ê³¼ ì •í™•í•œ ì •ë³´ë¡œ ë‹µë³€ì„ ìƒì„±í•˜ê²Œ í•¨
    # 
    # Args:
    #     similar_answers (list): ê²€ìƒ‰ëœ ìœ ì‚¬ ë‹µë³€ ë¦¬ìŠ¤íŠ¸
    #     max_answers (int): í¬í•¨í•  ìµœëŒ€ ë‹µë³€ ê°œìˆ˜ (ê¸°ë³¸ê°’: 7)
    #     
    # Returns:
    #     str: GPTìš© ì»¨í…ìŠ¤íŠ¸ ë¬¸ìì—´
    def create_enhanced_context(self, similar_answers: list, max_answers: int = 7, target_lang: str = 'ko') -> str:
        if not similar_answers:
            return ""
        
        context_parts = [] # ì»¨í…ìŠ¤íŠ¸ ë¶€ë¶„ë“¤ì„ ì €ì¥í•  ë¦¬ìŠ¤íŠ¸
        used_answers = 0 # ì‚¬ìš©ëœ ë‹µë³€ ê°œìˆ˜
        
        # ìœ ì‚¬ë„ ì ìˆ˜ì— ë”°ë¥¸ ë‹µë³€ ê·¸ë£¹í•‘
        # ê³„ì¸µì  ê·¸ë£¹í•‘ìœ¼ë¡œ í’ˆì§ˆë³„ ë‹µë³€ì„ ë¶„ë¥˜: ì´ëŠ” ë‚˜ì¤‘ì— ìš°ì„ ìˆœìœ„ì— ë”°ë¼ ì„ íƒí•˜ê¸° ìœ„í•¨
        high_score = [ans for ans in similar_answers if ans['score'] >= 0.7]      # ê³ í’ˆì§ˆ (70% ì´ìƒ ìœ ì‚¬)
        medium_score = [ans for ans in similar_answers if 0.5 <= ans['score'] < 0.7]  # ì¤‘í’ˆì§ˆ (50-70%)
        medium_low_score = [ans for ans in similar_answers if 0.5 <= ans['score'] < 0.6] # ë‚®ì€ í’ˆì§ˆ (50-60%)

        # 1ë‹¨ê³„: ê³ í’ˆì§ˆ ë‹µë³€ ìš°ì„  í¬í•¨ (ìµœëŒ€ 4ê°œ)
        for ans in high_score[:4]:
            if used_answers >= max_answers:
                break
            # ì œì–´ ë¬¸ì(ì¤„ë°”ê¿ˆ, íƒ­, ê°œí–‰ ë“±) ë° HTML íƒœê·¸ ì œê±°
            clean_answer = re.sub(r'[\b\r\f\v\x00-\x08\x0B\x0C\x0E-\x1F\x7F]|<[^>]+>', '', ans['answer'])
            clean_answer = self.remove_greeting_and_closing(clean_answer, 'ko') # ì¸ì‚¬ë§ê³¼ ëë§ºìŒë§ ì œê±°í•˜ì—¬ ë³¸ë¬¸ë§Œ ì¶”ì¶œ
            
            # ì˜ì–´ ì§ˆë¬¸ì¸ ê²½ìš° ë‹µë³€ì„ ë²ˆì—­
            if target_lang == 'en' and ans.get('lang', 'ko') == 'ko':
                clean_answer = self.translate_text(clean_answer, 'ko', 'en')
            
            # ìœ íš¨ì„± ê²€ì‚¬ ë° ê¸¸ì´ ì œí•œ (ìµœì†Œ 20ì ì´ìƒí™•ì¸, 400ìë¡œ ì˜ë¼ì„œ ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´ ì œí•œ ë°©ì§€)
            if self.is_valid_text(clean_answer, target_lang) and len(clean_answer.strip()) > 20:
                context_parts.append(f"[ì°¸ê³ ë‹µë³€ {used_answers+1} - ì ìˆ˜: {ans['score']:.2f}]\n{clean_answer[:400]}")
                used_answers += 1
        
        # 2ë‹¨ê³„: ì¤‘í’ˆì§ˆ ë‹µë³€ìœ¼ë¡œ ë³´ì™„ (ìµœëŒ€ 3ê°œ)
        for ans in medium_score[:3]:
            if used_answers >= max_answers:
                break
            # ì œì–´ ë¬¸ì(ì¤„ë°”ê¿ˆ, íƒ­, ê°œí–‰ ë“±) ë° HTML íƒœê·¸ ì œê±°
            clean_answer = re.sub(r'[\b\r\f\v\x00-\x08\x0B\x0C\x0E-\x1F\x7F]|<[^>]+>', '', ans['answer'])
            clean_answer = self.remove_greeting_and_closing(clean_answer, 'ko') # ì¸ì‚¬ë§ê³¼ ëë§ºìŒë§ ì œê±°í•˜ì—¬ ë³¸ë¬¸ë§Œ ì¶”ì¶œ
            
            # ì˜ì–´ ì§ˆë¬¸ì¸ ê²½ìš° ë‹µë³€ì„ ë²ˆì—­
            if target_lang == 'en' and ans.get('lang', 'ko') == 'ko':
                clean_answer = self.translate_text(clean_answer, 'ko', 'en')
            
            if self.is_valid_text(clean_answer, target_lang) and len(clean_answer.strip()) > 20:
                context_parts.append(f"[ì°¸ê³ ë‹µë³€ {used_answers+1} - ì ìˆ˜: {ans['score']:.2f}]\n{clean_answer[:300]}")
                used_answers += 1

        # 3ë‹¨ê³„: ë‹µë³€ì´ ë¶€ì¡±í•œ ê²½ìš° ì¤‘ê°„ í’ˆì§ˆ ë‹µë³€ ì¶”ê°€ (50-60% êµ¬ê°„)
        if used_answers < 3:  # ìµœì†Œ 3ê°œ ì´ìƒ í™•ë³´í•˜ê¸° ìœ„í•¨
            for ans in medium_low_score[:2]:
                if used_answers >= max_answers:
                    break
                clean_answer = re.sub(r'[\b\r\f\v\x00-\x08\x0B\x0C\x0E-\x1F\x7F]|<[^>]+>', '', ans['answer'])
                clean_answer = self.remove_greeting_and_closing(clean_answer, 'ko')
                
                # ì˜ì–´ ì§ˆë¬¸ì¸ ê²½ìš° ë‹µë³€ì„ ë²ˆì—­
                if target_lang == 'en' and ans.get('lang', 'ko') == 'ko':
                    clean_answer = self.translate_text(clean_answer, 'ko', 'en')
                
                if self.is_valid_text(clean_answer, target_lang) and len(clean_answer.strip()) > 20:
                    context_parts.append(f"[ì°¸ê³ ë‹µë³€ {used_answers+1} - ì ìˆ˜: {ans['score']:.2f}]\n{clean_answer[:250]}")
                    used_answers += 1
        
        logging.info(f"ì»¨í…ìŠ¤íŠ¸ ìƒì„±: {used_answers}ê°œì˜ ë‹µë³€ í¬í•¨ (ì–¸ì–´: {target_lang})")
        
        # ìµœì¢… ì»¨í…ìŠ¤íŠ¸ ì¡°í•© (êµ¬ë¶„ì„ ìœ¼ë¡œ ë‹µë³€ë“¤ ë¶„ë¦¬)
        return "\n\n" + "="*50 + "\n\n".join(context_parts)

    # â˜† ì´ì „ ì•± ì´ë¦„ì„ ì œê±°í•˜ëŠ” ë©”ì„œë“œ (êµ¬ ë‹¤ë²ˆì—­ì„±ê²½ì°¬ì†¡ ë“±)
    # Args:
    #     text (str): ì œê±°í•  í…ìŠ¤íŠ¸
            
    # Returns:
    #     str: ì œê±°ëœ í…ìŠ¤íŠ¸
    def remove_old_app_name(self, text: str) -> str:
        patterns_to_remove = [
            r'\s*\(êµ¬\)\s*ë‹¤ë²ˆì—­ì„±ê²½ì°¬ì†¡',
            r'\s*\(êµ¬\)ë‹¤ë²ˆì—­ì„±ê²½ì°¬ì†¡',
            r'ë°”ì´ë¸”\s*ì• í”Œ\s*\(êµ¬\)\s*ë‹¤ë²ˆì—­ì„±ê²½ì°¬ì†¡',
            r'ë°”ì´ë¸”ì• í”Œ\s*\(êµ¬\)ë‹¤ë²ˆì—­ì„±ê²½ì°¬ì†¡',
        ]
        
        for pattern in patterns_to_remove:
            text = re.sub(pattern, '', text, flags=re.IGNORECASE)
        
        text = re.sub(r'(GOODTV\s+ë°”ì´ë¸”\s*ì• í”Œ)\s+', r'\1', text)
        
        return text

    # â˜† ë‹µë³€ í…ìŠ¤íŠ¸ë¥¼ HTML ë‹¨ë½ í˜•ì‹ìœ¼ë¡œ í¬ë§·íŒ…í•˜ëŠ” ë©”ì„œë“œ
    def format_answer_with_html_paragraphs(self, text: str, lang: str = 'ko') -> str:
        if not text:
            return ""
        
        text = self.remove_old_app_name(text)
        
        # ë¬¸ì¥ì„ ë§ˆì¹¨í‘œ, ëŠë‚Œí‘œ, ë¬¼ìŒí‘œë¡œ ë¶„ë¦¬
        sentences = re.split(r'(?<=[.!?])\s+', text)
        
        paragraphs = [] # ë‹¨ë½ë“¤ì„ ì €ì¥í•  ë¦¬ìŠ¤íŠ¸
        current_paragraph = [] # í˜„ì¬ ë‹¨ë½ì„ ì €ì¥í•  ë¦¬ìŠ¤íŠ¸
        
        # ë‹¨ë½ ë¶„ë¦¬ íŠ¸ë¦¬ê±° í‚¤ì›Œë“œë“¤ (ë” í¬ê´„ì ìœ¼ë¡œ í™•ì¥)
        # ì ‘ì†ì‚¬ë‚˜ ì¸ì‚¬ë§ë¡œ ì‹œì‘í•˜ëŠ” ë¬¸ì¥ì€ ë“¤ì—¬ì“°ê¸°ë¥¼ í†µí•´ ìƒˆ ë‹¨ë½ìœ¼ë¡œ ë¶„ë¦¬
        if lang == 'ko':
            paragraph_triggers = [
                # ì¸ì‚¬ ë° ê°ì‚¬
                'ì•ˆë…•í•˜ì„¸ìš”', 'ê°ì‚¬í•©ë‹ˆë‹¤', 'ê°ì‚¬ë“œë¦½ë‹ˆë‹¤', 'ë°”ì´ë¸” ì• í”Œì„',
                # ì ‘ì†ì–´ ë° ì—°ê²°ì–´
                'ë”°ë¼ì„œ', 'ê·¸ëŸ¬ë¯€ë¡œ', 'ë˜í•œ', 'ê·¸ë¦¬ê³ ', 'ë˜ëŠ”', 'í•˜ì§€ë§Œ', 'ê·¸ëŸ°ë°',
                'ì´ì™¸', 'ì´ì—', 'ì´ë¥¼', 'ì´ë¡œ', 'ì´ì™€', 'ì´ì—', 'ì´ìƒ', 'ì´í•˜',
                # ìƒí™© ì„¤ëª…
                'í˜„ì¬', 'ì§€ê¸ˆ', 'í˜„ì¬ë¡œ', 'í˜„ì¬ê¹Œì§€', 'í˜„ì¬ë¡œì„œëŠ”',
                'ë‚´ë¶€ì ìœ¼ë¡œ', 'ì™¸ë¶€ì ìœ¼ë¡œ', 'ê¸°ìˆ ì ìœ¼ë¡œ', 'ìš´ì˜ìƒ',
                # ì¡°ê±´ ë° ê°€ì •
                'ë§Œì•½', 'í˜¹ì‹œ', 'ë§Œì¼', 'ë§Œì•½ì—', 'ë§Œì•½ì˜',
                'í•´ë‹¹', 'ì´', 'ê·¸', 'ì €', 'ì´ëŸ°', 'ê·¸ëŸ°', 'ì €ëŸ°',
                # ìš”ì²­ ë° ì•ˆë‚´
                'ì„±ë„ë‹˜', 'ê³ ê°ë‹˜', 'ì´ìš©ì', 'ì‚¬ìš©ì',
                'ë²ˆê±°ë¡œìš°ì‹œ', 'ë¶ˆí¸í•˜ì‹œ', 'ì£„ì†¡í•˜ì§€ë§Œ', 'ì°¸ê³ ë¡œ',
                'ì–‘í•´ë¶€íƒë“œë¦½ë‹ˆë‹¤', 'ì–‘í•´í•´ì£¼ì‹œê¸°', 'ì´í•´í•´ì£¼ì‹œê¸°',
                # ì‹œê°„ ê´€ë ¨
                'í•­ìƒ', 'ëŠ˜', 'ì•ìœ¼ë¡œë„', 'ì§€ì†ì ìœ¼ë¡œ', 'ê³„ì†ì ìœ¼ë¡œ',
                'ì‹œê°„ì´', 'ì†Œìš”ë ', 'ê±¸ë¦´', 'í•„ìš”í•œ',
                # ê¸°ëŠ¥ ê´€ë ¨
                'ê¸°ëŠ¥', 'ê¸°ëŠ¥ì€', 'ê¸°ëŠ¥ì˜', 'ê¸°ëŠ¥ì´', 'ê¸°ëŠ¥ì„',
                'ìŠ¤í”¼ì»¤', 'ë²„íŠ¼', 'ë©”ë‰´', 'í™”ë©´', 'ì„¤ì •', 'ì˜µì…˜',
                # ì˜ê²¬ ë° ì „ë‹¬
                'ì˜ê²¬ì€', 'ì˜ê²¬ì„', 'ì „ë‹¬í• ', 'ì „ë‹¬í•˜ê² ìŠµë‹ˆë‹¤', 'ì „ë‹¬ë“œë¦¬ê² ìŠµë‹ˆë‹¤',
                'í† ì˜ê°€', 'ê²€í† ê°€', 'ê²€í† ë¥¼', 'ë…¼ì˜ê°€', 'ë…¼ì˜ë¥¼'
            ]
        else:  # ì˜ì–´
            paragraph_triggers = [
                # Greetings and appreciation
                'Hello', 'Hi', 'Dear', 'Thank', 'Thanks', 'Appreciate',
                'Grateful', 'Welcome', 'Greetings',
                
                # Conjunctions and transitions
                'Therefore', 'However', 'Additionally', 'Furthermore', 
                'Moreover', 'Nevertheless', 'Nonetheless', 'Meanwhile',
                'Subsequently', 'Consequently', 'Hence', 'Thus', 'Besides',
                'Although', 'Though', 'While', 'Whereas', 'Instead',
                
                # Situation descriptions
                'Currently', 'Presently', 'At the moment', 'Now',
                'At this time', 'As of now', 'Recently', 'Lately',
                'Technically', 'Internally', 'Externally', 'Generally',
                'Specifically', 'Basically', 'Essentially', 'Fundamentally',
                
                # Conditions and assumptions
                'If', 'When', 'Where', 'Whether', 'Unless', 'Provided',
                'Assuming', 'Suppose', 'In case', 'Should', 'Would',
                'Could', 'Might', 'May',
                
                # Requests and guidance
                'Please', 'Kindly', 'We recommend', 'We suggest',
                'You can', 'You may', 'You should', 'You might',
                'Try', 'Consider', 'Note that', 'Be aware',
                'Remember', 'Keep in mind', 'Important',
                
                # Apologies and understanding
                'Sorry', 'Apologize', 'Apologies', 'Unfortunately',
                'Regret', 'Understand', 'Realize', 'Acknowledge',
                'We know', 'We understand', 'We appreciate',
                
                # Time-related
                'Always', 'Usually', 'Often', 'Sometimes', 'Occasionally',
                'Frequently', 'Regularly', 'Continuously', 'Constantly',
                'Soon', 'Shortly', 'Eventually', 'Later', 'Previously',
                
                # Feature and function related
                'Feature', 'Function', 'Option', 'Setting', 'Button',
                'Menu', 'Screen', 'Tab', 'Page', 'Section', 'Tool',
                'Service', 'System', 'Application', 'Update', 'Version',
                
                # Problem-solving related
                'To fix', 'To solve', 'To resolve', 'To address',
                'Solution', 'Resolution', 'Workaround', 'Alternative',
                'Issue', 'Problem', 'Error', 'Bug', 'Trouble',
                
                # Feedback and communication
                'Your feedback', 'Your suggestion', 'Your opinion',
                'We will', 'We are', 'We have', 'Our team',
                'Will be', 'Has been', 'Have been', 'Working on',
                'Looking into', 'Reviewing', 'Considering', 'Planning',
                
                # Instructions and steps
                'First', 'Second', 'Third', 'Next', 'Then', 'After',
                'Before', 'Finally', 'Lastly', 'To begin', 'To start',
                'Step', 'Follow', 'Navigate', 'Click', 'Tap', 'Select',
                
                # Emphasis and clarification
                'Indeed', 'In fact', 'Actually', 'Certainly', 'Definitely',
                'Clearly', 'Obviously', 'Importantly', 'Notably',
                'Particularly', 'Especially', 'Specifically'
            ]
        
        for i, sentence in enumerate(sentences):
            sentence = sentence.strip()
            if not sentence:
                continue
            
            # ì²« ë²ˆì§¸ ë¬¸ì¥ (ì¸ì‚¬ë§)ì€ í•­ìƒ ë³„ë„ ë‹¨ë½
            if i == 0:
                if current_paragraph:
                    paragraphs.append(' '.join(current_paragraph))
                    current_paragraph = []
                paragraphs.append(sentence)
                continue
            
            should_break = False
            
            # íŠ¸ë¦¬ê±° í‚¤ì›Œë“œë¡œ ì‹œì‘í•˜ëŠ” ë¬¸ì¥ì€ ìƒˆ ë‹¨ë½
            for trigger in paragraph_triggers:
                if sentence.startswith(trigger):
                    should_break = True
                    break
            
            # í˜„ì¬ ë‹¨ë½ì— 2ê°œ ì´ìƒ ë¬¸ì¥ì´ ìˆìœ¼ë©´ ìƒˆ ë‹¨ë½
            if current_paragraph and len(current_paragraph) >= 2:
                should_break = True

            # ëë§ºìŒë§ì´ í¬í•¨ëœ ë¬¸ì¥ì€ ìƒˆ ë‹¨ë½
            if any(closing in sentence for closing in ['ê°ì‚¬í•©ë‹ˆë‹¤', 'ê°ì‚¬ë“œë¦½ë‹ˆë‹¤', 'í‰ì•ˆí•˜ì„¸ìš”', 'ì£¼ë‹˜ ì•ˆì—ì„œ']):
                should_break = True
            
            # ë¬¸ì¥ ê¸¸ì´ê°€ 50ì ì´ìƒì´ê³  í˜„ì¬ ë‹¨ë½ì´ ìˆìœ¼ë©´ ìƒˆ ë‹¨ë½
            if len(sentence) > 50 and current_paragraph:
                should_break = True
            
            # ìƒˆ ë‹¨ë½ ë¶„ë¦¬
            if should_break and current_paragraph:
                paragraphs.append(' '.join(current_paragraph))
                current_paragraph = [sentence]
            else:
                current_paragraph.append(sentence)
        
        # ë§ˆì§€ë§‰ ë‹¨ë½ ì¶”ê°€
        if current_paragraph:
            paragraphs.append(' '.join(current_paragraph))
        
        # Quill ì—ë””í„° í˜¸í™˜ì„ ìœ„í•œ HTML ë‹¨ë½ìœ¼ë¡œ ë³€í™˜
        # ê° ë‹¨ë½ì„ <p> íƒœê·¸ë¡œ ê°ì‹¸ê³ , ë‹¨ë½ ì‚¬ì´ì— <p><br></p> íƒœê·¸ë¡œ ë¹ˆ ì¤„ ì¶”ê°€
        html_paragraphs = []
        for i, paragraph in enumerate(paragraphs):
            html_paragraphs.append(f"<p>{paragraph}</p>")
            
            # ë‹¨ë½ ì‚¬ì´ì— ë¹ˆ ì¤„ ì¶”ê°€ (ë§ˆì§€ë§‰ ë‹¨ë½ ì œì™¸)
            if i < len(paragraphs) - 1:
                html_paragraphs.append("<p><br></p>")
        
        return ''.join(html_paragraphs)

    # â˜† ë‹µë³€ í…ìŠ¤íŠ¸ë¥¼ ì •ë¦¬í•˜ê³  í¬ë§·íŒ…í•˜ëŠ” ë©”ì„œë“œ (Quill ì—ë””í„°ìš©)
    def clean_answer_text(self, text: str) -> str:
        if not text:
            return ""
        
        # ì œì–´ ë¬¸ìë§Œ ì œê±°í•˜ê³  HTML íƒœê·¸ëŠ” ìœ ì§€
        text = re.sub(r'[\b\r\f\v]', '', text)
        text = re.sub(r'[\x00-\x08\x0B\x0C\x0E-\x1F\x7F]', '', text)

        # HTML íƒœê·¸ ì œê±°í•˜ì§€ ì•ŠìŒ (Quill ì—ë””í„°ìš©)
        text = re.sub(r'\*\*([^*]+)\*\*', r'\1', text)
        text = re.sub(r'\*([^*]+)\*', r'\1', text)
        
        # HTML íƒœê·¸ ë‚´ë¶€ì˜ ê³µë°±ë§Œ ì •ë¦¬ (íƒœê·¸ ìì²´ëŠ” ìœ ì§€)
        text = re.sub(r'>\s+<', '><', text)  # íƒœê·¸ ì‚¬ì´ ê³µë°± ì œê±°
        text = re.sub(r'<p>\s+', '<p>', text)  # <p> íƒœê·¸ ë‚´ë¶€ ì• ê³µë°± ì œê±°
        text = re.sub(r'\s+</p>', '</p>', text)  # </p> íƒœê·¸ ì• ê³µë°± ì œê±°
        
        text = self.remove_old_app_name(text)
        text = self.format_answer_with_html_paragraphs(text)
        
        return text

    # â˜† í…ìŠ¤íŠ¸ ìœ íš¨ì„± ê²€ì¦ ë©”ì„œë“œ
    def is_valid_text(self, text: str, lang: str = 'ko') -> bool:
        if not text or len(text.strip()) < 3:
            return False
        
        if lang == 'ko':
            return self.is_valid_korean_text(text)
        else:  # ì˜ì–´
            return self.is_valid_english_text(text)

    # â˜† í•œêµ­ì–´ í…ìŠ¤íŠ¸ì˜ ìœ íš¨ì„±ì„ ê²€ì¦í•˜ëŠ” ë©”ì„œë“œ
    def is_valid_korean_text(self, text: str) -> bool:
        if not text or len(text.strip()) < 3:
            logging.info(f"í•œêµ­ì–´ ê²€ì¦ ì‹¤íŒ¨: í…ìŠ¤íŠ¸ê°€ ë„ˆë¬´ ì§§ìŒ (ê¸¸ì´: {len(text.strip()) if text else 0})")
            return False
        
        korean_chars = len(re.findall(r'[ê°€-í£]', text))
        total_chars = len(re.sub(r'\s', '', text))
        
        if total_chars == 0:
            logging.info("í•œêµ­ì–´ ê²€ì¦ ì‹¤íŒ¨: ì´ ê¸€ì ìˆ˜ê°€ 0")
            return False
            
        korean_ratio = korean_chars / total_chars
        logging.info(f"í•œêµ­ì–´ ë¹„ìœ¨: {korean_ratio:.3f} (í•œêµ­ì–´: {korean_chars}, ì „ì²´: {total_chars})")
        
        # í•œêµ­ì–´ ë¹„ìœ¨ ê¸°ì¤€ì„ ì™„í™” (0.2 â†’ 0.1)
        if korean_ratio < 0.1:
            logging.info(f"í•œêµ­ì–´ ê²€ì¦ ì‹¤íŒ¨: í•œêµ­ì–´ ë¹„ìœ¨ ë¶€ì¡± ({korean_ratio:.3f} < 0.1)")
            return False
        
        # ë¬´ì˜ë¯¸í•œ íŒ¨í„´ ê°ì§€ (GPT í• ë£¨ì‹œë„¤ì´ì…˜ ë°©ì§€)
        meaningless_patterns = [
            r'^[a-z\s\.,;:\(\)\[\]\-_&\/\'"]+$', # ì˜ì–´
            r'^[A-Z\s\.,;:\(\)\[\]\-_&\/\'"]+$', # ì˜ì–´ ëŒ€ë¬¸ì
            r'^[\s\.,;:\(\)\[\]\-_&\/\'"]+$',    # ê³µë°±
            r'^[0-9\s\.,;:\(\)\[\]\-_&\/\'"]+$', # ìˆ«ì
            r'.*[Ğ°-Ñ].*',                        # ëŸ¬ì‹œì•„ì–´
            r'.*[Î±-Ï‰].*',                        # ê·¸ë¦¬ìŠ¤ì–´
        ]
        
        for pattern in meaningless_patterns:
            if re.match(pattern, text, re.IGNORECASE):
                logging.info(f"í•œêµ­ì–´ ê²€ì¦ ì‹¤íŒ¨: ë¬´ì˜ë¯¸í•œ íŒ¨í„´ ê°ì§€")
                return False
        
        # ë°˜ë³µ ë¬¸ì ê°ì§€: ê°™ì€ ë¬¸ìê°€ 5ë²ˆ ì´ìƒ ì—°ì†ìœ¼ë¡œ ë‚˜íƒ€ë‚˜ë©´ ë¹„ì •ìƒ í…ìŠ¤íŠ¸ë¡œ ê°„ì£¼
        if re.search(r'(.)\1{5,}', text):
            logging.info("í•œêµ­ì–´ ê²€ì¦ ì‹¤íŒ¨: ë°˜ë³µ ë¬¸ì ê°ì§€")
            return False
        
        # ì˜ì–´ ë¹„ìœ¨ ê²€ì‚¬ë¥¼ ì™„í™” (0.5 â†’ 0.7)
        random_pattern = r'[a-zA-Z]{8,}'
        if re.search(random_pattern, text) and korean_ratio < 0.3:
            logging.info(f"í•œêµ­ì–´ ê²€ì¦ ì‹¤íŒ¨: ê¸´ ì˜ì–´ ë‹¨ì–´ì™€ ë‚®ì€ í•œêµ­ì–´ ë¹„ìœ¨")
            return False
        
        logging.info("í•œêµ­ì–´ ê²€ì¦ ì„±ê³µ")
        return True

    # â˜† ì˜ì–´ í…ìŠ¤íŠ¸ì˜ ìœ íš¨ì„±ì„ ê²€ì¦í•˜ëŠ” ë©”ì„œë“œ
    def is_valid_english_text(self, text: str) -> bool:
        if not text or len(text.strip()) < 3:
            return False
        
        english_chars = len(re.findall(r'[a-zA-Z]', text))
        total_chars = len(re.sub(r'\s', '', text))
        
        if total_chars == 0:
            return False
            
        english_ratio = english_chars / total_chars
        
        if english_ratio < 0.7:  # ì˜ì–´ ë¹„ìœ¨ì´ 70% ë¯¸ë§Œì´ë©´ ë¬´íš¨
            return False
        
        # ë°˜ë³µ ë¬¸ì ê°ì§€
        if re.search(r'(.)\1{5,}', text):
            return False
        
        return True

    # â˜† ìƒì„±ëœ í…ìŠ¤íŠ¸ë¥¼ ì •ë¦¬í•˜ê³  ê²€ì¦í•˜ëŠ” ë©”ì„œë“œ
    def clean_generated_text(self, text: str) -> str:
        if not text:
            return ""
        # ì œì–´ ë¬¸ì ì œê±°
        text = re.sub(r'[\x00-\x08\x0B\x0C\x0E-\x1F\x7F]', '', text)
        text = re.sub(r'[\b\r\f\v]', '', text)

        # ì˜ì–´ ì•½ì–´ ì œê±°
        text = re.sub(r'\b[a-z]{1,2}\b(?:\s+[a-z]{1,2}\b)*', '', text, flags=re.IGNORECASE)
        text = re.sub(r'[Ğ°-Ñ]+', '', text)
        text = re.sub(r'[Î±-Ï‰]+', '', text)

        # í•œê¸€ ë¬¸ì ì œê±°
        text = re.sub(r'[^\w\sê°€-í£.,!?()"\'-]{3,}', '', text)
        text = re.sub(r'[.,;:!?]{3,}', '.', text)

        # ê³µë°± ì •ë¦¬
        text = re.sub(r'\s+', ' ', text)
        text = text.strip()
        
        return text

    # â˜† í†µì¼ëœ GPT í”„ë¡¬í”„íŠ¸ ìƒì„± ë©”ì„œë“œ (ëª¨ë“ˆí™”)
    def get_gpt_prompts(self, query: str, context: str, lang: str = 'ko') -> tuple:
        """ì–¸ì–´ë³„ GPT í”„ë¡¬í”„íŠ¸ ìƒì„±"""
        if lang == 'en': # ì˜ì–´
            system_prompt = """You are a GOODTV Bible App customer service representative.

Guidelines:
1. Follow the style and content of the provided reference answers faithfully
2. Find and apply solutions from similar situations in the reference answers
3. Adapt to the customer's specific situation while maintaining the tone and style of the reference answers

âš ï¸ Absolute Prohibitions:
- Do not guide non-existent features or menus
- Do not create specific settings methods or button locations
- If a feature is not in the reference answers, say "Sorry, this feature is currently not available"
- If uncertain, respond with "We will review this internally"

4. For feature requests or improvement suggestions, use:
   - "Thank you for your valuable feedback"
   - "We will discuss/review this internally"
   - "We will forward this as an improvement"

5. Address customers as 'Dear user' or similar polite forms
6. Use 'GOODTV Bible App' or 'Bible App' as the app name

ğŸš« Do NOT generate greetings or closings:
- Do not use "Hello", "Thank you", "Best regards", etc.
- Do not use "God bless", "In Christ", etc.
- Only write the main content

7. Do not use HTML tags, write in natural sentences"""

            user_prompt = f"""Customer inquiry: {query}

Reference answers (main content only, greetings and closings removed):
{context}

Based on the reference answers' solution methods and tone, write a specific answer to the customer's problem.
Important: Do not include greetings or closings. Only write the main content."""

        else:  # í•œêµ­ì–´
            system_prompt = """ë‹¹ì‹ ì€ GOODTV ë°”ì´ë¸” ì• í”Œ ê³ ê°ì„¼í„° ìƒë‹´ì›ì…ë‹ˆë‹¤.

ğŸ¯ í•µì‹¬ ì›ì¹™:
1. ê³ ê°ì˜ ì§ˆë¬¸ì„ ì •í™•íˆ ì´í•´í•˜ê³  ì§ˆë¬¸ ì˜ë„ì— ë§ëŠ” ì ì ˆí•œ ë‹µë³€ì„ ì œê³µí•˜ì„¸ìš”
2. ì°¸ê³  ë‹µë³€ì´ ì§ˆë¬¸ê³¼ ê´€ë ¨ì„±ì´ ìˆë‹¤ë©´ ì´ë¥¼ í™œìš©í•˜ì—¬ ë‹µë³€í•˜ì„¸ìš”
3. ì°¸ê³  ë‹µë³€ì´ ì§ˆë¬¸ê³¼ ê´€ë ¨ì„±ì´ ë‚®ë‹¤ë©´ ê³ ê° ì§ˆë¬¸ì— ë§ëŠ” ìƒˆë¡œìš´ ë‹µë³€ì„ ìƒì„±í•˜ì„¸ìš”

ğŸ“‹ ë‹µë³€ ì‘ì„± ì§€ì¹¨:

âœ… ì§ˆë¬¸ë³„ ì ì ˆí•œ ëŒ€ì‘:
- ë¬¸ì˜ ìœ í˜•ê³¼ ë‚´ìš©ì„ ì •í™•íˆ íŒŒì•…í•˜ì—¬ ê·¸ì— ë§ëŠ” ë‹µë³€ ì œê³µ
- ê³ ê°ì˜ êµ¬ì²´ì ì¸ ìš”ì²­ì‚¬í•­ì— ì§ì ‘ì ìœ¼ë¡œ ëŒ€ì‘
- ì‹¤ì œ ì¡´ì¬í•˜ëŠ” ê¸°ëŠ¥ê³¼ ì •ì±…ì— ê¸°ë°˜í•œ ì •í™•í•œ ì •ë³´ ì œê³µ

ğŸ” ë‹µë³€ í’ˆì§ˆ ê¸°ì¤€:
- ì§ˆë¬¸ì˜ í•µì‹¬ ì˜ë„ì™€ ì¼ì¹˜í•˜ëŠ” ë‚´ìš©ìœ¼ë¡œ ë‹µë³€
- ëª¨í˜¸í•˜ê±°ë‚˜ íšŒí”¼ì ì¸ í‘œí˜„ë³´ë‹¤ëŠ” êµ¬ì²´ì ì´ê³  ëª…í™•í•œ ì•ˆë‚´
- ê³ ê°ì˜ ë¬¸ì œ í•´ê²°ì— ì‹¤ì§ˆì ìœ¼ë¡œ ë„ì›€ì´ ë˜ëŠ” ë‚´ìš©

âš ï¸ ë‹µë³€ ì¼ê´€ì„± ìœ ì§€:
- ì§ˆë¬¸ ìœ í˜•ê³¼ ì „í˜€ ë‹¤ë¥¸ ë‚´ìš©ì˜ ë‹µë³€ ì§€ì–‘
- í•˜ë‚˜ì˜ ë¬¸ì˜ì— ëŒ€í•´ ì¼ê´€ëœ ì£¼ì œì™€ í•´ê²°ë°©í–¥ ì œì‹œ
- ë¶ˆí™•ì‹¤í•œ ì •ë³´ë³´ë‹¤ëŠ” í™•ì‹¤í•œ ë²”ìœ„ ë‚´ì—ì„œ ë‹µë³€

ğŸš« í˜•ì‹ ì œí•œì‚¬í•­:
- ì¸ì‚¬ë§("ì•ˆë…•í•˜ì„¸ìš”", "ê°ì‚¬í•©ë‹ˆë‹¤" ë“±) ì‚¬ìš© ê¸ˆì§€
- ëë§ºìŒë§("í‰ì•ˆí•˜ì„¸ìš”", "ì£¼ë‹˜ ì•ˆì—ì„œ" ë“±) ì‚¬ìš© ê¸ˆì§€  
- ë³¸ë¬¸ ë‚´ìš©ë§Œ ì‘ì„±í•˜ê³  ê²©ì‹ì  í‘œí˜„ ìƒëµ

ğŸ’¡ ì°½ì˜ì  ë¬¸ì œí•´ê²°:
- ì°¸ê³  ë‹µë³€ì´ ë¶€ì ì ˆí•  ë•ŒëŠ” ê³ ê° ìƒí™©ì— ë§ëŠ” ìƒˆë¡œìš´ í•´ê²°ì±… ì œì‹œ
- ë°”ì´ë¸” ì• í”Œì˜ ì‹¤ì œ ì„œë¹„ìŠ¤ ë²”ìœ„ ë‚´ì—ì„œ í˜„ì‹¤ì ì¸ ë‹µë³€ ì œê³µ
- ê³ ê° ê´€ì ì—ì„œ ë„ì›€ì´ ë˜ëŠ” ì‹¤ìš©ì ì¸ ì¡°ì–¸ í¬í•¨"""

            user_prompt = f"""ê³ ê° ë¬¸ì˜: {query}

ì°¸ê³  ë‹µë³€ë“¤:
{context}

â— ì¤‘ìš” ì§€ì‹œì‚¬í•­:
ìœ„ ì°¸ê³  ë‹µë³€ë“¤ì´ ê³ ê°ì˜ ì§ˆë¬¸ê³¼ ê´€ë ¨ì´ ìˆëŠ”ì§€ ë¨¼ì € íŒë‹¨í•˜ì„¸ìš”.
- ê´€ë ¨ì´ ìˆë‹¤ë©´: ì°¸ê³  ë‹µë³€ì˜ í•´ê²° ë°©ì‹ì„ í™œìš©í•˜ì—¬ ë‹µë³€í•˜ì„¸ìš”
- ê´€ë ¨ì´ ì—†ë‹¤ë©´: ì°¸ê³  ë‹µë³€ì„ ì°¸ê³ í•˜ì—¬ ìƒˆë¡œìš´ ë‹µë³€ì„ ì‘ì„±í•˜ì„¸ìš”

ê³ ê°ì˜ êµ¬ì²´ì ì¸ ì§ˆë¬¸ì— ì •í™•íˆ ë§ëŠ” ë‹µë³€ë§Œ ì‘ì„±í•˜ì„¸ìš”.
ì¸ì‚¬ë§ì´ë‚˜ ëë§ºìŒë§ ì—†ì´ ë³¸ë¬¸ ë‚´ìš©ë§Œ ì‘ì„±í•˜ì„¸ìš”."""

        return system_prompt, user_prompt

    # â˜† í–¥ìƒëœ GPT ìƒì„± - í†µì¼ëœ í”„ë¡¬í”„íŠ¸ ì‚¬ìš©
    def generate_with_enhanced_gpt(self, query: str, similar_answers: list, context_analysis: dict, lang: str = 'ko') -> str:
        try:
            with memory_cleanup():
                approach = context_analysis['recommended_approach']
                context = self.create_enhanced_context(similar_answers, target_lang=lang)
                
                if not context:
                    logging.warning("ìœ íš¨í•œ ì»¨í…ìŠ¤íŠ¸ê°€ ì—†ì–´ GPT ìƒì„± ì¤‘ë‹¨")
                    return ""
                
                # í†µì¼ëœ í”„ë¡¬í”„íŠ¸ ìƒì„±
                system_prompt, user_prompt = self.get_gpt_prompts(query, context, lang)
                
                # ğŸ”¥ ì ‘ê·¼ ë°©ì‹ë³„ temperatureì™€ max_tokens ì„¤ì • ê°œì„ 
                if approach == 'gpt_with_strong_context':
                    # ê´€ë ¨ì„±ì´ ë†’ì€ ê²½ìš° ë” ì°½ì˜ì ìœ¼ë¡œ ì„¤ì •
                    temperature = 0.7 if context_analysis.get('context_relevance') == 'high' else 0.6
                    max_tokens = 700
                elif approach == 'gpt_with_weak_context':
                    # ê´€ë ¨ì„±ì´ ë‚®ì€ ê²½ìš° ë” ì°½ì˜ì ìœ¼ë¡œ ì„¤ì •í•˜ì—¬ ìƒˆë¡œìš´ ë‹µë³€ ìƒì„± ìœ ë„
                    temperature = 0.8
                    max_tokens = 650
                else: # fallbackì´ë‚˜ ê¸°íƒ€
                    return ""
                
                # GPT API í˜¸ì¶œ
                response = self.openai_client.chat.completions.create(
                    model=GPT_MODEL,
                    messages=[
                        {"role": "system", "content": system_prompt},
                        {"role": "user", "content": user_prompt}
                    ],
                    max_tokens=max_tokens,
                    temperature=temperature,
                    top_p=0.8,
                    frequency_penalty=0.1,
                    presence_penalty=0.1
                )
                
                generated = response.choices[0].message.content.strip()
                del response
                
                # ìƒì„±ëœ í…ìŠ¤íŠ¸ ì •ë¦¬
                generated = self.clean_generated_text(generated)
                
                # ğŸ”¥ ë‹µë³€ ê´€ë ¨ì„± ì¶”ê°€ ê²€ì¦ (AI ê¸°ë°˜)
                if not self.validate_answer_relevance_ai(generated, query, context_analysis.get('question_analysis', {})):
                    logging.warning(f"ìƒì„±ëœ ë‹µë³€ì´ ì§ˆë¬¸ê³¼ ê´€ë ¨ì„±ì´ ë‚®ìŒ: {generated[:50]}...")
                    return ""
                
                # í…ìŠ¤íŠ¸ ìœ íš¨ì„± ê²€ì¦ (ì™„í™”)
                if not self.is_valid_text(generated, lang):
                    logging.warning(f"GPTê°€ ë¬´íš¨í•œ í…ìŠ¤íŠ¸ ìƒì„±: {generated[:50]}...")
                    # ìœ íš¨ì„± ê²€ì¦ ì‹¤íŒ¨í•´ë„ ê´€ë ¨ì„±ì´ ë†’ìœ¼ë©´ ì‚¬ìš©
                    if context_analysis.get('context_relevance') == 'high':
                        logging.info("ê´€ë ¨ì„±ì´ ë†’ì•„ ìœ íš¨ì„± ê²€ì¦ ìš°íšŒ")
                    else:
                        return ""
                
                logging.info(f"GPT ìƒì„± ì„±ê³µ ({approach}, ì–¸ì–´: {lang}): {len(generated)}ì")
                return generated[:650]
                
        except Exception as e:
            logging.error(f"í–¥ìƒëœ GPT ìƒì„± ì‹¤íŒ¨: {e}")
            return ""

    # â˜† AI ê¸°ë°˜ ë‹µë³€ ê´€ë ¨ì„± ê²€ì¦ ë©”ì„œë“œ (ì—„ê²©í•œ ê²€ì¦ ë²„ì „)
    def validate_answer_relevance_ai(self, answer: str, query: str, question_analysis: dict) -> bool:
        """AIë¥¼ ì´ìš©í•´ ìƒì„±ëœ ë‹µë³€ì´ ì§ˆë¬¸ê³¼ ê´€ë ¨ì„±ì´ ìˆëŠ”ì§€ ì—„ê²©í•˜ê²Œ ê²€ì¦"""
        
        try:
            with memory_cleanup():
                system_prompt = """ë‹¹ì‹ ì€ ë‹µë³€ í’ˆì§ˆ ê²€ì¦ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
ìƒì„±ëœ ë‹µë³€ì´ ê³ ê°ì˜ ì§ˆë¬¸ì— ì ì ˆíˆ ëŒ€ì‘í•˜ëŠ”ì§€ ì—„ê²©í•˜ê²Œ í‰ê°€í•˜ì„¸ìš”.

âš ï¸ ì—„ê²©í•œ í‰ê°€ ê¸°ì¤€:
1. ë‹µë³€ì´ ì§ˆë¬¸ì˜ í•µì‹¬ í–‰ë™ ìš”ì²­ê³¼ ì¼ì¹˜í•˜ëŠ”ê°€? (ë³µì‚¬â‰ ì¬ìƒ)
2. ë‹µë³€ì´ ì§ˆë¬¸ì˜ ì£¼ì œ ì˜ì—­ê³¼ ì¼ì¹˜í•˜ëŠ”ê°€? (í…ìŠ¤íŠ¸â‰ ìŒì„±)
3. ë‹µë³€ì´ ì‹¤ì œ ë¬¸ì œ í•´ê²°ì— ì§ì ‘ì ìœ¼ë¡œ ë„ì›€ì´ ë˜ëŠ”ê°€?
4. ë‹µë³€ì—ì„œ ì–¸ê¸‰í•˜ëŠ” ê¸°ëŠ¥ì´ ì§ˆë¬¸ì—ì„œ ìš”ì²­í•œ ê¸°ëŠ¥ê³¼ ê°™ì€ê°€?

ğŸš« ë¶€ì ì ˆí•œ ë‹µë³€ ì˜ˆì‹œ:
- í…ìŠ¤íŠ¸ ë³µì‚¬ ì§ˆë¬¸ì— ìŒì„± ì¬ìƒ ë‹µë³€
- ê²€ìƒ‰ ê¸°ëŠ¥ ì§ˆë¬¸ì— ì„¤ì • ë³€ê²½ ë‹µë³€  
- ì˜¤ë¥˜ ì‹ ê³ ì— ì¼ë°˜ ì‚¬ìš©ë²• ë‹µë³€
- êµ¬ì²´ì  ê¸°ëŠ¥ ì§ˆë¬¸ì— ì¶”ìƒì  ì•ˆë‚´ ë‹µë³€

ê²°ê³¼: "relevant" ë˜ëŠ” "irrelevant" ì¤‘ í•˜ë‚˜ë§Œ ë°˜í™˜í•˜ì„¸ìš”."""

                user_prompt = f"""ì§ˆë¬¸ ë¶„ì„:
ì˜ë„: {question_analysis.get('intent_type', 'N/A')}
ì£¼ì œ: {question_analysis.get('main_topic', 'N/A')}
í–‰ë™ìœ í˜•: {question_analysis.get('action_type', 'N/A')}
ìš”ì²­ì‚¬í•­: {question_analysis.get('specific_request', 'N/A')}

ì›ë³¸ ì§ˆë¬¸: {query}

ìƒì„±ëœ ë‹µë³€: {answer}

âš ï¸ íŠ¹íˆ ì£¼ì˜: ì§ˆë¬¸ì˜ í–‰ë™ìœ í˜•ê³¼ ë‹µë³€ì—ì„œ ë‹¤ë£¨ëŠ” í–‰ë™ì´ ë‹¤ë¥´ë©´ "irrelevant"ì…ë‹ˆë‹¤.
ì´ ë‹µë³€ì´ ì§ˆë¬¸ì— ì ì ˆí•œì§€ ì—„ê²©í•˜ê²Œ í‰ê°€í•´ì£¼ì„¸ìš”."""

                response = self.openai_client.chat.completions.create(
                    model='gpt-3.5-turbo',
                    messages=[
                        {"role": "system", "content": system_prompt},
                        {"role": "user", "content": user_prompt}
                    ],
                    max_tokens=30,
                    temperature=0.1
                )
                
                result = response.choices[0].message.content.strip().lower()
                
                is_relevant = 'relevant' in result and 'irrelevant' not in result
                
                logging.info(f"AI ë‹µë³€ ê´€ë ¨ì„± ê²€ì¦: {result} -> {is_relevant}")
                
                return is_relevant
                
        except Exception as e:
            logging.error(f"AI ë‹µë³€ ê´€ë ¨ì„± ê²€ì¦ ì‹¤íŒ¨: {e}")
            # í´ë°±: ê¸°ë³¸ì ì¸ í‚¤ì›Œë“œ ë§¤ì¹­
            query_keywords = set(self.extract_keywords(query.lower()))
            answer_keywords = set(self.extract_keywords(answer.lower()))
            
            keyword_overlap = len(query_keywords & answer_keywords)
            keyword_relevance = keyword_overlap / max(len(query_keywords), 1)
            
            return keyword_relevance >= 0.2  # 20% ì´ìƒ í‚¤ì›Œë“œ ì¼ì¹˜ì‹œ ê´€ë ¨ì„± ìˆìŒìœ¼ë¡œ íŒë‹¨

    # â˜† ìµœì ì˜ í´ë°± ë‹µë³€ ì„ íƒ ë©”ì„œë“œ (ì§ì ‘ ì‚¬ìš© ë‹µë³€ í¬í•¨)
    def get_best_fallback_answer(self, similar_answers: list, lang: str = 'ko') -> str:
        logging.info(f"=== get_best_fallback_answer ì‹œì‘ ===")
        logging.info(f"ì…ë ¥ëœ similar_answers ê°œìˆ˜: {len(similar_answers)}")
        
        if not similar_answers:
            logging.warning("similar_answersê°€ ë¹„ì–´ìˆìŒ")
            return ""
        
        # ì…ë ¥ëœ ë‹µë³€ë“¤ ë¯¸ë¦¬ë³´ê¸°
        for i, ans in enumerate(similar_answers[:3]):
            logging.info(f"ë‹µë³€ #{i+1}: ì ìˆ˜={ans['score']:.3f}, ê¸¸ì´={len(ans.get('answer', ''))}, ë‚´ìš©ë¯¸ë¦¬ë³´ê¸°={ans.get('answer', '')[:50]}...")
        
        # ì ìˆ˜ì™€ í…ìŠ¤íŠ¸ í’ˆì§ˆì„ ì¢…í•© í‰ê°€
        best_answer = ""
        best_score = 0
        
        for i, ans in enumerate(similar_answers[:3]): # ìƒìœ„ 3ê°œë§Œ ê²€í†  (í’ˆì§ˆ í–¥ìƒ)
            logging.info(f"--- ë‹µë³€ #{i+1} ì²˜ë¦¬ ì‹œì‘ ---")
            score = ans['score']
            answer_text = ans['answer']  # ì›ë³¸ ë‹µë³€ í…ìŠ¤íŠ¸ ì‚¬ìš©
            logging.info(f"ì›ë³¸ ë‹µë³€ ê¸¸ì´: {len(answer_text)}, ë‚´ìš©: {answer_text[:100]}...")
            
            # ğŸ”¥ ê¸´ê¸‰ ìˆ˜ì •: 0.9 ì´ìƒ ì ìˆ˜ë©´ ì „ì²˜ë¦¬ ì—†ì´ ë°”ë¡œ ë°˜í™˜
            if score >= 0.9:
                logging.info(f"ğŸ”¥ ë§¤ìš° ë†’ì€ ìœ ì‚¬ë„({score:.3f}) - ì „ì²˜ë¦¬ ì—†ì´ ì›ë³¸ ë‹µë³€ ë°”ë¡œ ë°˜í™˜")
                print(f"ğŸ”¥ ë§¤ìš° ë†’ì€ ìœ ì‚¬ë„({score:.3f}) - ì „ì²˜ë¦¬ ì—†ì´ ì›ë³¸ ë‹µë³€ ë°”ë¡œ ë°˜í™˜")
                # ìµœì†Œí•œì˜ ì •ë¦¬ë§Œ
                clean_answer = answer_text.strip()
                if clean_answer:
                    logging.info(f"ğŸ”¥ ì›ë³¸ ë‹µë³€ ì§ì ‘ ë°˜í™˜: ê¸¸ì´={len(clean_answer)}")
                    return clean_answer
            
            # ê¸°ë³¸ ì •ë¦¬ë§Œ ìˆ˜í–‰
            answer_text = self.preprocess_text(answer_text)
            logging.info(f"ì „ì²˜ë¦¬ í›„ ê¸¸ì´: {len(answer_text)}, ë‚´ìš©: {answer_text[:100]}...")
            
            # ì˜ì–´ ì§ˆë¬¸ì¸ ê²½ìš° ë‹µë³€ì„ ë²ˆì—­
            if lang == 'en' and ans.get('lang', 'ko') == 'ko':
                answer_text = self.translate_text(answer_text, 'ko', 'en')
                logging.info(f"ë²ˆì—­ í›„ ê¸¸ì´: {len(answer_text)}")
            
            # ìœ íš¨ì„± ê²€ì‚¬ (ì„ì‹œë¡œ ìš°íšŒ - ë””ë²„ê¹…ìš©)
            is_valid = self.is_valid_text(answer_text, lang)
            logging.info(f"ìœ íš¨ì„± ê²€ì‚¬ ê²°ê³¼: {is_valid}")
            
            # ì„ì‹œë¡œ ìœ íš¨ì„± ê²€ì‚¬ ë¬´ì‹œí•˜ê³  ì§„í–‰
            if not is_valid:
                logging.warning(f"âš ï¸ ë‹µë³€ #{i+1} ìœ íš¨ì„± ê²€ì‚¬ ì‹¤íŒ¨í–ˆì§€ë§Œ ê°•ì œë¡œ ì§„í–‰")
                # continueë¥¼ ì£¼ì„ ì²˜ë¦¬í•˜ì—¬ ìœ íš¨ì„± ê²€ì‚¬ ì‹¤íŒ¨í•´ë„ ê³„ì† ì§„í–‰
            
            # ë†’ì€ ìœ ì‚¬ë„(0.8+)ì¸ ê²½ìš° ê°„ë‹¨í•˜ê²Œ ì²« ë²ˆì§¸ ë‹µë³€ ì„ íƒ
            if score >= 0.8:
                logging.info(f"ë†’ì€ ìœ ì‚¬ë„({score:.3f})ë¡œ ë‹µë³€ #{i+1} ì§ì ‘ ì„ íƒ")
                logging.info(f"ì„ íƒëœ ë‹µë³€ ìµœì¢… ê¸¸ì´: {len(answer_text)}")
                # ğŸ”¥ ìœ íš¨ì„± ê²€ì‚¬ ì‹¤íŒ¨í•´ë„ ê°•ì œë¡œ ë°˜í™˜
                if answer_text and len(answer_text.strip()) > 0:
                    return answer_text
                else:
                    logging.error(f"ğŸ”¥ ì „ì²˜ë¦¬ í›„ ë‹µë³€ì´ ë¹„ì–´ìˆìŒ! ì›ë³¸ìœ¼ë¡œ í´ë°±")
                    return ans['answer'].strip()
            
            # ì¢…í•© ì ìˆ˜ ê³„ì‚° (ìœ ì‚¬ë„ + í…ìŠ¤íŠ¸ ê¸¸ì´ + ì™„ì„±ë„)
            length_score = min(len(answer_text) / 200, 1.0) # 200ì ê¸°ì¤€ ì •ê·œí™”
            completeness_score = 1.0 if answer_text.endswith(('.', '!', '?')) else 0.8
            
            total_score = score * 0.8 + length_score * 0.1 + completeness_score * 0.1  # ìœ ì‚¬ë„ ê°€ì¤‘ì¹˜ ì¦ê°€
            
            logging.info(f"ë‹µë³€ #{i+1} ì¢…í•© ì ìˆ˜: {total_score:.3f} (ìœ ì‚¬ë„:{score:.3f}, ê¸¸ì´:{length_score:.3f}, ì™„ì„±ë„:{completeness_score:.3f})")
            
            if total_score > best_score:
                best_score = total_score
                best_answer = answer_text
                logging.info(f"ìƒˆë¡œìš´ ìµœê³  ì ìˆ˜ ë‹µë³€ìœ¼ë¡œ ì„ íƒë¨")
        
        logging.info(f"=== get_best_fallback_answer ì™„ë£Œ ===")
        logging.info(f"ìµœì¢… ì„ íƒëœ ë‹µë³€ ì ìˆ˜: {best_score:.3f}")
        logging.info(f"ìµœì¢… ë‹µë³€ ê¸¸ì´: {len(best_answer)}")
        logging.info(f"ìµœì¢… ë‹µë³€ ë¯¸ë¦¬ë³´ê¸°: {best_answer[:100] if best_answer else 'None'}...")
        
        # ğŸ”¥ ê¸´ê¸‰ ì•ˆì „ì¥ì¹˜: ë‹µë³€ì´ ë¹„ì–´ìˆìœ¼ë©´ ì²« ë²ˆì§¸ ì›ë³¸ ë‹µë³€ ê°•ì œ ë°˜í™˜
        if not best_answer and similar_answers:
            logging.error("ğŸš¨ ìµœì¢… ë‹µë³€ì´ ë¹„ì–´ìˆìŒ! ì²« ë²ˆì§¸ ì›ë³¸ ë‹µë³€ ê°•ì œ ë°˜í™˜")
            print("ğŸš¨ ìµœì¢… ë‹µë³€ì´ ë¹„ì–´ìˆìŒ! ì²« ë²ˆì§¸ ì›ë³¸ ë‹µë³€ ê°•ì œ ë°˜í™˜")
            emergency_answer = similar_answers[0]['answer'].strip()
            logging.info(f"ğŸš¨ ê¸´ê¸‰ ë‹µë³€ ê¸¸ì´: {len(emergency_answer)}")
            return emergency_answer
        
        return best_answer

    # â˜† ë” ë³´ìˆ˜ì ì¸ GPT-3.5-turbo ìƒì„± ë©”ì„œë“œ (ê¸°ì¡´ ì½”ë“œì™€ì˜ í˜¸í™˜ì„± ìœ ì§€)
    # ë³´ìˆ˜ì ì´ê³  ì°¸ê³  ë‹µë³€ì— ì¶©ì‹¤í•œ GPT-3.5-turbo í…ìŠ¤íŠ¸ ìƒì„±
    @profile
    def generate_ai_answer(self, query: str, similar_answers: list, lang: str) -> str:
        
        # 1. ì–¸ì–´ ê°ì§€ (lang íŒŒë¼ë¯¸í„°ê°€ ì—†ê±°ë‚˜ 'auto'ì¸ ê²½ìš°)
        if not lang or lang == 'auto':
            detected_lang = self.detect_language(query)
            lang = detected_lang
            logging.info(f"ê°ì§€ëœ ì–¸ì–´: {lang}")
        
        # 2. ìœ ì‚¬ ë‹µë³€ì´ ì—†ëŠ” ê²½ìš°
        if not similar_answers:
            logging.error("ğŸš¨ ìœ ì‚¬ ë‹µë³€ì´ ì „í˜€ ì—†ìŒ - Pinecone ê²€ìƒ‰ ì‹¤íŒ¨ ë˜ëŠ” ë°ì´í„° ì—†ìŒ")
            print(f"ğŸš¨ CRITICAL: ìœ ì‚¬ ë‹µë³€ì´ ì „í˜€ ì—†ìŒ! query='{query[:50]}...', lang='{lang}'")
            if lang == 'en':
                default_msg = "<p>We need more detailed information to provide an accurate answer to your inquiry.</p><p><br></p><p>Please contact our customer service center for prompt assistance.</p>"
            else:
                default_msg = "<p>ì•ˆë…•í•˜ì„¸ìš”, GOODTV ë°”ì´ë¸” ì• í”Œì…ë‹ˆë‹¤.</p><p><br></p><p>ë°”ì´ë¸” ì• í”Œì„ ì´ìš©í•´ ì£¼ì…”ì„œ ì§„ì‹¬ìœ¼ë¡œ ê°ì‚¬ë“œë¦½ë‹ˆë‹¤.</p><p><br></p><p>ë‚¨ê²¨ì£¼ì‹  ë¬¸ì˜ëŠ” í˜„ì¬ ë‹´ë‹¹ìê°€ ì§ì ‘ í™•ì¸í•˜ê³  ìˆìŠµë‹ˆë‹¤.</p><p><br></p><p>ì„±ë„ë‹˜ê»˜ ë„ì›€ì´ ë  ìˆ˜ ìˆë„ë¡ ë‚´ìš©ì„ ê¼¼ê¼¼íˆ ì‚´í´</p><p><br></p><p>ì •í™•í•˜ê³  êµ¬ì²´ì ì¸ ë‹µë³€ì„ ì¤€ë¹„í•˜ê² ìŠµë‹ˆë‹¤.</p><p><br></p><p>ë‹µë³€ì€ ìµœëŒ€ í•˜ë£¨ ì´ë‚´ì— ë“œë¦´ ì˜ˆì •ì´ì˜¤ë‹ˆ</p><p><br></p><p>ì¡°ê¸ˆë§Œ ê¸°ë‹¤ë ¤ ì£¼ì‹œë©´ ê°ì‚¬í•˜ê² ìŠµë‹ˆë‹¤.</p><p><br></p><p>í•­ìƒ ì£¼ë‹˜ ì•ˆì—ì„œ í‰ì•ˆí•˜ì„¸ìš”, ê°ì‚¬í•©ë‹ˆë‹¤.</p>"
            return default_msg
        
        # 3. ì»¨í…ìŠ¤íŠ¸ ë¶„ì„
        context_analysis = self.analyze_context_quality(similar_answers, query)
        
        # 4. ê²€ìƒ‰ ê²°ê³¼ ìƒì„¸ ë¡œê¹…
        logging.info(f"âœ… ê²€ìƒ‰ëœ ìœ ì‚¬ ë‹µë³€ ê°œìˆ˜: {len(similar_answers)}")
        print(f"âœ… ê²€ìƒ‰ëœ ìœ ì‚¬ ë‹µë³€ ê°œìˆ˜: {len(similar_answers)}")
        
        if similar_answers:
            for i, ans in enumerate(similar_answers[:3]):
                log_msg = f"ğŸ“ ë‹µë³€ #{i+1}: ì ìˆ˜={ans['score']:.3f}, ì¹´í…Œê³ ë¦¬={ans['category']}"
                logging.info(log_msg)
                print(log_msg)
        
        # 5. ë‹µë³€ì´ ì „í˜€ ì—†ì„ ë•Œë§Œ ê¸°ë³¸ ë©”ì‹œì§€ ë°˜í™˜ (ì¤‘ë³µ ì²´í¬ ì œê±°)
        # ì´ë¯¸ ìœ„ì—ì„œ ì²´í¬í–ˆìœ¼ë¯€ë¡œ ì´ ë¶€ë¶„ì€ ì‹¤í–‰ë˜ì§€ ì•Šì•„ì•¼ í•¨
        

        try:
            approach = context_analysis['recommended_approach']
            logging.info(f"=== ì ‘ê·¼ ë°©ì‹ ê²°ì • ===")
            logging.info(f"ğŸ¯ ì„ íƒëœ ì ‘ê·¼ ë°©ì‹: {approach}, ì–¸ì–´: {lang}")
            logging.info(f"ğŸ“Š ì»¨í…ìŠ¤íŠ¸ ë¶„ì„: {context_analysis}")
            
            # ì½˜ì†”ì—ë„ ì¶œë ¥
            print(f"ğŸ¯ ì„ íƒëœ ì ‘ê·¼ ë°©ì‹: {approach}, ì–¸ì–´: {lang}")
            print(f"ğŸ“Š ì»¨í…ìŠ¤íŠ¸ ë¶„ì„: {context_analysis}")
            
            base_answer = ""
            
            if approach == 'direct_use':
                logging.info("=== ì§ì ‘ ì‚¬ìš© ë°©ì‹ ì‹œì‘ ===")
                base_answer = self.get_best_fallback_answer(similar_answers[:3], lang)
                logging.info(f"ì§ì ‘ ì‚¬ìš© ê²°ê³¼ ê¸¸ì´: {len(base_answer) if base_answer else 0}")
                logging.info(f"ì§ì ‘ ì‚¬ìš© ê²°ê³¼ ë¯¸ë¦¬ë³´ê¸°: {base_answer[:100] if base_answer else 'None'}...")
                
            elif approach in ['gpt_with_strong_context', 'gpt_with_weak_context']:
                logging.info(f"=== GPT ìƒì„± ë°©ì‹ ì‹œì‘: {approach} ===")
                base_answer = self.generate_with_enhanced_gpt(query, similar_answers, context_analysis, lang)
                logging.info(f"GPT ìƒì„± ê²°ê³¼ ê¸¸ì´: {len(base_answer) if base_answer else 0}")
                
                if not base_answer or not self.is_valid_text(base_answer, lang):
                    logging.warning("GPT ìƒì„± ì‹¤íŒ¨, í´ë°± ë‹µë³€ ì‚¬ìš©")
                    base_answer = self.get_best_fallback_answer(similar_answers, lang)
                    logging.info(f"í´ë°± ë‹µë³€ ê¸¸ì´: {len(base_answer) if base_answer else 0}")
                    
            elif approach == 'smart_fallback':
                logging.info("=== ìŠ¤ë§ˆíŠ¸ í´ë°± ë°©ì‹ ì‚¬ìš© ===")
                base_answer = self.generate_smart_fallback_answer(query, context_analysis.get('question_analysis', {}), lang)
                logging.info(f"ìŠ¤ë§ˆíŠ¸ í´ë°± ë‹µë³€ ê¸¸ì´: {len(base_answer) if base_answer else 0}")
                    
            else:
                logging.info("=== ì¼ë°˜ í´ë°± ë°©ì‹ ì‚¬ìš© ===")
                base_answer = self.get_best_fallback_answer(similar_answers, lang)
                logging.info(f"ì¼ë°˜ í´ë°± ë‹µë³€ ê¸¸ì´: {len(base_answer) if base_answer else 0}")
            
            # ìµœì¢… ê²€ì¦ ì „ ìƒì„¸ ë¡œê¹…
            logging.info(f"=== ìµœì¢… ê²€ì¦ ì‹œì‘ ===")
            logging.info(f"base_answer ì¡´ì¬ ì—¬ë¶€: {base_answer is not None and base_answer != ''}")
            if base_answer:
                logging.info(f"base_answer ê¸¸ì´: {len(base_answer)}")
                logging.info(f"base_answer ë¯¸ë¦¬ë³´ê¸°: {base_answer[:200]}...")
                is_valid = self.is_valid_text(base_answer, lang)
                logging.info(f"is_valid_text ê²°ê³¼: {is_valid}")
                if not is_valid:
                    logging.warning(f"ìœ íš¨ì„± ê²€ì‚¬ ì‹¤íŒ¨ ì‚¬ìœ  ë¶„ì„ ì¤‘...")
                    # í•œêµ­ì–´ í…ìŠ¤íŠ¸ ë¶„ì„
                    if lang == 'ko':
                        korean_chars = len(re.findall(r'[ê°€-í£]', base_answer))
                        total_chars = len(re.sub(r'\s', '', base_answer))
                        korean_ratio = korean_chars / total_chars if total_chars > 0 else 0
                        logging.warning(f"í•œêµ­ì–´ ë¹„ìœ¨: {korean_ratio:.2f} (ê¸°ì¤€: 0.2 ì´ìƒ)")
                        logging.warning(f"í•œêµ­ì–´ ê¸€ì ìˆ˜: {korean_chars}, ì „ì²´ ê¸€ì ìˆ˜: {total_chars}")
            else:
                logging.error("base_answerê°€ ë¹„ì–´ìˆìŒ")
            
            # ğŸ”¥ ê¸´ê¸‰ ìˆ˜ì •: is_valid_text ê²€ì¦ ì™„ì „íˆ ìš°íšŒ
            if not base_answer:
                logging.error("=== base_answerê°€ ë¹„ì–´ìˆìŒ ===")
                logging.error(f"similar_answers ê°œìˆ˜: {len(similar_answers)}")
                if similar_answers:
                    logging.error(f"ì²« ë²ˆì§¸ ë‹µë³€ ì ìˆ˜: {similar_answers[0]['score']}")
                    logging.error(f"ì²« ë²ˆì§¸ ë‹µë³€ ë‚´ìš©: {similar_answers[0]['answer'][:100]}...")
                
                if lang == 'en':
                    return "<p>We need more detailed information to provide an accurate answer to your inquiry.</p><p><br></p><p>Please contact our customer service center for prompt assistance.</p>"
                else:
                    return "<p>ì•ˆë…•í•˜ì„¸ìš”, GOODTV ë°”ì´ë¸” ì• í”Œì…ë‹ˆë‹¤.</p><p><br></p><p>ë°”ì´ë¸” ì• í”Œì„ ì´ìš©í•´ ì£¼ì…”ì„œ ì§„ì‹¬ìœ¼ë¡œ ê°ì‚¬ë“œë¦½ë‹ˆë‹¤.</p><p><br></p><p>ë‚¨ê²¨ì£¼ì‹  ë¬¸ì˜ëŠ” í˜„ì¬ ë‹´ë‹¹ìê°€ ì§ì ‘ í™•ì¸í•˜ê³  ìˆìŠµë‹ˆë‹¤.</p><p><br></p><p>ì„±ë„ë‹˜ê»˜ ë„ì›€ì´ ë  ìˆ˜ ìˆë„ë¡ ë‚´ìš©ì„ ê¼¼ê¼¼íˆ ì‚´í´</p><p><br></p><p>ì •í™•í•˜ê³  êµ¬ì²´ì ì¸ ë‹µë³€ì„ ì¤€ë¹„í•˜ê² ìŠµë‹ˆë‹¤.</p><p><br></p><p>ë‹µë³€ì€ ìµœëŒ€ í•˜ë£¨ ì´ë‚´ì— ë“œë¦´ ì˜ˆì •ì´ì˜¤ë‹ˆ</p><p><br></p><p>ì¡°ê¸ˆë§Œ ê¸°ë‹¤ë ¤ ì£¼ì‹œë©´ ê°ì‚¬í•˜ê² ìŠµë‹ˆë‹¤.</p><p><br></p><p>í•­ìƒ ì£¼ë‹˜ ì•ˆì—ì„œ í‰ì•ˆí•˜ì„¸ìš”, ê°ì‚¬í•©ë‹ˆë‹¤.</p>"
            
            # ğŸ”¥ is_valid_text ê²€ì¦ì„ ì„ì‹œë¡œ ì£¼ì„ ì²˜ë¦¬
            # elif not self.is_valid_text(base_answer, lang):
            elif False:  # í•­ìƒ Falseê°€ ë˜ì–´ ì´ ë¸”ë¡ì€ ì‹¤í–‰ë˜ì§€ ì•ŠìŒ
                logging.warning(f"ìœ íš¨ì„± ê²€ì‚¬ ì‹¤íŒ¨í–ˆì§€ë§Œ ë‹µë³€ ì¡´ì¬í•¨ - ê°•ì œ ì§„í–‰")
                # ì´ ë¸”ë¡ì€ ì‹¤í–‰ë˜ì§€ ì•ŠìŒ
            
            # ğŸ”¥ ìŠ¤ë§ˆíŠ¸ í´ë°±ì˜ ê²½ìš° ì¶”ê°€ í¬ë§·íŒ… ê±´ë„ˆë›°ê¸°
            if approach == 'smart_fallback':
                logging.info("ğŸ¯ ìŠ¤ë§ˆíŠ¸ í´ë°± ë‹µë³€ì€ ì´ë¯¸ ì™„ì„±ëœ í˜•íƒœë¡œ ë°”ë¡œ ë°˜í™˜")
                print("ğŸ¯ ìŠ¤ë§ˆíŠ¸ í´ë°± ë‹µë³€ì€ ì´ë¯¸ ì™„ì„±ëœ í˜•íƒœë¡œ ë°”ë¡œ ë°˜í™˜")
                return base_answer
            
            # ğŸ”¥ ì„±ê³µ ë¡œê·¸ ì¶”ê°€
            logging.info("ğŸ‰ ìœ íš¨ì„± ê²€ì‚¬ ìš°íšŒ ì„±ê³µ - ë‹µë³€ í¬ë§·íŒ… ì‹œì‘")
            print("ğŸ‰ ìœ íš¨ì„± ê²€ì‚¬ ìš°íšŒ ì„±ê³µ - ë‹µë³€ í¬ë§·íŒ… ì‹œì‘")
            
            # ì–¸ì–´ë³„ í¬ë§·íŒ…
            if lang == 'en':
                # ì˜ì–´ ë‹µë³€ í¬ë§·íŒ…
                base_answer = self.remove_old_app_name(base_answer)
                
                # ê¸°ì¡´ ì¸ì‚¬ë§/ëë§ºìŒë§ ì œê±°
                base_answer = re.sub(r'^Hello[^.]*\.\s*', '', base_answer, flags=re.IGNORECASE)
                base_answer = re.sub(r'^This is GOODTV Bible App[^.]*\.\s*', '', base_answer, flags=re.IGNORECASE)
                base_answer = re.sub(r'\s*Thank you[^.]*\.?\s*$', '', base_answer, flags=re.IGNORECASE)
                base_answer = re.sub(r'\s*Best regards[^.]*\.?\s*$', '', base_answer, flags=re.IGNORECASE)
                base_answer = re.sub(r'\s*God bless[^.]*\.?\s*$', '', base_answer, flags=re.IGNORECASE)
                
                formatted_body = self.format_answer_with_html_paragraphs(base_answer.strip(), 'en')
                
                # ì˜ì–´ ê³ ì • ì¸ì‚¬ë§ê³¼ ëë§ºìŒë§
                final_answer = "<p>Hello, this is GOODTV Bible Apple App customer service team.</p><p><br></p><p>Thank you very much for using our app and for taking the time to contact us.</p><p><br></p>"
                final_answer += formatted_body
                final_answer += "<p><br></p><p>Thank you once again for sharing your thoughts with us!</p><p><br></p><p>May God's peace and grace always be with you.</p>"
                
            else:  # í•œêµ­ì–´
                # í•œêµ­ì–´ ë‹µë³€ ìµœì¢… í¬ë§·íŒ… (Quill ì—ë””í„°ìš© HTML í˜•ì‹ ìœ ì§€)
                # ì•± ì´ë¦„ ì •ë¦¬ ë° ê³ ê°ë‹˜ â†’ ì„±ë„ë‹˜ ë³€ê²½
                base_answer = self.remove_old_app_name(base_answer)
                base_answer = re.sub(r'ê³ ê°ë‹˜', 'ì„±ë„ë‹˜', base_answer)
                
                # ê¸°ì¡´ ì¸ì‚¬ë§/ëë§ºìŒë§ ì œê±° (ì¼ë°˜ í…ìŠ¤íŠ¸ì—ì„œ)
                # ì¸ì‚¬ë§ ì œê±°
                base_answer = re.sub(r'^ì•ˆë…•í•˜ì„¸ìš”[^.]*\.\s*', '', base_answer, flags=re.IGNORECASE)
                base_answer = re.sub(r'^GOODTV\s+ë°”ì´ë¸”\s*ì• í”Œ[^.]*\.\s*', '', base_answer, flags=re.IGNORECASE)
                base_answer = re.sub(r'^ë°”ì´ë¸”\s*ì• í”Œ[^.]*\.\s*', '', base_answer, flags=re.IGNORECASE)
                base_answer = re.sub(r'ê³ ê°ì„¼í„°[^.]*\.\s*', '', base_answer, flags=re.IGNORECASE)
                
                # ëë§ºìŒë§ ì œê±° (ë” ê°•í™”ëœ íŒ¨í„´) - 'í•­ìƒ' ì¤‘ë³µ ì œê±° í¬í•¨
                base_answer = re.sub(r'\s*ê°ì‚¬í•©ë‹ˆë‹¤[^.]*\.?\s*$', '', base_answer, flags=re.IGNORECASE)
                base_answer = re.sub(r'\s*í‰ì•ˆí•˜ì„¸ìš”[^.]*\.?\s*$', '', base_answer, flags=re.IGNORECASE)
                base_answer = re.sub(r'\s*ì£¼ë‹˜\s*ì•ˆì—ì„œ[^.]*\.?\s*$', '', base_answer, flags=re.IGNORECASE)
                base_answer = re.sub(r'\s*í•¨ê»˜\s*ê¸°ë„í•˜ë©°[^.]*\.?\s*$', '', base_answer, flags=re.IGNORECASE)
                base_answer = re.sub(r'\s*í•­ìƒ[^.]*ë°”ì´ë¸”\s*ì• í”Œ[^.]*\.?\s*$', '', base_answer, flags=re.IGNORECASE)
                
                # ì¶”ê°€ ëë§ºìŒë§ íŒ¨í„´ë“¤ (ë” í¬ê´„ì ìœ¼ë¡œ)
                base_answer = re.sub(r'\s*í•­ìƒ\s*ì£¼ë‹˜\s*ì•ˆì—ì„œ[^.]*\.?\s*$', '', base_answer, flags=re.IGNORECASE)
                base_answer = re.sub(r'\s*ì£¼ë‹˜\s*ì•ˆì—ì„œ\s*í‰ì•ˆí•˜ì„¸ìš”[^.]*\.?\s*$', '', base_answer, flags=re.IGNORECASE)
                base_answer = re.sub(r'\s*í‰ì•ˆí•˜ì„¸ìš”[^.]*\.?\s*$', '', base_answer, flags=re.IGNORECASE)
                
                # ë¬¸ì¥ ëì˜ ëë§ºìŒë§ë“¤ë„ ì œê±°
                base_answer = re.sub(r'[,.!?]\s*í•­ìƒ\s*ì£¼ë‹˜\s*ì•ˆì—ì„œ[^.]*\.?\s*$', '', base_answer, flags=re.IGNORECASE)
                base_answer = re.sub(r'[,.!?]\s*ê°ì‚¬í•©ë‹ˆë‹¤[^.]*\.?\s*$', '', base_answer, flags=re.IGNORECASE)
                base_answer = re.sub(r'[,.!?]\s*í‰ì•ˆí•˜ì„¸ìš”[^.]*\.?\s*$', '', base_answer, flags=re.IGNORECASE)
                
                # ğŸ”¥ êµ¬ ì•± ì´ë¦„ì„ ë°”ì´ë¸” ì• í”Œë¡œ ì™„ì „ êµì²´ (ì¤‘ë³µ ë°©ì§€)
                # ì¤‘ë³µ ë°©ì§€ë¥¼ ìœ„í•´ ìˆœì„œë¥¼ ì¡°ì •: ì „ì²´ íŒ¨í„´ë¶€í„° ì²˜ë¦¬
                base_answer = re.sub(r'ë°”ì´ë¸”\s*ì• í”Œ\s*\(êµ¬\)\s*ë‹¤ë²ˆì—­\s*ì„±ê²½\s*ì°¬ì†¡', 'ë°”ì´ë¸” ì• í”Œ', base_answer, flags=re.IGNORECASE)
                base_answer = re.sub(r'ë°”ì´ë¸”\s*ì• í”Œ\s*\(êµ¬\)\s*ë‹¤ë²ˆì—­ì„±ê²½ì°¬ì†¡', 'ë°”ì´ë¸” ì• í”Œ', base_answer, flags=re.IGNORECASE)
                base_answer = re.sub(r'\(êµ¬\)\s*ë‹¤ë²ˆì—­\s*ì„±ê²½\s*ì°¬ì†¡', 'ë°”ì´ë¸” ì• í”Œ', base_answer, flags=re.IGNORECASE)
                base_answer = re.sub(r'\(êµ¬\)\s*ë‹¤ë²ˆì—­ì„±ê²½ì°¬ì†¡', 'ë°”ì´ë¸” ì• í”Œ', base_answer, flags=re.IGNORECASE)
                base_answer = re.sub(r'ë‹¤ë²ˆì—­\s*ì„±ê²½\s*ì°¬ì†¡', 'ë°”ì´ë¸” ì• í”Œ', base_answer, flags=re.IGNORECASE)
                base_answer = re.sub(r'ë‹¤ë²ˆì—­ì„±ê²½ì°¬ì†¡', 'ë°”ì´ë¸” ì• í”Œ', base_answer, flags=re.IGNORECASE)
                
                # ğŸ”¥ ì™„ì „íˆ ê°•í™”ëœ ì¤‘ë³µ ëë§ºìŒë§ ì œê±° ì‹œìŠ¤í…œ
                # 1ë‹¨ê³„: ëª¨ë“  í˜•íƒœì˜ "í•­ìƒ ì„±ë„ë‹˜ê»˜..." íŒ¨í„´ ì œê±°
                base_answer = re.sub(r'í•­ìƒ\s*ì„±ë„ë‹˜ë“¤?ê»˜\s*ì¢‹ì€\s*(ì„œë¹„ìŠ¤|ì„±ê²½ì•±)ì„?\s*ì œê³µí•˜ê¸°\s*ìœ„í•´\s*ë…¸ë ¥í•˜ëŠ”\s*ë°”ì´ë¸”\s*ì• í”Œì´\s*ë˜ê² ìŠµë‹ˆë‹¤\.?\s*', 
                                   '', base_answer, flags=re.IGNORECASE)
                
                # 2ë‹¨ê³„: ê°ì‚¬í•©ë‹ˆë‹¤ íŒ¨í„´ ì™„ì „ ì œê±°
                base_answer = re.sub(r'ê°ì‚¬í•©ë‹ˆë‹¤\.?\s*(ì£¼ë‹˜\s*ì•ˆì—ì„œ\s*í‰ì•ˆí•˜ì„¸ìš”\.?)?\s*', 
                                   '', base_answer, flags=re.IGNORECASE)
                
                # 3ë‹¨ê³„: ë¶ˆì™„ì „í•œ ë¬¸ì¥ë“¤ ì œê±°
                base_answer = re.sub(r'ì˜¤ëŠ˜ë„\s*$', '', base_answer, flags=re.IGNORECASE)
                base_answer = re.sub(r'ì˜¤ëŠ˜ë„\s*\n', '\n', base_answer, flags=re.IGNORECASE)
                base_answer = re.sub(r'í•­ìƒ\s*$', '', base_answer, flags=re.IGNORECASE)
                
                # ğŸ”¥ 'í•­ìƒ' ë‹¨ë…ìœ¼ë¡œ ë‚¨ì€ ê²½ìš° ì œê±° (ì¤‘ë³µ ë¬¸ì œ í•´ê²°)
                base_answer = re.sub(r'\s*í•­ìƒ\s*$', '', base_answer, flags=re.IGNORECASE)
                base_answer = re.sub(r'\n\s*í•­ìƒ\s*\n', '\n', base_answer, flags=re.IGNORECASE)
                base_answer = re.sub(r'<p>\s*í•­ìƒ\s*</p>', '', base_answer, flags=re.IGNORECASE)
                base_answer = re.sub(r'<p><br></p>\s*<p>\s*í•­ìƒ\s*</p>', '', base_answer, flags=re.IGNORECASE)
                
                # ë³¸ë¬¸ì„ HTML ë‹¨ë½ í˜•ì‹ìœ¼ë¡œ í¬ë§·íŒ…
                formatted_body = self.format_answer_with_html_paragraphs(base_answer.strip(), 'ko')
                
                # í•œêµ­ì–´ ê³ ì • ì¸ì‚¬ë§ (HTML í˜•ì‹ìœ¼ë¡œ)
                final_answer = "<p>ì•ˆë…•í•˜ì„¸ìš”. GOODTV ë°”ì´ë¸” ì• í”Œì…ë‹ˆë‹¤.</p><p><br></p><p>ë°”ì´ë¸” ì• í”Œì„ ì´ìš©í•´ì£¼ì…”ì„œ ê°ì‚¬ë“œë¦½ë‹ˆë‹¤.</p><p><br></p>"
                
                # í¬ë§·íŒ…ëœ ë³¸ë¬¸ ì¶”ê°€ ì „ ìµœì¢… ì •ë¦¬
                # ğŸ”¥ HTML í¬ë§·íŒ… í›„ ì™„ì „í•œ ì •ë¦¬ ì‘ì—…
                # ì¤‘ë³µëœ ëë§ºìŒë§ HTML íƒœê·¸ ì œê±°
                formatted_body = re.sub(r'<p>\s*í•­ìƒ\s*ì„±ë„ë‹˜ë“¤?ê»˜\s*ì¢‹ì€\s*(ì„œë¹„ìŠ¤|ì„±ê²½ì•±)ì„?\s*ì œê³µí•˜ê¸°\s*ìœ„í•´\s*ë…¸ë ¥í•˜ëŠ”\s*ë°”ì´ë¸”\s*ì• í”Œì´\s*ë˜ê² ìŠµë‹ˆë‹¤\.?\s*</p>', '', formatted_body, flags=re.IGNORECASE)
                formatted_body = re.sub(r'<p>\s*ê°ì‚¬í•©ë‹ˆë‹¤\.?\s*(ì£¼ë‹˜\s*ì•ˆì—ì„œ\s*í‰ì•ˆí•˜ì„¸ìš”\.?)?\s*</p>', '', formatted_body, flags=re.IGNORECASE)
                
                # ë¶ˆì™„ì „í•œ ë¬¸ì¥ë“¤ ì œê±°
                formatted_body = re.sub(r'<p>\s*í•­ìƒ\s*</p>', '', formatted_body, flags=re.IGNORECASE)
                formatted_body = re.sub(r'<p>\s*ì˜¤ëŠ˜ë„\s*</p>', '', formatted_body, flags=re.IGNORECASE)
                formatted_body = re.sub(r'<p><br></p>\s*<p>\s*(í•­ìƒ|ì˜¤ëŠ˜ë„)\s*</p>', '', formatted_body, flags=re.IGNORECASE)
                formatted_body = re.sub(r'<p>\s*(í•­ìƒ|ì˜¤ëŠ˜ë„)\s*<br></p>', '', formatted_body, flags=re.IGNORECASE)
                
                # ì—°ì†ëœ ë¹ˆ íƒœê·¸ë“¤ ì •ë¦¬
                formatted_body = re.sub(r'(<p><br></p>\s*){3,}', '<p><br></p><p><br></p>', formatted_body)
                formatted_body = re.sub(r'(<p><br></p>\s*)+$', '', formatted_body)  # ëì˜ ë¹ˆ íƒœê·¸ë“¤ ì œê±°
                
                final_answer += formatted_body
                
                # ê³ ì •ëœ ëë§ºìŒë§ (HTML í˜•ì‹ìœ¼ë¡œ)
                final_answer += "<p><br></p><p>í•­ìƒ ì„±ë„ë‹˜ê»˜ ì¢‹ì€ ì„±ê²½ì•±ì„ ì œê³µí•˜ê¸° ìœ„í•´ ë…¸ë ¥í•˜ëŠ” ë°”ì´ë¸” ì• í”Œì´ ë˜ê² ìŠµë‹ˆë‹¤.</p><p><br></p><p>ê°ì‚¬í•©ë‹ˆë‹¤. ì£¼ë‹˜ ì•ˆì—ì„œ í‰ì•ˆí•˜ì„¸ìš”.</p>"
            
            logging.info(f"ìµœì¢… ë‹µë³€ ìƒì„± ì™„ë£Œ: {len(final_answer)}ì, ì ‘ê·¼ë°©ì‹: {approach}, ì–¸ì–´: {lang}")
            return final_answer
            
        except Exception as e:
            logging.error(f"ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜: {e}")
            if lang == 'en':
                return "<p>Sorry, we cannot generate an answer at this moment.</p><p><br></p><p>Please contact our customer service center.</p>"
            else:
                return "<p>ì•ˆë…•í•˜ì„¸ìš”, GOODTV ë°”ì´ë¸” ì• í”Œì…ë‹ˆë‹¤.</p><p><br></p><p>ë°”ì´ë¸” ì• í”Œì„ ì´ìš©í•´ ì£¼ì…”ì„œ ì§„ì‹¬ìœ¼ë¡œ ê°ì‚¬ë“œë¦½ë‹ˆë‹¤.</p><p><br></p><p>ë‚¨ê²¨ì£¼ì‹  ë¬¸ì˜ëŠ” í˜„ì¬ ë‹´ë‹¹ìê°€ ì§ì ‘ í™•ì¸í•˜ê³  ìˆìŠµë‹ˆë‹¤.</p><p><br></p><p>ì„±ë„ë‹˜ê»˜ ë„ì›€ì´ ë  ìˆ˜ ìˆë„ë¡ ë‚´ìš©ì„ ê¼¼ê¼¼íˆ ì‚´í´</p><p><br></p><p>ì •í™•í•˜ê³  êµ¬ì²´ì ì¸ ë‹µë³€ì„ ì¤€ë¹„í•˜ê² ìŠµë‹ˆë‹¤.</p><p><br></p><p>ë‹µë³€ì€ ìµœëŒ€ í•˜ë£¨ ì´ë‚´ì— ë“œë¦´ ì˜ˆì •ì´ì˜¤ë‹ˆ</p><p><br></p><p>ì¡°ê¸ˆë§Œ ê¸°ë‹¤ë ¤ ì£¼ì‹œë©´ ê°ì‚¬í•˜ê² ìŠµë‹ˆë‹¤.</p><p><br></p><p>í•­ìƒ ì£¼ë‹˜ ì•ˆì—ì„œ í‰ì•ˆí•˜ì„¸ìš”, ê°ì‚¬í•©ë‹ˆë‹¤.</p>"

    # â˜† ë©”ëª¨ë¦¬ ìµœì í™”ëœ ë©”ì¸ ì²˜ë¦¬ ë©”ì„œë“œ
    def process(self, seq: int, question: str, lang: str) -> dict:
        try:
            with memory_cleanup():
                # 1. ì „ì²˜ë¦¬
                processed_question = self.preprocess_text(question)

                # 2. ì˜¤íƒ€ ìˆ˜ì • ì¶”ê°€ (Pinecone ì €ì¥ ì‹œì™€ ë™ì¼í•˜ê²Œ!)
                if lang == 'ko' or lang == 'auto':
                    processed_question = self.fix_korean_typos_with_ai(processed_question)
                    logging.info(f"ì˜¤íƒ€ ìˆ˜ì • ì ìš©: {question[:50]} â†’ {processed_question[:50]}")
                
                if not processed_question:
                    return {"success": False, "error": "ì§ˆë¬¸ì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤."}
                
                # ì–¸ì–´ ìë™ ê°ì§€
                if not lang or lang == 'auto':
                    lang = self.detect_language(processed_question)
                    logging.info(f"ìë™ ê°ì§€ëœ ì–¸ì–´: {lang}")
                
                logging.info(f"ì²˜ë¦¬ ì‹œì‘ - SEQ: {seq}, ì–¸ì–´: {lang}, ì§ˆë¬¸: {processed_question[:50]}...")
                
                # 3. ìœ ì‚¬ ë‹µë³€ ê²€ìƒ‰ (ì´ì œ ì˜¤íƒ€ ìˆ˜ì •ëœ ì§ˆë¬¸ìœ¼ë¡œ ê²€ìƒ‰)
                similar_answers = self.search_similar_answers(processed_question, lang=lang)
                
                # AI ë‹µë³€ ìƒì„± (ì–¸ì–´ íŒŒë¼ë¯¸í„° ì „ë‹¬)
                ai_answer = self.generate_ai_answer(processed_question, similar_answers, lang)
                
                # íŠ¹ìˆ˜ë¬¸ì ì •ë¦¬
                ai_answer = ai_answer.replace('"', '"').replace('"', '"')
                ai_answer = ai_answer.replace(''', "'").replace(''', "'")
                
                result = {
                    "success": True,
                    "answer": ai_answer,
                    "similar_count": len(similar_answers),
                    "embedding_model": "text-embedding-3-small",
                    "generation_model": "gpt-3.5-turbo",
                    "detected_language": lang
                }
                
                logging.info(f"ì²˜ë¦¬ ì™„ë£Œ - SEQ: {seq}, ì–¸ì–´: {lang}, HTML ë‹µë³€ ìƒì„±ë¨")
                return result
                
        except Exception as e:
            logging.error(f"ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ - SEQ: {seq}, ì˜¤ë¥˜: {str(e)}")
            return {"success": False, "error": str(e)}

    # â˜† ë‹¨ìˆœí™”ëœ ì»¨í…ìŠ¤íŠ¸ í’ˆì§ˆ ë¶„ì„ ë©”ì„œë“œ (ê·œì¹™ ê¸°ë°˜)
    def analyze_context_quality_simple(self, similar_answers: list, query: str) -> dict:
        """ìœ ì‚¬ë„ ì ìˆ˜ ê¸°ë°˜ì˜ ë‹¨ìˆœí•˜ê³  ëª…í™•í•œ í’ˆì§ˆ ë¶„ì„"""
        
        if not similar_answers:
            return {
                'has_good_context': False,
                'best_score': 0.0,
                'recommended_approach': 'fallback',
                'quality_level': 'none',
                'top_scores': []
            }
        
        # ìƒìœ„ 5ê°œ ë‹µë³€ì˜ ì ìˆ˜ë§Œ í™•ì¸
        top_5_scores = [ans['score'] for ans in similar_answers[:5]]
        best_score = top_5_scores[0] if top_5_scores else 0.0
        
        # ì ìˆ˜ ë¶„í¬ ë¶„ì„
        high_quality_count = len([s for s in top_5_scores if s >= 0.8])
        medium_quality_count = len([s for s in top_5_scores if 0.6 <= s < 0.8])
        
        # ëª…í™•í•œ ê·œì¹™ ê¸°ë°˜ ì ‘ê·¼ ë°©ì‹ ê²°ì •
        if best_score >= 0.95:
            approach = 'direct_use'
            quality_level = 'excellent'
        elif best_score >= 0.85 and high_quality_count >= 2:
            approach = 'direct_use'
            quality_level = 'very_high'
        elif best_score >= 0.75:
            approach = 'gpt_with_strong_context'
            quality_level = 'high'
        elif best_score >= 0.6 and (high_quality_count + medium_quality_count) >= 2:
            approach = 'gpt_with_strong_context'
            quality_level = 'medium'
        elif best_score >= 0.45:
            approach = 'gpt_with_weak_context'
            quality_level = 'low'
        else:
            approach = 'fallback'
            quality_level = 'very_low'
        
        return {
            'has_good_context': quality_level in ['excellent', 'very_high', 'high', 'medium'],
            'best_score': best_score,
            'high_quality_count': high_quality_count,
            'medium_quality_count': medium_quality_count,
            'recommended_approach': approach,
            'quality_level': quality_level,
            'top_scores': top_5_scores,
            'context_summary': f"í’ˆì§ˆ: {quality_level}, ìµœê³ ì ìˆ˜: {best_score:.3f}, ê³ í’ˆì§ˆ: {high_quality_count}ê°œ"
        }

    # â˜† ë‹¨ìˆœí™”ëœ ê²€ìƒ‰ ê²°ê³¼ í•„í„°ë§ (ì„ê³„ê°’ ê¸°ë°˜)
    def search_similar_answers(self, query: str, top_k: int = 8, lang: str = 'ko') -> list:
        """ë‹¨ìˆœí™”ëœ ìœ ì‚¬ ë‹µë³€ ê²€ìƒ‰ - ëª…í™•í•œ ì ìˆ˜ ê¸°ë°˜ í•„í„°ë§"""
        try:
            with memory_cleanup():
                logging.info(f"=== ë‹¨ìˆœí™”ëœ ê²€ìƒ‰ ì‹œì‘ ===")
                logging.info(f"ê²€ìƒ‰ ì§ˆë¬¸: {query[:100]}")
                
                # ì˜¤íƒ€ ìˆ˜ì • (í•œêµ­ì–´ë§Œ)
                if lang == 'ko':
                    corrected_query = self.fix_korean_typos_with_ai(query)
                    query_to_embed = corrected_query
                else:
                    query_to_embed = query
                
                # ì„ë² ë”© ìƒì„±
                query_vector = self.create_embedding(query_to_embed)
                if query_vector is None:
                    logging.error("ì„ë² ë”© ìƒì„± ì‹¤íŒ¨")
                    return []
                
                # Pinecone ê²€ìƒ‰ (ë” ë§ì´ ê²€ìƒ‰í•´ì„œ ì¢‹ì€ ê²°ê³¼ í™•ë³´)
                results = index.query(
                    vector=query_vector,
                    top_k=top_k * 3,  # 3ë°° ë” ê²€ìƒ‰
                    include_metadata=True
                )
                
                # ì˜ì–´ ì§ˆë¬¸ì¸ ê²½ìš° í•œêµ­ì–´ ë²ˆì—­ìœ¼ë¡œ ì¶”ê°€ ê²€ìƒ‰
                if lang == 'en':
                    korean_query = self.translate_text(query_to_embed, 'en', 'ko')
                    korean_vector = self.create_embedding(korean_query)
                    if korean_vector:
                        korean_results = index.query(
                            vector=korean_vector,
                            top_k=top_k,
                            include_metadata=True
                        )
                        # ê²°ê³¼ ë³‘í•© ë° ì¤‘ë³µ ì œê±°
                        seen_ids = set()
                        merged_matches = []
                        for match in results['matches'] + korean_results['matches']:
                            if match['id'] not in seen_ids:
                                seen_ids.add(match['id'])
                                merged_matches.append(match)
                        results['matches'] = sorted(merged_matches, key=lambda x: x['score'], reverse=True)
                
                # ë‹¨ìˆœí•œ ì ìˆ˜ ê¸°ë°˜ í•„í„°ë§
                filtered_results = []
                for i, match in enumerate(results['matches'][:top_k*2]):  # ìƒìœ„ 2ë°°ë§Œ ê²€í† 
                    score = match['score']
                    question = match['metadata'].get('question', '')
                    answer = match['metadata'].get('answer', '')
                    category = match['metadata'].get('category', 'ì¼ë°˜')
                    
                    # ëª…í™•í•œ í¬í•¨ ê¸°ì¤€
                    should_include = False
                    
                    if score >= 0.4:  # ê¸°ë³¸ ì„ê³„ê°’
                        should_include = True
                    elif i < 5:  # ìƒìœ„ 5ê°œëŠ” ì ìˆ˜ê°€ ë‚®ì•„ë„ í¬í•¨
                        should_include = True
                    elif score >= 0.3 and len(filtered_results) < 3:  # ìµœì†Œ 3ê°œ ë³´ì¥
                        should_include = True
                    
                    if should_include and len(filtered_results) < top_k:
                        # ê¸°ë³¸ì ì¸ í…ìŠ¤íŠ¸ í’ˆì§ˆ ê²€ì¦ë§Œ
                        if len(answer.strip()) >= 10:  # ìµœì†Œ ê¸¸ì´ë§Œ í™•ì¸
                            filtered_results.append({
                                'score': score,
                                'question': question,
                                'answer': answer,
                                'category': category,
                                'rank': i + 1,
                                'lang': 'ko'
                            })
                            
                            logging.info(f"í¬í•¨: #{i+1} ì ìˆ˜={score:.3f} ì¹´í…Œê³ ë¦¬={category}")
                
                # ë©”ëª¨ë¦¬ ì •ë¦¬
                del results, query_vector
                
                logging.info(f"ê²€ìƒ‰ ì™„ë£Œ: {len(filtered_results)}ê°œ ë‹µë³€ (ì–¸ì–´: {lang})")
                return filtered_results
            
        except Exception as e:
            logging.error(f"ê²€ìƒ‰ ì‹¤íŒ¨: {str(e)}")
            return []

    # â˜† ë‹¨ìˆœí™”ëœ ì»¨í…ìŠ¤íŠ¸ ìƒì„± (ì ìˆ˜ ê¸°ë°˜ ìš°ì„ ìˆœìœ„)
    def create_enhanced_context_simple(self, similar_answers: list, max_answers: int = 6, target_lang: str = 'ko') -> str:
        """ì ìˆ˜ ê¸°ë°˜ì˜ ë‹¨ìˆœí•œ ì»¨í…ìŠ¤íŠ¸ ìƒì„±"""
        
        if not similar_answers:
            return ""
        
        context_parts = []
        used_count = 0
        
        # ì ìˆ˜ ìˆœìœ¼ë¡œ ì •ë ¬ë˜ì–´ ìˆìœ¼ë¯€ë¡œ ìˆœì„œëŒ€ë¡œ ì²˜ë¦¬
        for i, ans in enumerate(similar_answers[:max_answers]):
            if used_count >= max_answers:
                break
            
            score = ans['score']
            answer_text = ans['answer']
            
            # ê¸°ë³¸ ì •ë¦¬
            clean_answer = self.preprocess_text(answer_text)
            clean_answer = self.remove_greeting_and_closing(clean_answer, 'ko')
            
            # ì˜ì–´ ìš”ì²­ì‹œ ë²ˆì—­
            if target_lang == 'en' and ans.get('lang', 'ko') == 'ko':
                clean_answer = self.translate_text(clean_answer, 'ko', 'en')
            
            # ìµœì†Œ í’ˆì§ˆ ê²€ì¦
            if len(clean_answer.strip()) >= 20:
                # ì ìˆ˜ì— ë”°ë¥¸ ê¸¸ì´ ì¡°ì •
                max_length = 500 if score >= 0.8 else 350 if score >= 0.6 else 250
                
                context_parts.append(
                    f"[ì°¸ê³ ë‹µë³€ {used_count+1} - ìœ ì‚¬ë„: {score:.2f}]\n{clean_answer[:max_length]}"
                )
                used_count += 1
        
        logging.info(f"ì»¨í…ìŠ¤íŠ¸ ìƒì„±: {used_count}ê°œ ë‹µë³€ í¬í•¨ (ì–¸ì–´: {target_lang})")
        return "\n\n" + "="*50 + "\n\n".join(context_parts)

    # â˜† ë‹¨ìˆœí™”ëœ GPT ìƒì„± (ëª…í™•í•œ í”„ë¡¬í”„íŠ¸)
    def generate_with_simple_gpt(self, query: str, similar_answers: list, context_analysis: dict, lang: str = 'ko') -> str:
        """ë‹¨ìˆœí™”ëœ GPT ë‹µë³€ ìƒì„± - ë³µì¡í•œ ê²€ì¦ ì œê±°"""
        
        try:
            with memory_cleanup():
                approach = context_analysis['recommended_approach']
                quality_level = context_analysis['quality_level']
                
                # ì ‘ê·¼ ë°©ì‹ì´ GPT ìƒì„±ì´ ì•„ë‹ˆë©´ ë¹ˆ ë¬¸ìì—´ ë°˜í™˜
                if approach not in ['gpt_with_strong_context', 'gpt_with_weak_context']:
                    return ""
                
                context = self.create_enhanced_context_simple(similar_answers, target_lang=lang)
                if not context:
                    return ""
                
                # ë‹¨ìˆœí™”ëœ í”„ë¡¬í”„íŠ¸ ìƒì„±
                system_prompt, user_prompt = self.get_gpt_prompts(query, context, lang)
                
                # í’ˆì§ˆì— ë”°ë¥¸ ë‹¨ìˆœí•œ íŒŒë¼ë¯¸í„° ì„¤ì •
                if quality_level in ['high', 'medium']:
                    temperature = 0.6
                    max_tokens = 650
                else:  # low
                    temperature = 0.7
                    max_tokens = 600
                
                # GPT í˜¸ì¶œ
                response = self.openai_client.chat.completions.create(
                    model=GPT_MODEL,
                    messages=[
                        {"role": "system", "content": system_prompt},
                        {"role": "user", "content": user_prompt}
                    ],
                    max_tokens=max_tokens,
                    temperature=temperature,
                    top_p=0.85,
                    frequency_penalty=0.1,
                    presence_penalty=0.1
                )
                
                generated = response.choices[0].message.content.strip()
                del response
                
                # ê¸°ë³¸ì ì¸ ì •ë¦¬ë§Œ
                generated = self.clean_generated_text(generated)
                
                # ìµœì†Œ ê¸¸ì´ ê²€ì¦ë§Œ
                if len(generated.strip()) < 10:
                    logging.warning("ìƒì„±ëœ ë‹µë³€ì´ ë„ˆë¬´ ì§§ìŒ")
                    return ""
                
                logging.info(f"GPT ìƒì„± ì„±ê³µ ({approach}, í’ˆì§ˆ: {quality_level}): {len(generated)}ì")
                return generated
                
        except Exception as e:
            logging.error(f"GPT ìƒì„± ì‹¤íŒ¨: {e}")
            return ""

    # â˜† ë‹¨ìˆœí™”ëœ í´ë°± ë‹µë³€ ì„ íƒ (ì ìˆ˜ ê¸°ë°˜)
    def get_best_fallback_answer_simple(self, similar_answers: list, lang: str = 'ko') -> str:
        """ì ìˆ˜ ê¸°ë°˜ì˜ ë‹¨ìˆœí•œ ìµœì  ë‹µë³€ ì„ íƒ"""
        
        if not similar_answers:
            return ""
        
        # ìƒìœ„ 3ê°œ ì¤‘ì—ì„œ ì„ íƒ
        for i, ans in enumerate(similar_answers[:3]):
            score = ans['score']
            answer_text = ans['answer']
            
            # ì ìˆ˜ê°€ ë§¤ìš° ë†’ìœ¼ë©´ ì¦‰ì‹œ ë°˜í™˜
            if score >= 0.9:
                logging.info(f"ìµœê³  ì ìˆ˜({score:.3f}) ë‹µë³€ ì§ì ‘ ì‚¬ìš©")
                clean_answer = answer_text.strip()
                return clean_answer if clean_answer else ""
            
            # ê¸°ë³¸ ì „ì²˜ë¦¬
            processed = self.preprocess_text(answer_text)
            
            # ì˜ì–´ ë²ˆì—­
            if lang == 'en' and ans.get('lang', 'ko') == 'ko':
                processed = self.translate_text(processed, 'ko', 'en')
            
            # ê¸°ë³¸ í’ˆì§ˆ ê²€ì¦
            if len(processed.strip()) >= 20:
                # ì²« ë²ˆì§¸ ìœ íš¨í•œ ë‹µë³€ ì„ íƒ
                logging.info(f"í´ë°± ë‹µë³€ ì„ íƒ: #{i+1}, ì ìˆ˜={score:.3f}")
                return processed
        
        # ëª¨ë“  ë‹µë³€ì´ ë¶€ì ì ˆí•œ ê²½ìš° ì²« ë²ˆì§¸ ì›ë³¸ ë°˜í™˜
        if similar_answers:
            return similar_answers[0]['answer'].strip()
        
        return ""

# ==================================================
# 8. Pinecone ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ë™ê¸°í™” í´ë˜ìŠ¤
# ==================================================
# MSSQL ìš´ì˜ ë°ì´í„°ë² ì´ìŠ¤ì™€ Pinecone ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ê°„ì˜ ë™ê¸°í™”ë¥¼ ë‹´ë‹¹í•˜ëŠ” í´ë˜ìŠ¤
# 
# ì£¼ìš” ê¸°ëŠ¥:
# 1. MSSQLì—ì„œ ìƒˆë¡œìš´ Q&A ë°ì´í„° ì¡°íšŒ
# 2. AIë¥¼ ì´ìš©í•œ í•œêµ­ì–´ ì˜¤íƒ€ ìˆ˜ì •
# 3. OpenAIë¡œ ì„ë² ë”© ë²¡í„° ìƒì„±
# 4. Pineconeì— ë²¡í„° ë°ì´í„° ì €ì¥/ìˆ˜ì •/ì‚­ì œ
# 
# ìš´ì˜ ì‹œë‚˜ë¦¬ì˜¤:
# - ìƒˆë¡œìš´ ê³ ê° ë¬¸ì˜ ë‹µë³€ì´ MSSQLì— ì €ì¥ë˜ë©´
# - ì´ í´ë˜ìŠ¤ë¥¼ í†µí•´ Pineconeì— ë™ê¸°í™”í•˜ì—¬
# - í–¥í›„ ìœ ì‚¬ ì§ˆë¬¸ ê²€ìƒ‰ì´ ê°€ëŠ¥í•˜ê²Œ í•¨
class PineconeSyncManager:
    
    # ë™ê¸°í™” ë§¤ë‹ˆì € ì´ˆê¸°í™”
    # ì™¸ë¶€ì—ì„œ ìƒì„±ëœ Pinecone ì¸ë±ìŠ¤ì™€ OpenAI í´ë¼ì´ì–¸íŠ¸ ì°¸ì¡°
    def __init__(self):
        self.index = index                    # Pinecone ë²¡í„° ì¸ë±ìŠ¤
        self.openai_client = openai_client    # OpenAI API í´ë¼ì´ì–¸íŠ¸
    
    # â˜† AIë¥¼ ì´ìš©í•œ í•œêµ­ì–´ ì˜¤íƒ€ ìˆ˜ì • ë©”ì„œë“œ
    def fix_korean_typos_with_ai(self, text: str) -> str:
        if not text or len(text.strip()) < 3:
            return text
        
        # ë„ˆë¬´ ê¸´ í…ìŠ¤íŠ¸ëŠ” ì²˜ë¦¬í•˜ì§€ ì•ŠìŒ (ë¹„ìš© ì ˆì•½)
        if len(text) > 500:
            logging.warning(f"í…ìŠ¤íŠ¸ê°€ ë„ˆë¬´ ê¸¸ì–´ ì˜¤íƒ€ ìˆ˜ì • ê±´ë„ˆëœ€: {len(text)}ì")
            return text
        
        try:
            with memory_cleanup():
                system_prompt = """ë‹¹ì‹ ì€ í•œêµ­ì–´ ë§ì¶¤ë²• ë° ì˜¤íƒ€ êµì • ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

ì§€ì¹¨:
1. ì…ë ¥ëœ í•œêµ­ì–´ í…ìŠ¤íŠ¸ì˜ ë§ì¶¤ë²•ê³¼ ì˜¤íƒ€ë§Œ ìˆ˜ì •í•˜ì„¸ìš”
2. ì›ë¬¸ì˜ ì˜ë¯¸ì™€ ì–´ì¡°ëŠ” ì ˆëŒ€ ë³€ê²½í•˜ì§€ ë§ˆì„¸ìš”
3. ë„ì–´ì“°ê¸°, ë§ì¶¤ë²•, ì¡°ì‚¬ ì‚¬ìš©ë²•ì„ ì •í™•íˆ êµì •í•˜ì„¸ìš”
4. ì•±/ì–´í”Œë¦¬ì¼€ì´ì…˜ ê´€ë ¨ ê¸°ìˆ  ìš©ì–´ëŠ” í‘œì¤€ ìš©ì–´ë¡œ í†µì¼í•˜ì„¸ìš”
5. ìˆ˜ì •ì´ í•„ìš”ì—†ë‹¤ë©´ ì›ë¬¸ ê·¸ëŒ€ë¡œ ë°˜í™˜í•˜ì„¸ìš”
6. ìˆ˜ì •ëœ í…ìŠ¤íŠ¸ë§Œ ë°˜í™˜í•˜ê³  ì¶”ê°€ ì„¤ëª…ì€ í•˜ì§€ ë§ˆì„¸ìš”

ì˜ˆì‹œ:
- "ì–´í”Œì´ ì•ˆë€ë‹¤" â†’ "ì•±ì´ ì•ˆ ë¼ìš”"
- "ë‹¤ìš´ë°›ê¸°ê°€ ì•ˆë˜ìš”" â†’ "ë‹¤ìš´ë¡œë“œê°€ ì•ˆ ë¼ìš”"
- "ì‚­ì¬í•˜ê³ ì‹¶ì–´ìš”" â†’ "ì‚­ì œí•˜ê³  ì‹¶ì–´ìš”"
- "ì—…ë°ì´ë“œí•´ì£¼ì„¸ìš”" â†’ "ì—…ë°ì´íŠ¸í•´ì£¼ì„¸ìš”"
"""

                user_prompt = f"ë‹¤ìŒ í…ìŠ¤íŠ¸ì˜ ë§ì¶¤ë²•ê³¼ ì˜¤íƒ€ë¥¼ ìˆ˜ì •í•´ì£¼ì„¸ìš”:\n\n{text}"

                response = self.openai_client.chat.completions.create(
                    model='gpt-3.5-turbo',
                    messages=[
                        {"role": "system", "content": system_prompt},
                        {"role": "user", "content": user_prompt}
                    ],
                    max_tokens=600,
                    temperature=0.1,  # ë§¤ìš° ë³´ìˆ˜ì ìœ¼ë¡œ ì„¤ì •
                    top_p=0.8,
                    frequency_penalty=0.0,
                    presence_penalty=0.0
                )
                
                corrected_text = response.choices[0].message.content.strip()
                del response # ë©”ëª¨ë¦¬ í•´ì œ
                
                # ê²°ê³¼ ê²€ì¦
                if not corrected_text or len(corrected_text) == 0:
                    logging.warning("AI ì˜¤íƒ€ ìˆ˜ì • ê²°ê³¼ê°€ ë¹„ì–´ìˆìŒ, ì›ë¬¸ ë°˜í™˜")
                    return text
                
                # ë„ˆë¬´ ë§ì´ ë³€ê²½ëœ ê²½ìš° ì˜ì‹¬ìŠ¤ëŸ¬ìš°ë¯€ë¡œ ì›ë¬¸ ë°˜í™˜
                if len(corrected_text) > len(text) * 2:
                    logging.warning("AI ì˜¤íƒ€ ìˆ˜ì • ê²°ê³¼ê°€ ì›ë¬¸ë³´ë‹¤ ë„ˆë¬´ ê¸¸ì–´ì§, ì›ë¬¸ ë°˜í™˜")
                    return text
                
                # ìˆ˜ì • ë‚´ìš©ì´ ìˆìœ¼ë©´ ë¡œê·¸ ê¸°ë¡
                if corrected_text != text:
                    logging.info(f"AI ì˜¤íƒ€ ìˆ˜ì •: '{text[:50]}...' â†’ '{corrected_text[:50]}...'")
                
                return corrected_text
                
        except Exception as e:
            logging.error(f"AI ì˜¤íƒ€ ìˆ˜ì • ì‹¤íŒ¨: {e}")
            # AI ì‹¤íŒ¨ ì‹œ ì›ë¬¸ ê·¸ëŒ€ë¡œ ë°˜í™˜
            return text
        
    # â˜† í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ ë©”ì„œë“œ
    def preprocess_text(self, text: str, for_metadata: bool = False) -> str:
        if not text or text == 'None':
            return ""
        
        text = str(text)
        text = html.unescape(text)
        
        # HTML íƒœê·¸ ì œê±°
        text = re.sub(r'<br\s*/?>', '\n', text, flags=re.IGNORECASE)
        text = re.sub(r'</p>', '\n', text, flags=re.IGNORECASE)
        text = re.sub(r'<p[^>]*>', '', text, flags=re.IGNORECASE)
        text = re.sub(r'<[^>]+>', '', text)
        
        # ìœ ë‹ˆì½”ë“œ ì •ê·œí™”
        text = unicodedata.normalize('NFC', text)
        
        # ê³µë°± ì •ë¦¬
        if for_metadata:
            text = re.sub(r'\n{3,}', '\n\n', text)
            text = re.sub(r'[ \t]+', ' ', text)
        else:
            text = re.sub(r'\s+', ' ', text)
        
        text = text.strip()
        
        # ê¸¸ì´ ì œí•œ
        max_length = 1000 if for_metadata else MAX_TEXT_LENGTH
        if len(text) > max_length:
            text = text[:max_length-3] + "..."
        
        return text
    
    # â˜† OpenAIë¡œ ì„ë² ë”© ìƒì„±í•˜ëŠ” ë©”ì„œë“œ
    def create_embedding(self, text: str) -> Optional[list]:
        try:
            if not text or not text.strip():
                return None
            
            with memory_cleanup():
                response = openai_client.embeddings.create(
                    model=MODEL_NAME,
                    input=text
                )
                return response.data[0].embedding
            
        except Exception as e:
            logging.error(f"ì„ë² ë”© ìƒì„± ì‹¤íŒ¨: {e}")
            return None
    
    # â˜† ì¹´í…Œê³ ë¦¬ ì¸ë±ìŠ¤ë¥¼ ì´ë¦„ìœ¼ë¡œ ë³€í™˜í•˜ëŠ” ë©”ì„œë“œ
    def get_category_name(self, cate_idx: str) -> str:
        return CATEGORY_MAPPING.get(str(cate_idx), 'ì‚¬ìš© ë¬¸ì˜(ê¸°íƒ€)')
    
    # â˜† MSSQLì—ì„œ ë°ì´í„° ì¡°íšŒí•˜ëŠ” ë©”ì„œë“œ
    # íŒŒë¼ë¯¸í„°í™”ëœ ì¿¼ë¦¬ë¡œ SQL ì¸ì ì…˜ ë°©ì§€, ? í”Œë ˆì´ìŠ¤í™€ë”ë¥¼ ì‚¬ìš©í•˜ì—¬ ì•ˆì „í•˜ê²Œ ê°’ì„ ë°”ì¸ë”©
    def get_mssql_data(self, seq: int) -> Optional[Dict]:
        try:
            with memory_cleanup():
                conn = pyodbc.connect(connection_string)
                cursor = conn.cursor()
                
                query = """
                SELECT seq, contents, reply_contents, cate_idx, name, 
                       CONVERT(varchar, regdate, 120) as regdate
                FROM mobile.dbo.bible_inquiry
                WHERE seq = ? AND answer_YN = 'Y'
                """
                
                cursor.execute(query, seq)
                row = cursor.fetchone()
                
                if row:
                    data = {
                        'seq': row[0],
                        'contents': row[1],
                        'reply_contents': row[2],
                        'cate_idx': row[3],
                        'name': row[4],
                        'regdate': row[5]
                    }
                    return data
                
                cursor.close()
                conn.close()
                return None
            
        except Exception as e:
            logging.error(f"MSSQL ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return None
    
    # â˜† MSSQL ë°ì´í„°ë¥¼ Pineconeì— ë™ê¸°í™”í•˜ëŠ” ë©”ì„œë“œ
    def sync_to_pinecone(self, seq: int, mode: str = 'upsert') -> Dict[str, Any]:
        try:
            with memory_cleanup():
                # ì‚­ì œ ëª¨ë“œ
                if mode == 'delete':
                    vector_id = f"qa_bible_{seq}"
                    self.index.delete(ids=[vector_id])
                    logging.info(f"Pineconeì—ì„œ ì‚­ì œ ì™„ë£Œ: {vector_id}")
                    return {"success": True, "message": "ì‚­ì œ ì™„ë£Œ", "seq": seq}
                
                # MSSQLì—ì„œ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
                data = self.get_mssql_data(seq)
                if not data:
                    return {"success": False, "error": "ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤"}
                
                # í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ (ì§ˆë¬¸ì— AI ì˜¤íƒ€ ìˆ˜ì • ì ìš©)
                raw_question = self.preprocess_text(data['contents'])
                question = self.fix_korean_typos_with_ai(raw_question)
                answer = self.preprocess_text(data['reply_contents'])
                
                # ì„ë² ë”© ìƒì„± (ì§ˆë¬¸ ê¸°ë°˜)
                embedding = self.create_embedding(question)
                if not embedding:
                    return {"success": False, "error": "ì„ë² ë”© ìƒì„± ì‹¤íŒ¨"}
                
                # ì¹´í…Œê³ ë¦¬ ì´ë¦„ ê°€ì ¸ì˜¤ê¸°
                category = self.get_category_name(data['cate_idx'])
                
                # ë©”íƒ€ë°ì´í„° êµ¬ì„± (ì§ˆë¬¸ì€ ì˜¤íƒ€ ìˆ˜ì •ëœ ë²„ì „ ì‚¬ìš©)
                metadata = {
                    "seq": int(data['seq']),
                    "question": question,
                    "answer": self.preprocess_text(data['reply_contents'], for_metadata=True),
                    "category": category,
                    "name": data['name'] if data['name'] else "ìµëª…",
                    "regdate": data['regdate'],
                    "source": "bible_inquiry_mssql",
                    "updated_at": datetime.now().strftime("%Y-%m-%d %H:%M:%S")
                }
                
                # Pineconeì— upsert
                vector_id = f"qa_bible_{seq}"
                
                # ê¸°ì¡´ ë²¡í„° í™•ì¸
                existing = self.index.fetch(ids=[vector_id])
                is_update = vector_id in existing['vectors']
                
                # ë²¡í„° ë°ì´í„° êµ¬ì„±
                vector_data = {
                    "id": vector_id,
                    "values": embedding,
                    "metadata": metadata
                }
                
                # Pineconeì— ì €ì¥
                self.index.upsert(vectors=[vector_data])
                
                action = "ìˆ˜ì •" if is_update else "ìƒì„±"
                logging.info(f"Pinecone {action} ì™„ë£Œ: {vector_id}")
                
                return {
                    "success": True,
                    "message": f"Pinecone {action} ì™„ë£Œ",
                    "seq": seq,
                    "vector_id": vector_id,
                    "is_update": is_update
                }
            
        except Exception as e:
            logging.error(f"Pinecone ë™ê¸°í™” ì‹¤íŒ¨: {str(e)}")
            return {"success": False, "error": str(e)}

# ==================================================
# 9. ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤ ìƒì„± (ì „ì—­ ê°ì²´)
# ==================================================
# ì• í”Œë¦¬ì¼€ì´ì…˜ ì „ì²´ì—ì„œ ì‚¬ìš©í•  ë‹¨ì¼ ì¸ìŠ¤í„´ìŠ¤ë“¤
# ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±ê³¼ ìƒíƒœ ì¼ê´€ì„±ì„ ìœ„í•´ ì‹±ê¸€í†¤ íŒ¨í„´ ì ìš©
generator = AIAnswerGenerator()      # AI ë‹µë³€ ìƒì„±ê¸°
sync_manager = PineconeSyncManager() # Pinecone ë™ê¸°í™” ë§¤ë‹ˆì €

# ==================================================
# 10. Flask RESTful API ì—”ë“œí¬ì¸íŠ¸ ì •ì˜
# ==================================================
# â˜… AI ë‹µë³€ ìƒì„± API ì—”ë“œí¬ì¸íŠ¸ (ë©”ì¸ ê¸°ëŠ¥)
# ASP Classicì—ì„œ í˜¸ì¶œí•˜ëŠ” ì£¼ìš” APIë¡œ, ê³ ê° ì§ˆë¬¸ì— ëŒ€í•œ AI ë‹µë³€ì„ ìƒì„±
# ì²˜ë¦¬ ê³¼ì •:
# 1. ì§ˆë¬¸ ì „ì²˜ë¦¬ ë° ê²€ì¦
# 2. Pineconeì—ì„œ ìœ ì‚¬ ë‹µë³€ ê²€ìƒ‰
# 3. ì»¨í…ìŠ¤íŠ¸ í’ˆì§ˆ ë¶„ì„
# 4. GPTë¥¼ ì´ìš©í•œ ë§ì¶¤ ë‹µë³€ ìƒì„±
# 5. ìµœì¢… í¬ë§·íŒ… ë° ë°˜í™˜
@app.route('/generate_answer', methods=['POST'])
def generate_answer():
    try:
        with memory_cleanup():
            data = request.get_json()
            seq = data.get('seq', 0)
            question = data.get('question', '')
            lang = data.get('lang', 'auto')  # ê¸°ë³¸ê°’ì„ 'auto'ë¡œ ë³€ê²½ (ìë™ ê°ì§€)
            
            if not question:
                return jsonify({"success": False, "error": "ì§ˆë¬¸ì´ í•„ìš”í•©ë‹ˆë‹¤."}), 400
            
            result = generator.process(seq, question, lang)
            
            response = jsonify(result)
            response.headers['Content-Type'] = 'application/json; charset=utf-8'

            # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ëª¨ë‹ˆí„°ë§
            snapshot = tracemalloc.take_snapshot() # ê° ìš”ì²­ í›„ ë©”ëª¨ë¦¬ ìŠ¤ëƒ…ìƒ· ì´¬ì˜
            top_stats = snapshot.statistics('lineno') # ê° ìš”ì²­ í›„ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ í†µê³„
            memory_usage = sum(stat.size for stat in top_stats) / 1024 / 1024  # MBë¡œ ë°˜í™˜
            logging.info(f"í˜„ì¬ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {memory_usage:.2f}MB")
            
            if memory_usage > 500: # 500MB ì´ˆê³¼ì‹œ ê²½ê³  ë° ê°€ë¹„ì§€ ì»¬ë ‰ì…˜ ê°•ì œ ì‹¤í–‰
                logging.warning(f"ë†’ì€ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ê°ì§€: {memory_usage:.2f}MB")
                gc.collect()

            return response
        
    except Exception as e:
        logging.error(f"API í˜¸ì¶œ ì˜¤ë¥˜: {str(e)}")
        return jsonify({"success": False, "error": str(e)}), 500

# â˜… MSSQL ë°ì´í„°ë¥¼ Pineconeì— ë™ê¸°í™”í•˜ëŠ” API ì—”ë“œí¬ì¸íŠ¸
# ìš´ì˜ ì‹œìŠ¤í…œì—ì„œ ìƒˆë¡œìš´ Q&A ë°ì´í„°ê°€ ìƒì„±ë˜ê±°ë‚˜ ìˆ˜ì •ë  ë•Œ í˜¸ì¶œ
# ì²˜ë¦¬ ê³¼ì •:
# 1. MSSQLì—ì„œ í•´ë‹¹ seq ë°ì´í„° ì¡°íšŒ
# 2. AIë¡œ ì§ˆë¬¸ ì˜¤íƒ€ ìˆ˜ì •
# 3. OpenAIë¡œ ì„ë² ë”© ë²¡í„° ìƒì„±
# 4. Pineconeì— ë²¡í„° ì €ì¥/ìˆ˜ì •/ì‚­ì œ
@app.route('/sync_to_pinecone', methods=['POST'])
def sync_to_pinecone():
    try:
        data = request.get_json()
        seq = data.get('seq')
        mode = data.get('mode', 'upsert')

        logging.info(f"ë™ê¸°í™” ìš”ì²­ ìˆ˜ì‹ : seq={seq}, mode={mode}")
        
        if not seq:
            logging.warning("seq ëˆ„ë½")
            return jsonify({"success": False, "error": "seqê°€ í•„ìš”í•©ë‹ˆë‹¤"}), 400
        
        if not isinstance(seq, int):
            seq = int(seq)
        
        result = sync_manager.sync_to_pinecone(seq, mode)

        logging.info(f"ë™ê¸°í™” ê²°ê³¼: {result}")
        
        status_code = 200 if result["success"] else 500
        return jsonify(result), status_code
        
    except ValueError as e:
        logging.error(f"ì˜ëª»ëœ seq ê°’: {str(e)}")
        return jsonify({"success": False, "error": f"ì˜ëª»ëœ seq ê°’: {str(e)}"}), 400
    except Exception as e:
        logging.error(f"Pinecone ë™ê¸°í™” API ì˜¤ë¥˜: {str(e)}")
        return jsonify({"success": False, "error": str(e)}), 500

# â˜… ì‹œìŠ¤í…œ ìƒíƒœ í™•ì¸ì„ ìœ„í•œ í—¬ìŠ¤ì²´í¬ API ì—”ë“œí¬ì¸íŠ¸
# ë¡œë“œë°¸ëŸ°ì„œë‚˜ ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œì—ì„œ í˜¸ì¶œí•˜ì—¬ ì„œë²„ ìƒíƒœ í™•ì¸
@app.route('/health', methods=['GET'])
def health_check():
    try:
        stats = index.describe_index_stats()
        
        return jsonify({
            "status": "healthy",
            "pinecone_vectors": stats.get('total_vector_count', 0),
            "timestamp": datetime.now().isoformat(),
            "services": {
                "ai_answer": "active",
                "pinecone_sync": "active",
                "multilingual_support": "active"  # ë‹¤êµ­ì–´ ì§€ì› ìƒíƒœ ì¶”ê°€
            }
        }), 200
    except Exception as e:
        return jsonify({
            "status": "unhealthy",
            "error": str(e)
        }), 500

# ==================================================
# 11. ë©”ì¸ ì‹¤í–‰ ë¶€ë¶„
# ==================================================
# Flask ì›¹ ì„œë²„ ì‹œì‘ì 
# 
# ì´ ë¶€ë¶„ì€ ì´ íŒŒì¼(ìŠ¤í¬ë¦½íŠ¸)ê°€ ì§ì ‘ ì‹¤í–‰ë  ë•Œë§Œ ì‹¤í–‰ë˜ëŠ” ì ˆì°¨ì  ì½”ë“œ
# ë‹¤ë¥¸ ëª¨ë“ˆì—ì„œ importí•  ë•ŒëŠ” ì‹¤í–‰ë˜ì§€ ì•ŠìŒ
# 
# ì„¤ì •:
# - í¬íŠ¸: í™˜ê²½ë³€ìˆ˜ FLASK_PORT ë˜ëŠ” ê¸°ë³¸ê°’ 8000
# - í˜¸ìŠ¤íŠ¸: 0.0.0.0 (ëª¨ë“  IPì—ì„œ ì ‘ê·¼ ê°€ëŠ¥)
# - ë””ë²„ê·¸: False (ìš´ì˜ ëª¨ë“œ)
# - ìŠ¤ë ˆë“œ: True (ë©€í‹°ìŠ¤ë ˆë”© ì§€ì›)
if __name__ == "__main__":
    
    # í™˜ê²½ë³€ìˆ˜ì—ì„œ í¬íŠ¸ ì„¤ì • ë¡œë“œ (ê¸°ë³¸ê°’: 8000)
    port = int(os.getenv('FLASK_PORT', 8000))
    
    # ì‹œì‘ ë©”ì‹œì§€ ì¶œë ¥
    print("="*60)
    print("ğŸš€ GOODTV ë°”ì´ë¸” ì• í”Œ AI ë‹µë³€ ìƒì„± ì„œë²„ ì‹œì‘")
    print("="*60)
    print(f"ğŸ“¡ ì„œë²„ í¬íŠ¸: {port}")
    print(f"ğŸ¤– AI ëª¨ë¸: {GPT_MODEL} (Enhanced Context Mode)")
    print(f"ğŸ” ì„ë² ë”© ëª¨ë¸: {MODEL_NAME}")
    print(f"ğŸ—ƒï¸  ë²¡í„° DB: Pinecone ({INDEX_NAME})")
    print(f"ğŸŒ ë‹¤êµ­ì–´ ì§€ì›: í•œêµ­ì–´(ko), ì˜ì–´(en)")
    print("ğŸ”§ ì œê³µ ì„œë¹„ìŠ¤:")
    print("   â”œâ”€â”€ AI ë‹µë³€ ìƒì„± (/generate_answer)")
    print("   â”œâ”€â”€ Pinecone ë™ê¸°í™” (/sync_to_pinecone)")
    print("   â””â”€â”€ í—¬ìŠ¤ì²´í¬ (/health)")
    print("="*60)
    
    # Flask ì›¹ ì„œë²„ ì‹œì‘
    # host='0.0.0.0': ëª¨ë“  ë„¤íŠ¸ì›Œí¬ ì¸í„°í˜ì´ìŠ¤ì—ì„œ ì ‘ê·¼ í—ˆìš©
    # debug=False: ìš´ì˜ ëª¨ë“œ (ë³´ì•ˆìƒ ì¤‘ìš”)
    # threaded=True: ë©€í‹°ìŠ¤ë ˆë”© í™œì„±í™”, ë™ì‹œ ìš”ì²­ ì²˜ë¦¬ ê°€ëŠ¥
    app.run(host='0.0.0.0', port=port, debug=False, threaded=True)