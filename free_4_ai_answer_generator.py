#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
=== AI ë‹µë³€ ìƒì„± Flask API ì„œë²„ (ë‹¤êµ­ì–´ ì§€ì›) ===
íŒŒì¼ëª…: free_4_ai_answer_generator.py
ëª©ì : ASP Classicì—ì„œ í˜¸ì¶œí•˜ëŠ” AI ë‹µë³€ ìƒì„± API + Pinecone ë²¡í„°DB ë™ê¸°í™”
ì£¼ìš” ê¸°ëŠ¥:
1. OpenAI GPT-3.5-turboë¥¼ ì´ìš©í•œ ìì—°ì–´ ë‹µë³€ ìƒì„± (í•œêµ­ì–´/ì˜ì–´)
2. Pinecone ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ìœ ì‚¬ ë‹µë³€ ê²€ìƒ‰
3. MSSQL ë°ì´í„°ë² ì´ìŠ¤ì™€ Pinecone ë™ê¸°í™”
4. ë©”ëª¨ë¦¬ ìµœì í™” ë° ëª¨ë‹ˆí„°ë§
5. ë‹¤êµ­ì–´ ì§€ì› (í•œêµ­ì–´, ì˜ì–´)
"""

# ==================================================
# 1. í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸ êµ¬ê°„
# ==================================================
# ê¸°ë³¸ Python ëª¨ë“ˆë“¤
import os                   # í™˜ê²½ë³€ìˆ˜ ë° íŒŒì¼ ì‹œìŠ¤í…œ ì‘ì—…
import sys                  # ì‹œìŠ¤í…œ ê´€ë ¨ ê¸°ëŠ¥
import json                 # JSON ë°ì´í„° ì²˜ë¦¬
import json as json_module  # JSON ëª¨ë“ˆì˜ ë³„ì¹­ (ì¼ë¦¬ì•„ìŠ¤, ì½”ë“œ ë‚´ ì¤‘ë³µ ë°©ì§€)
import re                   # ì •ê·œí‘œí˜„ì‹ íŒ¨í„´ ë§¤ì¹­
import html                 # HTML ì—”í‹°í‹° ì²˜ë¦¬
import unicodedata          # ìœ ë‹ˆì½”ë“œ ë¬¸ì ì •ê·œí™”
import logging              # ë¡œê·¸ ê¸°ë¡ ì‹œìŠ¤í…œ
import gc                   # ê°€ë¹„ì§€ ì»¬ë ‰ì…˜ (ë©”ëª¨ë¦¬ ê´€ë¦¬)

# ì›¹ í”„ë ˆì„ì›Œí¬ ê´€ë ¨
from flask import Flask, request, jsonify  # Flask ì›¹ í”„ë ˆì„ì›Œí¬
from flask_cors import CORS                 # CORS(Cross-Origin Resource Sharing) ì²˜ë¦¬

# AI ë° ë°ì´í„°ë² ì´ìŠ¤ ê´€ë ¨
from pinecone import Pinecone      # Pinecone ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤
import openai                      # OpenAI API í´ë¼ì´ì–¸íŠ¸
import pyodbc                      # MSSQL ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²°

# í™˜ê²½ì„¤ì • ë° ìœ í‹¸ë¦¬í‹°
from dotenv import load_dotenv     # .env íŒŒì¼ì—ì„œ í™˜ê²½ë³€ìˆ˜ ë¡œë“œ
from datetime import datetime      # ë‚ ì§œ/ì‹œê°„ ì²˜ë¦¬
from typing import Optional, Dict, Any, List  # íƒ€ì… íŒíŒ…

# ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ê´€ë ¨
from memory_profiler import profile                  # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ í”„ë¡œíŒŒì¼ë§
import tracemalloc                                   # ë©”ëª¨ë¦¬ ì¶”ì 
import threading                                     # ë©€í‹°ìŠ¤ë ˆë”©
from contextlib import contextmanager                # ì»¨í…ìŠ¤íŠ¸ ë§¤ë‹ˆì € (withë¬¸ ì‚¬ìš©)
from langdetect import detect, LangDetectException   # ì–¸ì–´ ê°ì§€

# ==================================================
# 2. ì‹œìŠ¤í…œ ì´ˆê¸°í™” ë° ì„¤ì •
# ==================================================
# ë©”ëª¨ë¦¬ ì¶”ì  ì‹œì‘ - ë©”ëª¨ë¦¬ ëˆ„ìˆ˜ ë° ì‚¬ìš©ëŸ‰ì„ ëª¨ë‹ˆí„°ë§í•˜ê¸° ìœ„í•¨ (ì´ ì‹œì ë¶€í„° ëª¨ë“  ë©”ëª¨ë¦¬ í• ë‹¹ì´ ê¸°ë¡ë˜ì–´ ë‚˜ì¤‘ì— ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ë¶„ì„ì´ ê°€ëŠ¥í•´ì§)
tracemalloc.start() 

# Flask ì›¹ ì• í”Œë¦¬ì¼€ì´ì…˜ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
# __name__: í˜„ì¬ ëª¨ë“ˆëª…ì„ ì „ë‹¬í•˜ì—¬ Flaskê°€ ë¦¬ì†ŒìŠ¤ ìœ„ì¹˜ë¥¼ ì°¾ì„ ìˆ˜ ìˆê²Œ í•¨
app = Flask(__name__)

# CORS ì„¤ì • - ëª¨ë“  ì—”ë“œí¬ì¸íŠ¸ì—ì„œ cross-origin ìš”ì²­ì„ í—ˆìš© (ASP Classicì—ì„œ í˜¸ì¶œí•˜ê¸° ìœ„í•¨)
CORS(app)

# ==================================================
# 3. ë¡œê¹… ì‹œìŠ¤í…œ ì„¤ì •
# ==================================================
# ìš´ì˜í™˜ê²½ì—ì„œ ë¡œê·¸ë¥¼ íŒŒì¼ë¡œ ì €ì¥í•˜ì—¬ ë””ë²„ê¹… ë° ëª¨ë‹ˆí„°ë§ ê°€ëŠ¥
# ë¡œê¹… ì„¤ì •ì—ì„œ ì¤‘ìš”í•œ ê²ƒì€ encoding='utf-8'. í•œê¸€ ì—ëŸ¬ ë©”ì‹œì§€ë‚˜ ë””ë²„ê·¸ ì •ë³´ê°€ ê¹¨ì§€ì§€ ì•Šê³  ë¡œê·¸ íŒŒì¼ì— ê¸°ë¡ë˜ë„ë¡ í•¨
logging.basicConfig(
    filename='/home/ec2-user/python/logs/ai_generator.log',  # ë¡œê·¸ íŒŒì¼ ê²½ë¡œ
    level=logging.INFO,                                      # INFO ë ˆë²¨ ì´ìƒ ë¡œê·¸ ê¸°ë¡
    format='%(asctime)s - %(levelname)s - %(message)s',      # ë¡œê·¸ í¬ë§·
    encoding='utf-8'                                         # í•œê¸€ ì§€ì›ì„ ìœ„í•œ UTF-8 ì¸ì½”ë”©
)

# ==================================================
# 4. í™˜ê²½ë³€ìˆ˜ ë¡œë“œ ë° ì‹œìŠ¤í…œ ìƒìˆ˜ ì •ì˜
# ==================================================
# .env íŒŒì¼ì—ì„œ API í‚¤ ë° ë°ì´í„°ë² ì´ìŠ¤ ì„¤ì • ë¡œë“œ
load_dotenv()

# AI ì„ë² ë”© ëª¨ë¸ ë° ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ì„¤ì • ìƒìˆ˜ë“¤
MODEL_NAME = 'text-embedding-3-small'          # OpenAI ì„ë² ë”© ëª¨ë¸ëª…
INDEX_NAME = "bible-app-support-1536-openai"   # Pinecone ì¸ë±ìŠ¤ëª…
EMBEDDING_DIMENSION = 1536                      # ì„ë² ë”© ë²¡í„° ì°¨ì›ìˆ˜
MAX_TEXT_LENGTH = 8000                          # í…ìŠ¤íŠ¸ ìµœëŒ€ ê¸¸ì´ ì œí•œ

# GPT ìì—°ì–´ ëª¨ë¸ ì„¤ì • - ë³´ìˆ˜ì  ì„¤ì •ìœ¼ë¡œ ì¼ê´€ì„± ìˆëŠ” ë‹µë³€ ìƒì„±
GPT_MODEL = 'gpt-3.5-turbo'     # ì‚¬ìš©í•  GPT ëª¨ë¸
MAX_TOKENS = 600                 # ìƒì„±í•  ìµœëŒ€ í† í° ìˆ˜ (ë‹µë³€ ê¸¸ì´ ì œí•œ)
TEMPERATURE = 0.3                # ì°½ì˜ì„± ìˆ˜ì¤€ (ë‚®ì„ìˆ˜ë¡ ì¼ê´€ëœ ë‹µë³€)

# ê³ ê° ë¬¸ì˜ ì¹´í…Œê³ ë¦¬ ë§¤í•‘ í…Œì´ë¸”
# ì¹´í…Œê³ ë¦¬ ë§¤í•‘ ë”•ì…”ë„ˆë¦¬ëŠ” MSSQLì˜ ìˆ«ì ì¸ë±ìŠ¤ë¥¼ ì‚¬ëŒì´ ì½ì„ ìˆ˜ ìˆëŠ” í•œê¸€ ì¹´í…Œê³ ë¦¬ëª…ìœ¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤. 
# ì´ëŠ” Pinecone ë©”íƒ€ë°ì´í„°ì— ì €ì¥ë˜ì–´ ê²€ìƒ‰ ê²°ê³¼ì˜ ê°€ë…ì„±ì„ ë†’ì…ë‹ˆë‹¤.
CATEGORY_MAPPING = {
    '1': 'í›„ì›/í•´ì§€',                   
    '2': 'ì„±ê²½ í†µë…(ì½ê¸°,ë“£ê¸°,ë…¹ìŒ)',   
    '3': 'ì„±ê²½ë‚­ë… ë ˆì´ìŠ¤',             
    '4': 'ê°œì„ /ì œì•ˆ',                   
    '5': 'ì˜¤ë¥˜/ì¥ì• ',               
    '6': 'ë¶ˆë§Œ',                        
    '7': 'ì˜¤íƒˆìì œë³´',                   
    '0': 'ì‚¬ìš© ë¬¸ì˜(ê¸°íƒ€)'               
}

# ì˜ì–´ ì¹´í…Œê³ ë¦¬ ë§¤í•‘ ì¶”ê°€
CATEGORY_MAPPING_EN = {
    '1': 'Sponsorship/Cancellation',
    '2': 'Bible Reading(Read,Listen,Record)',
    '3': 'Bible Reading Race',
    '4': 'Improvement/Suggestion',
    '5': 'Error/Failure',
    '6': 'Complaint',
    '7': 'Typo Report',
    '0': 'Usage Inquiry(Other)'
}

# ==================================================
# 5. ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ ì •ì˜
# ==================================================
# ë©”ëª¨ë¦¬ ì •ë¦¬ë¥¼ ìœ„í•œ ì»¨í…ìŠ¤íŠ¸ ë§¤ë‹ˆì €
# withë¬¸ ì§„ì…ì‹œ try ë¸”ë¡ ì‹¤í–‰
# yieldì—ì„œ ì¼ì‹œì •ì§€í•˜ê³  with ë¸”ë¡ ë‚´ë¶€ ì½”ë“œ ì‹¤í–‰
# yieldëŠ” íŒŒì´ì¬ì—ì„œ ì œë„ˆë ˆì´í„°ë¥¼ ë§Œë“¤ë•Œ ì‚¬ìš©í•˜ëŠ” í‚¤ì›Œë“œë¡œì¨, í•¨ìˆ˜ ë‚´ì—ì„œ yieldë¥¼ ì‚¬ìš©í•˜ë©´ ê·¸ í•¨ìˆ˜ëŠ” ì œë„ˆë ˆì´í„° í•¨ìˆ˜ê°€ ë©ë‹ˆë‹¤.
# ì œë„ˆë ˆì´í„° í•¨ìˆ˜ëŠ” ê°’ì„ ë°˜í™˜í•˜ëŠ” ëŒ€ì‹  ê°’ì„ í•˜ë‚˜ì”© ìƒì„±í•˜ì—¬ ë°˜í™˜í•©ë‹ˆë‹¤. (íŒŒì´ì¬ì—ì„œ ëŒ€ìš©ëŸ‰ ë°ì´í„° ì²˜ë¦¬ë¥¼ ìœ„í•´ ì‚¬ìš©)
# with ë¸”ë¡ ì¢…ë£Œì‹œ finallyì—ì„œ gc.collect() ì‹¤í–‰

# ì´ë ‡ê²Œ í•˜ë©´ ëŒ€ìš©ëŸ‰ ë°ì´í„° ì²˜ë¦¬ í›„ ìë™ìœ¼ë¡œ ê°€ë¹„ì§€ ì»¬ë ‰ì…˜ì´ ì‹¤í–‰ë©ë‹ˆë‹¤.
@contextmanager
def memory_cleanup():
    try:
        yield  # with ë¸”ë¡ ë‚´ë¶€ ì½”ë“œ ì‹¤í–‰
    finally:
        gc.collect()  # ê°€ë¹„ì§€ ì»¬ë ‰ì…˜ ê°•ì œ ì‹¤í–‰ìœ¼ë¡œ ë©”ëª¨ë¦¬ ì •ë¦¬

# ==================================================
# 6. ì™¸ë¶€ ì„œë¹„ìŠ¤ ì—°ê²° ë° ì´ˆê¸°í™”
# ==================================================
try:
    # Pinecone ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì„¤ì •
    # ìœ ì‚¬ ë‹µë³€ ê²€ìƒ‰ì„ ìœ„í•œ ë²¡í„°DB - ì„ë² ë”©ëœ ì§ˆë¬¸/ë‹µë³€ ì €ì¥ì†Œ
    pc = Pinecone(api_key=os.getenv('PINECONE_API_KEY'))
    index = pc.Index(INDEX_NAME)
    
    # OpenAI API í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
    # GPT ëª¨ë¸ ë° ì„ë² ë”© ìƒì„±ì„ ìœ„í•œ í´ë¼ì´ì–¸íŠ¸
    # openai.api_key = ... ë°©ì‹ë³´ë‹¤ ê°ì²´ì§€í–¥ì ìœ¼ë¡œ ì„¤ê³„
    openai_client = openai.OpenAI(api_key=os.getenv('OPENAI_API_KEY'))
    
    # MSSQL ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì„¤ì •
    # ê¸°ì¡´ ê³ ê° ë¬¸ì˜ ë°ì´í„°ê°€ ì €ì¥ëœ ìš´ì˜ DB ì—°ê²°ì„ ìœ„í•œ ì„¤ì •
    mssql_config = {
        'server': os.getenv('MSSQL_SERVER'),       # DB ì„œë²„ ì£¼ì†Œ
        'database': os.getenv('MSSQL_DATABASE'),   # ë°ì´í„°ë² ì´ìŠ¤ëª…
        'username': os.getenv('MSSQL_USERNAME'),   # DB ì‚¬ìš©ìëª…
        'password': os.getenv('MSSQL_PASSWORD')    # DB ë¹„ë°€ë²ˆí˜¸
    }
    
    # MSSQL Server ì—°ê²° ë¬¸ìì—´ êµ¬ì„±
    # MSSQL Server í‘œì¤€ ODBC ë“œë¼ì´ë²„ë¥¼ ì‚¬ìš©í•œ ì—°ê²° ë¬¸ìì—´
    connection_string = (
            f"DRIVER={{ODBC Driver 17 for SQL Server}};"  # ODBC ë“œë¼ì´ë²„ ë²„ì „
            f"SERVER={mssql_config['server']},1433;"      # ì„œë²„ ì£¼ì†Œì™€ í¬íŠ¸
            f"DATABASE={mssql_config['database']};"       # ë°ì´í„°ë² ì´ìŠ¤ëª…
            f"UID={mssql_config['username']};"            # ì‚¬ìš©ì ID
            f"PWD={mssql_config['password']};"            # ë¹„ë°€ë²ˆí˜¸
            f"TrustServerCertificate=yes;"                # SSL ì¸ì¦ì„œ ì‹ ë¢°
            f"Connection Timeout=30;"                     # ì—°ê²° íƒ€ì„ì•„ì›ƒ (30ì´ˆ)
    )

except Exception as e:
    # ì´ˆê¸°í™” ì‹¤íŒ¨ ì‹œ ë¡œê·¸ ê¸°ë¡ í›„ í”„ë¡œê·¸ë¨ ì¢…ë£Œ
    logging.error(f"ëª¨ë“ˆ ë¡œë“œ ì‹¤íŒ¨: {str(e)}")
    app.logger.error(f"ëª¨ë“ˆ ë¡œë“œ ì‹¤íŒ¨: {str(e)}")
    raise  # ì˜ˆì™¸ë¥¼ ë‹¤ì‹œ ë°œìƒì‹œì¼œ í”„ë¡œê·¸ë¨ ì¤‘ë‹¨

# ==================================================
# 7. AI ë‹µë³€ ìƒì„± ë©”ì¸ í´ë˜ìŠ¤ (ê°ì²´ ì§€í–¥ í”„ë¡œê·¸ë˜ë°, ë‹¤êµ­ì–´ ì§€ì› ì¶”ê°€)
# ==================================================
    # AI ë‹µë³€ ìƒì„±ì„ ë‹´ë‹¹í•˜ëŠ” ë©”ì¸ í´ë˜ìŠ¤
    # ê°ì²´ì§€í–¥ ì„¤ê³„ë¡œ ê´€ë ¨ ê¸°ëŠ¥ë“¤ì„ í•˜ë‚˜ì˜ í´ë˜ìŠ¤ì— ìº¡ìŠí™”
    
    # ì£¼ìš” ê¸°ëŠ¥:
    # 1. í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ ë° ì •ì œ
    # 2. OpenAIë¥¼ ì´ìš©í•œ ì„ë² ë”© ìƒì„±
    # 3. Pineconeì—ì„œ ìœ ì‚¬ ë‹µë³€ ê²€ìƒ‰
    # 4. GPTë¥¼ ì´ìš©í•œ ë§ì¶¤í˜• ë‹µë³€ ìƒì„±
    # 5. í•œêµ­ì–´ í…ìŠ¤íŠ¸ ê²€ì¦ ë° í¬ë§·íŒ…

class AIAnswerGenerator:
    
    # í´ë˜ìŠ¤ ì´ˆê¸°í™” ë©”ì„œë“œ
    # OpenAI í´ë¼ì´ì–¸íŠ¸ë¥¼ ì¸ìŠ¤í„´ìŠ¤ ë³€ìˆ˜ë¡œ ì„¤ì •. ì´ëŠ” ì˜ì¡´ì„± ì£¼ì… íŒ¨í„´ì˜ ê°„ì†Œí™”ëœ í˜•íƒœ
    def __init__(self):
        self.openai_client = openai_client
    
    # â˜† í…ìŠ¤íŠ¸ì˜ ì–¸ì–´ë¥¼ ê°ì§€í•˜ëŠ” ë©”ì„œë“œ
    def detect_language(self, text: str) -> str:
        try:
            # langdetect ë¼ì´ë¸ŒëŸ¬ë¦¬ ì‚¬ìš©
            detected = detect(text)
            
            # ì˜ì–´ì™€ í•œêµ­ì–´ë§Œ ì§€ì›
            if detected == 'en':
                return 'en'
            elif detected == 'ko':
                return 'ko'
            else:
                # ê¸°ë³¸ê°’ì€ í•œêµ­ì–´
                return 'ko'
        except LangDetectException:
            # ê°ì§€ ì‹¤íŒ¨ì‹œ í…ìŠ¤íŠ¸ ë‚´ í•œê¸€ ë¹„ìœ¨ë¡œ íŒë‹¨
            korean_chars = len(re.findall(r'[ê°€-í£]', text))
            english_chars = len(re.findall(r'[a-zA-Z]', text))
            
            if korean_chars > english_chars:
                return 'ko'
            else:
                return 'en'

    # â˜† ì…ë ¥ í…ìŠ¤íŠ¸ë¥¼ AI ì²˜ë¦¬ì— ì í•©í•˜ê²Œ ì „ì²˜ë¦¬í•˜ëŠ” ë©”ì„œë“œ
    # Args:
    #     text (str): ì›ë³¸ í…ìŠ¤íŠ¸
            
    # Returns:
    #     str: ì •ì œëœ í…ìŠ¤íŠ¸
    def preprocess_text(self, text: str) -> str:

        # null ì²´í¬
        if not text:
            return ""
        
        # ë¬¸ìì—´ë¡œ ë³€í™˜ ë° HTML ì—”í‹°í‹° ë””ì½”ë”©
        text = str(text)
        text = html.unescape(text)  # &amp; â†’ &, &lt; â†’ < ë“±
        
        # HTML íƒœê·¸ ì œê±° ë°ë° í…ìŠ¤íŠ¸ í˜•íƒœë¡œ ë³€í™˜ (êµ¬ì¡° ìœ ì§€)
        text = re.sub(r'<br\s*/?>', '\n', text, flags=re.IGNORECASE)      # <br> â†’ ì¤„ë°”ê¿ˆ
        text = re.sub(r'</p>', '\n\n', text, flags=re.IGNORECASE)         # </p> â†’ ë‹¨ë½ êµ¬ë¶„
        text = re.sub(r'<p[^>]*>', '\n', text, flags=re.IGNORECASE)       # <p> â†’ ì¤„ë°”ê¿ˆ
        text = re.sub(r'<li[^>]*>', '\nâ€¢ ', text, flags=re.IGNORECASE)    # <li> â†’ ë¶ˆë¦¿í¬ì¸íŠ¸
        text = re.sub(r'</li>', '', text, flags=re.IGNORECASE)            # </li> ì œê±°
        text = re.sub(r'<[^>]+>', '', text)                               # ë‚˜ë¨¸ì§€ HTML íƒœê·¸ ëª¨ë‘ ì œê±°
        
        # ê³µë°± ë° ì¤„ë°”ê¿ˆ ì •ê·œí™” - ì¼ê´€ëœ í˜•íƒœë¡œ ë³€í™˜
        text = re.sub(r'\n{3,}', '\n\n', text)    # 3ê°œ ì´ìƒ ì¤„ë°”ê¿ˆ â†’ 2ê°œë¡œ ì œí•œ
        text = re.sub(r'[ \t]+', ' ', text)       # ì—°ì† ê³µë°±/íƒ­ â†’ ë‹¨ì¼ ê³µë°±
        text = text.strip()                       # ì•ë’¤ ê³µë°± ì œê±°
        
        return text

    # â˜† JSON ë¬¸ìì—´ ì´ìŠ¤ì¼€ì´í”„ ì²˜ë¦¬
    # íŠ¹ìˆ˜ë¬¸ìê°€ í¬í•¨ëœ í…ìŠ¤íŠ¸ë¥¼ JSONìœ¼ë¡œ ì•ˆì „í•˜ê²Œ ë³€í™˜
    def escape_json_string(self, text: str) -> str:
        
        if not text:
            return ""
        escaped = json_module.dumps(text, ensure_ascii=False) # ensure_ascii=False: í•œê¸€ ê¹¨ì§ ë°©ì§€
        return escaped[1:-1]  # ì•ë’¤ ë”°ì˜´í‘œ ì œê±°

    # â˜† OpenAI APIë¥¼ ì‚¬ìš©í•˜ì—¬ í…ìŠ¤íŠ¸ë¥¼ ë²¡í„°ë¡œ ë³€í™˜í•˜ëŠ” ë©”ì„œë“œ
    # ë²¡í„° ì„ë² ë”©: í…ìŠ¤íŠ¸ì˜ ì˜ë¯¸ë¥¼ ìˆ˜ì¹˜ ë°°ì—´ë¡œ í‘œí˜„í•˜ì—¬ ìœ ì‚¬ë„ ê³„ì‚° ê°€ëŠ¥
    def create_embedding(self, text: str) -> Optional[list]:

        # ë¹ˆ ë¬¸ìì—´ë¿ë§Œ ì•„ë‹ˆë¼ ê³µë°±ë§Œ ìˆëŠ” ë¬¸ìì—´ë„ ê±¸ëŸ¬ëƒ„ (ì´ëŸ° ê²½ìš° JSON ë³€í™˜ ë¶ˆê°€)
        if not text or not text.strip():
            return None
            
        try:
            with memory_cleanup(): # ë©”ëª¨ë¦¬ ëˆ„ìˆ˜ ë°©ì§€ (ë¸”ë¡ ì¢…ë£Œ ì‹œ ê°€ë¹„ì§€ ì»¬ë ‰ì…˜)
                # OpenAI Embedding API í˜¸ì¶œ
                response = self.openai_client.embeddings.create(
                    model='text-embedding-3-small',    # ë²¡í„° ì„ë² ë”© ëª¨ë¸ëª…
                    input=text[:8000]                  # í…ìŠ¤íŠ¸ ê¸¸ì´ ì œí•œ (í† í° ì œí•œ ë°©ì§€ì§€)
                )
                
                # ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±ì„ ìœ„í•´ ë²¡í„°ë§Œ ë³µì‚¬ í›„ ì‘ë‹µ ê°ì²´ ì‚­ì œ
                embedding = response.data[0].embedding.copy() # ì„ë² ë”© ë²¡í„°ë§Œ ë³µì‚¬í•˜ì—¬ ë°˜í™˜ (ê¹Šì€ ë³µì‚¬ë¡œ ë…ë¦½ì ì¸ ë¦¬ìŠ¤íŠ¸ ìƒì„±)
                del response  # ì›ë³¸ ì‘ë‹µ ê°ì²´ ì¦‰ì‹œ ì‚­ì œ (ë©”ëª¨ë¦¬ í•´ì œ)
                return embedding
                
        except Exception as e:
            logging.error(f"ì„ë² ë”© ìƒì„± ì‹¤íŒ¨: {e}")
            return None

    # â˜† Pinecone ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ìœ ì‚¬í•œ ë‹µë³€ì„ ê²€ìƒ‰í•˜ëŠ” ë©”ì„œë“œ
    # Args:
    #     query (str): ê²€ìƒ‰í•  ì§ˆë¬¸
    #     top_k (int): ê²€ìƒ‰í•  ìµœëŒ€ ê°œìˆ˜ (ê¸°ë³¸ê°’: 5)
    #     similarity_threshold (float): ìœ ì‚¬ë„ ì„ê³„ê°’ (ê¸°ë³¸ê°’: 0.6)
            
    # Returns:
    #     list: ìœ ì‚¬ ë‹µë³€ ë¦¬ìŠ¤íŠ¸ [{'score': float, 'question': str, 'answer': str, ...}, ...]
    def search_similar_answers(self, query: str, top_k: int = 5, similarity_threshold: float = 0.6, lang: str = 'ko') -> list:
        """ì–¸ì–´ë³„ ìœ ì‚¬ ë‹µë³€ ê²€ìƒ‰"""
        try:
            with memory_cleanup():
                # ì˜ì–´ ì§ˆë¬¸ì¸ ê²½ìš° ë²ˆì—­í•˜ì—¬ í•œêµ­ì–´ë¡œë„ ê²€ìƒ‰
                if lang == 'en':
                    # 1. ê²€ìƒ‰ ì§ˆë¬¸ì„ ë²¡í„°ë¡œ ë³€í™˜
                    query_vector = self.create_embedding(query)
                    
                    # í•œêµ­ì–´ë¡œ ë²ˆì—­í•˜ì—¬ ì¶”ê°€ ê²€ìƒ‰ (ì„ íƒì )
                    translated_query = self.translate_text(query, 'en', 'ko')
                    if translated_query:
                        korean_vector = self.create_embedding(translated_query)
                else:
                    query_vector = self.create_embedding(query)
                    korean_vector = None
                
                if query_vector is None:
                    return []
                
                # 2. Pineconeì—ì„œ ë²¡í„° ìœ ì‚¬ë„ ê²€ìƒ‰ ìˆ˜í–‰
                # Pinecone ì¿¼ë¦¬ì—ì„œ include_metadata=TrueëŠ” ë²¡í„°ë¿ë§Œ ì•„ë‹ˆë¼ ì €ì¥ëœ ë©”íƒ€ë°ì´í„°(ì§ˆë¬¸, ë‹µë³€, ì¹´í…Œê³ ë¦¬)ë„ í•¨ê»˜ ë°˜í™˜ë°›ìŒ
                results = index.query(
                    vector=query_vector,           # ê²€ìƒ‰í•  ë²¡í„°
                    top_k=top_k,                   # ìƒìœ„ 5ê°œ ê²°ê³¼
                    include_metadata=True          # ë©”íƒ€ë°ì´í„° í¬í•¨ (ì§ˆë¬¸, ë‹µë³€, ì¹´í…Œê³ ë¦¬ ë“±)
                )
                
                # í•œêµ­ì–´ ë²¡í„°ë¡œ ì¶”ê°€ ê²€ìƒ‰ (ì˜ì–´ ì§ˆë¬¸ì¸ ê²½ìš°)
                if lang == 'en' and korean_vector:
                    korean_results = index.query(
                        vector=korean_vector,       # ê²€ìƒ‰í•  ë²¡í„°
                        top_k=3,                    # ë³´ì¡° ê²€ìƒ‰ì€ ì ê²Œ (3ê°œ)
                        include_metadata=True       # ë©”íƒ€ë°ì´í„° í¬í•¨ (ì§ˆë¬¸, ë‹µë³€, ì¹´í…Œê³ ë¦¬ ë“±)
                    )
                    # ê²°ê³¼ ë³‘í•© (ì¤‘ë³µ ì œê±°)
                    seen_ids = set()
                    merged_matches = []
                    for match in results['matches'] + korean_results['matches']:
                        if match['id'] not in seen_ids:
                            seen_ids.add(match['id'])
                            merged_matches.append(match)
                    results['matches'] = sorted(merged_matches, key=lambda x: x['score'], reverse=True)[:top_k]
                
                # 3. ê²°ê³¼ í•„í„°ë§ ë° êµ¬ì¡°í™”
                filtered_results = []
                for i, match in enumerate(results['matches']): # enumerateë¡œ ìˆœìœ„(rank) ìƒì„±
                    score = match['score'] # ìœ ì‚¬ë„ ì ìˆ˜ (0~1, ë†’ì„ìˆ˜ë¡ ìœ ì‚¬)
                    question = match['metadata'].get('question', '')
                    answer = match['metadata'].get('answer', '')
                    category = match['metadata'].get('category', 'ì¼ë°˜')
                    
                    # ìœ ì‚¬ë„ ì„ê³„ê°’(0.6) ì´ìƒë§Œ í¬í•¨í•˜ì—¬ ì •í™•ë„ í–¥ìƒ, ì„ê³„ê°’ ì´í•˜ëŠ” ë²„ë¦¼ (ë…¸ì´ì¦ˆ ì œê±°)
                    if score >= similarity_threshold:
                        filtered_results.append({
                            'score': score,
                            'question': question,
                            'answer': answer,
                            'category': category,
                            'rank': i + 1,
                            'lang': 'ko'  # ì›ë³¸ ë°ì´í„°ëŠ” í•œêµ­ì–´
                        })
                        
                        # ë””ë²„ê¹…ì„ ìœ„í•œ ìƒì„¸ ë¡œê¹…
                        logging.info(f"ìœ ì‚¬ ë‹µë³€ #{i+1}: ì ìˆ˜={score:.3f}, ì¹´í…Œê³ ë¦¬={category}, ì–¸ì–´={lang}")
                        logging.info(f"ì°¸ê³  ì§ˆë¬¸: {question[:50]}...")
                        logging.info(f"ì°¸ê³  ë‹µë³€: {answer[:100]}...")
                        logging.info(f"ìœ ì‚¬ë„ ì„ê³„ê°’: {similarity_threshold:.2f}")
                        logging.info(f"ê²€ìƒ‰ ì§ˆë¬¸: {query[:50]}...")
                        logging.info(f"ê²€ìƒ‰ ì–¸ì–´: {lang}")
                        
                # 4. ë©”ëª¨ë¦¬ ì •ë¦¬
                del results # ì›ë³¸ ì‘ë‹µ ê°ì²´ ì¦‰ì‹œ ì‚­ì œ (ë©”ëª¨ë¦¬ í•´ì œ)
                if korean_vector:
                    del korean_vector # í•œêµ­ì–´ ë²¡í„° ì¦‰ì‹œ ì‚­ì œ (ë©”ëª¨ë¦¬ í•´ì œ)
                del query_vector # ê²€ìƒ‰ ë²¡í„° ì¦‰ì‹œ ì‚­ì œ (ë©”ëª¨ë¦¬ í•´ì œ)
                
                logging.info(f"ì´ {len(filtered_results)}ê°œì˜ ìœ ì‚¬ ë‹µë³€ ê²€ìƒ‰ ì™„ë£Œ (ì–¸ì–´: {lang})")
                return filtered_results
                
        except Exception as e:
            logging.error(f"Pinecone ê²€ìƒ‰ ì‹¤íŒ¨: {str(e)}")
            return []

    # â˜† GPTë¥¼ ì‚¬ìš©í•œ ë²ˆì—­
    # Args:
    #     text (str): ë²ˆì—­í•  í…ìŠ¤íŠ¸
    #     source_lang (str): ì›ë³¸ ì–¸ì–´
    #     target_lang (str): ë²ˆì—­ ì–¸ì–´
            
    # Returns:
    #     str: ë²ˆì—­ëœ í…ìŠ¤íŠ¸
    def translate_text(self, text: str, source_lang: str, target_lang: str) -> str:
        try:
            # ì–¸ì–´ ë§¤í•‘
            lang_map = {
                'ko': 'Korean',
                'en': 'English'
            }
            
            system_prompt = f"You are a professional translator. Translate the following text from {lang_map[source_lang]} to {lang_map[target_lang]}. Keep the same tone and style. Only provide the translation without any explanation."
            
            response = self.openai_client.chat.completions.create(
                model='gpt-3.5-turbo',
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": text}
                ],
                max_tokens=500,
                temperature=0.3
            )
            
            return response.choices[0].message.content.strip()
            
        except Exception as e:
            logging.error(f"ë²ˆì—­ ì‹¤íŒ¨: {e}")
            return text

    # â˜† ê²€ìƒ‰ëœ ìœ ì‚¬ ë‹µë³€ë“¤ì˜ í’ˆì§ˆì„ ë¶„ì„í•˜ì—¬ ìµœì ì˜ ë‹µë³€ ìƒì„± ì „ëµì„ ê²°ì •í•˜ëŠ” ë©”ì„œë“œ

    # Args:
    #     similar_answers (list): ê²€ìƒ‰ëœ ìœ ì‚¬ ë‹µë³€ ë¦¬ìŠ¤íŠ¸
    #     query (str): ì›ë³¸ ì§ˆë¬¸
    #     
    # Returns:
    #     dict: ë¶„ì„ ê²°ê³¼ ë° ê¶Œì¥ ì ‘ê·¼ ë°©ì‹
    def analyze_context_quality(self, similar_answers: list, query: str) -> dict:
        # ìœ ì‚¬ ë‹µë³€ì´ ì—†ìœ¼ë©´ ê¸°ë³¸ê°’ ë°˜í™˜
        if not similar_answers:
            return {
                'has_good_context': False,
                'best_score': 0.0,
                'recommended_approach': 'fallback',
                'context_summary': 'ìœ ì‚¬ ë‹µë³€ì´ ì—†ìŠµë‹ˆë‹¤.'
            }
        
        # í’ˆì§ˆ ì§€í‘œ ê³„ì‚°
        # ë¦¬ìŠ¤íŠ¸ ì»´í”„ë¦¬í—¨ì…˜ (ë¦¬ìŠ¤íŠ¸ë¥¼ ì‰½ê²Œ, ì§§ê²Œ í•œ ì¤„ë¡œ ë§Œë“¤ ìˆ˜ ìˆëŠ” íŒŒì´ì¬ì˜ ë¬¸ë²•)ì„ ì‚¬ìš©í•œ íš¨ìœ¨ì ì¸ ì¹´ìš´íŒ…: í•œ ë²ˆì˜ ìˆœíšŒë¡œ ì¡°ê±´ì— ë§ëŠ” í•­ëª© ìˆ˜ë¥¼ ê³„ì‚°
        # [ ( ë³€ìˆ˜ë¥¼ í™œìš©í•œ ê°’ ) for ( ì‚¬ìš©í•  ë³€ìˆ˜ ì´ë¦„ ) in ( ìˆœíšŒí•  ìˆ˜ ìˆëŠ” ê°’ ) if ( ì¡°ê±´ ) ]
        best_score = similar_answers[0]['score']  # ê°€ì¥ ë†’ì€ ìœ ì‚¬ë„ ì ìˆ˜
        high_quality_count = len([ans for ans in similar_answers if ans['score'] >= 0.7])    # ê³ í’ˆì§ˆ(0.7+) ë‹µë³€ ê°œìˆ˜
        medium_quality_count = len([ans for ans in similar_answers if 0.5 <= ans['score'] < 0.7])  # ì¤‘í’ˆì§ˆ(0.5-0.7) ë‹µë³€ ê°œìˆ˜
        
        # ìƒìœ„ 5ê°œì—ì„œ ì¹´í…Œê³ ë¦¬ ì¶”ì¶œí•˜ì—¬ ë¶„í¬ ê³„ì‚° (ë¹„ìŠ·í•œ ì¹´í…Œê³ ë¦¬ê°€ ë§ìœ¼ë©´ ë„ë©”ì¸ íŠ¹í™”ëœ ë‹µë³€ ê°€ëŠ¥)
        # ë”•ì…”ë„ˆë¦¬ ì»´í”„ë¦¬í—¨ì…˜: { í‚¤: ê°’ for í‚¤, ê°’ in ìˆœíšŒí•  ìˆ˜ ìˆëŠ” ê°’ if ì¡°ê±´ }
        categories = [ans['category'] for ans in similar_answers[:5]] # ìƒìœ„ 5ê°œ ë‹µë³€ì˜ ì¹´í…Œê³ ë¦¬ ì¶”ì¶œ (ë¦¬ìŠ¤íŠ¸ ì»´í”„ë¦¬í—¨ì…˜)
        category_distribution = {cat: categories.count(cat) for cat in set(categories)} # ì¹´í…Œê³ ë¦¬ë³„ ê°œìˆ˜ ê³„ì‚° (ë”•ì…”ë„ˆë¦¬ ì»´í”„ë¦¬í—¨ì…˜), set()ìœ¼ë¡œ ì¤‘ë³µ ì œê±°, ì¹´ìš´íŠ¸ ë©”ì„œë“œ ì‚¬ìš©
        
        # ì˜ì‚¬ ê²°ì • íŠ¸ë¦¬ : ìµœì ì˜ ë‹µë³€ ìƒì„± ì „ëµì„ ê²°ì •í•˜ëŠ” ì•Œê³ ë¦¬ì¦˜
        # ìµœê³  ìœ ì‚¬ë„ ì ìˆ˜ê°€ 0.8 ì´ìƒì´ë©´ ê¸°ì¡´ ë‹µë³€ ì§ì ‘ í™œìš©
        # ìµœê³  ìœ ì‚¬ë„ ì ìˆ˜ê°€ 0.7 ì´ìƒì´ê±°ë‚˜ ê³ í’ˆì§ˆ ë‹µë³€ì´ 2ê°œ ì´ìƒì´ë©´ ê³ í’ˆì§ˆ ì»¨í…ìŠ¤íŠ¸ë¡œ GPT ìƒì„±
        # ìµœê³  ìœ ì‚¬ë„ ì ìˆ˜ê°€ 0.6 ì´ìƒì´ê±°ë‚˜ ì¤‘í’ˆì§ˆ ë‹µë³€ì´ 3ê°œ ì´ìƒì´ë©´ ì•½í•œ ì»¨í…ìŠ¤íŠ¸ë¡œ GPT ìƒì„±
        # ê·¸ ì™¸ëŠ” í’ˆì§ˆì´ ë‚®ì•„ í´ë°± ì²˜ë¦¬
        if best_score >= 0.8:
            approach = 'direct_use'                # ë§¤ìš° ìœ ì‚¬ â†’ ê¸°ì¡´ ë‹µë³€ ì§ì ‘ í™œìš©
        elif best_score >= 0.7 or high_quality_count >= 2:
            approach = 'gpt_with_strong_context'   # ê³ í’ˆì§ˆ ì»¨í…ìŠ¤íŠ¸ë¡œ GPT ìƒì„±
        elif best_score >= 0.6 or medium_quality_count >= 3:
            approach = 'gpt_with_weak_context'     # ì•½í•œ ì»¨í…ìŠ¤íŠ¸ë¡œ GPT ìƒì„±
        else:
            approach = 'fallback'                  # í’ˆì§ˆì´ ë‚®ì•„ í´ë°± ì²˜ë¦¬
        
        # ë¶„ì„ ê²°ê³¼ êµ¬ì¡°í™”
        analysis = {
            'has_good_context': best_score >= 0.6,
            'best_score': best_score,
            'high_quality_count': high_quality_count,
            'medium_quality_count': medium_quality_count,
            'category_distribution': category_distribution,
            'recommended_approach': approach,
            'context_summary': f"ìµœê³ ì ìˆ˜: {best_score:.3f}, ê³ í’ˆì§ˆ: {high_quality_count}ê°œ, ì¤‘í’ˆì§ˆ: {medium_quality_count}ê°œ"
        }
        
        logging.info(f"ì»¨í…ìŠ¤íŠ¸ ë¶„ì„ ê²°ê³¼: {analysis}")
        return analysis

    # â˜† ì°¸ê³  ë‹µë³€ì—ì„œ ì¸ì‚¬ë§ê³¼ ëë§ºìŒë§ì„ ì œê±°í•˜ëŠ” ë©”ì„œë“œ
    # Args:
    #     text (str): ì œê±°í•  í…ìŠ¤íŠ¸
    #     lang (str): ì–¸ì–´ (ê¸°ë³¸ê°’: í•œêµ­ì–´)
            
    # Returns:
    #     str: ì œê±°ëœ í…ìŠ¤íŠ¸
    def remove_greeting_and_closing(self, text: str, lang: str = 'ko') -> str:
        # null ì²´í¬
        if not text:
            return ""
        
        if lang == 'ko':
            # í•œêµ­ì–´ ì¸ì‚¬ë§ ì œê±° íŒ¨í„´
            greeting_patterns = [
                r'^ì•ˆë…•í•˜ì„¸ìš”[^.]*\.\s*',
                r'^GOODTV\s+ë°”ì´ë¸”\s*ì• í”Œ[^.]*\.\s*',
                r'^ë°”ì´ë¸”\s*ì• í”Œ[^.]*\.\s*',
                r'^ì„±ë„ë‹˜[^.]*\.\s*',
                r'^ê³ ê°ë‹˜[^.]*\.\s*',
                r'^ê°ì‚¬í•©ë‹ˆë‹¤[^.]*\.\s*',
                r'^ê°ì‚¬ë“œë¦½ë‹ˆë‹¤[^.]*\.\s*',
                r'^ë°”ì´ë¸”\s*ì• í”Œì„\s*ì´ìš©í•´ì£¼ì…”ì„œ[^.]*\.\s*',
                r'^ë°”ì´ë¸”\s*ì• í”Œì„\s*ì• ìš©í•´\s*ì£¼ì…”ì„œ[^.]*\.\s*'
            ]
            
            # í•œêµ­ì–´ ëë§ºìŒë§ ì œê±° íŒ¨í„´ë“¤
            closing_patterns = [
                r'\s*ê°ì‚¬í•©ë‹ˆë‹¤[^.]*\.?\s*$',
                r'\s*ê°ì‚¬ë“œë¦½ë‹ˆë‹¤[^.]*\.?\s*$',
                r'\s*í‰ì•ˆí•˜ì„¸ìš”[^.]*\.?\s*$',
                r'\s*ì£¼ë‹˜\s*ì•ˆì—ì„œ[^.]*\.?\s*$',
                r'\s*í•¨ê»˜\s*ê¸°ë„í•˜ë©°[^.]*\.?\s*$',
                r'\s*í•­ìƒ[^.]*ë°”ì´ë¸”\s*ì• í”Œ[^.]*\.?\s*$',
                r'\s*í•­ìƒ\s*ì£¼ë‹˜\s*ì•ˆì—ì„œ[^.]*\.?\s*$',
                r'\s*ì£¼ë‹˜\s*ì•ˆì—ì„œ\s*í‰ì•ˆí•˜ì„¸ìš”[^.]*\.?\s*$',
                r'\s*ì£¼ë‹˜ì˜\s*ì€ì´ì´[^.]*\.?\s*$',
                r'\s*ê¸°ë„ë“œë¦¬ê² ìŠµë‹ˆë‹¤[^.]*\.?\s*$'
            ]

        else:  # ì˜ì–´ ì¸ì‚¬ë§ ì œê±° íŒ¨í„´
            greeting_patterns = [
                r'^Hello[^.]*\.\s*',
                r'^Hi[^.]*\.\s*',
                r'^Dear[^.]*\.\s*',
                r'^Thank you[^.]*\.\s*',
                r'^Thanks[^.]*\.\s*',
                r'^This is GOODTV Bible App[^.]*\.\s*',
            ]
            
            # ì˜ì–´ ëë§ºìŒë§ ì œê±° íŒ¨í„´
            closing_patterns = [
                r'\s*Thank you[^.]*\.?\s*$',
                r'\s*Thanks[^.]*\.?\s*$',
                r'\s*Best regards[^.]*\.?\s*$',
                r'\s*Sincerely[^.]*\.?\s*$',
                r'\s*God bless[^.]*\.?\s*$',
                r'\s*May God[^.]*\.?\s*$',
            ]
        
        # ì¸ì‚¬ë§ ì œê±°
        for pattern in greeting_patterns:
            text = re.sub(pattern, '', text, flags=re.IGNORECASE)
        
        # ëë§ºìŒë§ ì œê±°
        for pattern in closing_patterns:
            text = re.sub(pattern, '', text, flags=re.IGNORECASE)
        
        # ë¬¸ì¥ ëì˜ ëë§ºìŒë§ë“¤ë„ ì œê±°
        text = re.sub(r'[,.!?]\s*í•­ìƒ\s*ì£¼ë‹˜\s*ì•ˆì—ì„œ[^.]*\.?\s*$', '', text, flags=re.IGNORECASE)
        text = re.sub(r'[,.!?]\s*ê°ì‚¬í•©ë‹ˆë‹¤[^.]*\.?\s*$', '', text, flags=re.IGNORECASE)
        text = re.sub(r'[,.!?]\s*í‰ì•ˆí•˜ì„¸ìš”[^.]*\.?\s*$', '', text, flags=re.IGNORECASE)
        
        # ì•ë’¤ ê³µë°± ì •ë¦¬
        text = text.strip()
        
        return text

        # GPT ë‹µë³€ ìƒì„±ì„ ìœ„í•œ í–¥ìƒëœ ì»¨í…ìŠ¤íŠ¸ ìƒì„± ë©”ì„œë“œ
    # 
    # ì»¨í…ìŠ¤íŠ¸ ìƒì„± ì „ëµ:
    # 1. í’ˆì§ˆë³„ ë‹µë³€ ê·¸ë£¹í•‘ (ê³ /ì¤‘/ë‚®ì€ í’ˆì§ˆ)
    # 2. ê³ í’ˆì§ˆ ë‹µë³€ ìš°ì„  ì„ íƒ (ìµœëŒ€ 4ê°œ)
    # 3. ì¤‘í’ˆì§ˆ ë‹µë³€ìœ¼ë¡œ ë³´ì™„ (ìµœëŒ€ 3ê°œ)
    # 4. ìµœì†Œ ê°œìˆ˜ ë¯¸ë‹¬ì‹œ ì¤‘ê°„ í’ˆì§ˆ ë‹µë³€ ì¶”ê°€
    # 5. í…ìŠ¤íŠ¸ ì •ì œ ë° ê¸¸ì´ ì œí•œ (ì¸ì‚¬ë§/ëë§ºìŒë§ ì œê±°)
    # 
    # ì´ë ‡ê²Œ êµ¬ì„±ëœ ì»¨í…ìŠ¤íŠ¸ëŠ” GPTì—ê²Œ ì°¸ê³  ìë£Œë¡œ ì œê³µë˜ì–´
    # ì¼ê´€ëœ ìŠ¤íƒ€ì¼ê³¼ ì •í™•í•œ ì •ë³´ë¡œ ë‹µë³€ì„ ìƒì„±í•˜ê²Œ í•¨
    # 
    # Args:
    #     similar_answers (list): ê²€ìƒ‰ëœ ìœ ì‚¬ ë‹µë³€ ë¦¬ìŠ¤íŠ¸
    #     max_answers (int): í¬í•¨í•  ìµœëŒ€ ë‹µë³€ ê°œìˆ˜ (ê¸°ë³¸ê°’: 7)
    #     
    # Returns:
    #     str: GPTìš© ì»¨í…ìŠ¤íŠ¸ ë¬¸ìì—´
    def create_enhanced_context(self, similar_answers: list, max_answers: int = 7, target_lang: str = 'ko') -> str:
        if not similar_answers:
            return ""
        
        context_parts = [] # ì»¨í…ìŠ¤íŠ¸ ë¶€ë¶„ë“¤ì„ ì €ì¥í•  ë¦¬ìŠ¤íŠ¸
        used_answers = 0 # ì‚¬ìš©ëœ ë‹µë³€ ê°œìˆ˜
        
        # ìœ ì‚¬ë„ ì ìˆ˜ì— ë”°ë¥¸ ë‹µë³€ ê·¸ë£¹í•‘
        # ê³„ì¸µì  ê·¸ë£¹í•‘ìœ¼ë¡œ í’ˆì§ˆë³„ ë‹µë³€ì„ ë¶„ë¥˜: ì´ëŠ” ë‚˜ì¤‘ì— ìš°ì„ ìˆœìœ„ì— ë”°ë¼ ì„ íƒí•˜ê¸° ìœ„í•¨
        high_score = [ans for ans in similar_answers if ans['score'] >= 0.7]      # ê³ í’ˆì§ˆ (70% ì´ìƒ ìœ ì‚¬)
        medium_score = [ans for ans in similar_answers if 0.5 <= ans['score'] < 0.7]  # ì¤‘í’ˆì§ˆ (50-70%)
        medium_low_score = [ans for ans in similar_answers if 0.5 <= ans['score'] < 0.6] # ë‚®ì€ í’ˆì§ˆ (50-60%)

        # 1ë‹¨ê³„: ê³ í’ˆì§ˆ ë‹µë³€ ìš°ì„  í¬í•¨ (ìµœëŒ€ 4ê°œ)
        for ans in high_score[:4]:
            if used_answers >= max_answers:
                break
            # ì œì–´ ë¬¸ì(ì¤„ë°”ê¿ˆ, íƒ­, ê°œí–‰ ë“±) ë° HTML íƒœê·¸ ì œê±°
            clean_answer = re.sub(r'[\b\r\f\v\x00-\x08\x0B\x0C\x0E-\x1F\x7F]|<[^>]+>', '', ans['answer'])
            clean_answer = self.remove_greeting_and_closing(clean_answer, 'ko') # ì¸ì‚¬ë§ê³¼ ëë§ºìŒë§ ì œê±°í•˜ì—¬ ë³¸ë¬¸ë§Œ ì¶”ì¶œ
            
            # ì˜ì–´ ì§ˆë¬¸ì¸ ê²½ìš° ë‹µë³€ì„ ë²ˆì—­
            if target_lang == 'en' and ans.get('lang', 'ko') == 'ko':
                clean_answer = self.translate_text(clean_answer, 'ko', 'en')
            
            # ìœ íš¨ì„± ê²€ì‚¬ ë° ê¸¸ì´ ì œí•œ (ìµœì†Œ 20ì ì´ìƒí™•ì¸, 400ìë¡œ ì˜ë¼ì„œ ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´ ì œí•œ ë°©ì§€)
            if self.is_valid_text(clean_answer, target_lang) and len(clean_answer.strip()) > 20:
                context_parts.append(f"[ì°¸ê³ ë‹µë³€ {used_answers+1} - ì ìˆ˜: {ans['score']:.2f}]\n{clean_answer[:400]}")
                used_answers += 1
        
        # 2ë‹¨ê³„: ì¤‘í’ˆì§ˆ ë‹µë³€ìœ¼ë¡œ ë³´ì™„ (ìµœëŒ€ 3ê°œ)
        for ans in medium_score[:3]:
            if used_answers >= max_answers:
                break
            # ì œì–´ ë¬¸ì(ì¤„ë°”ê¿ˆ, íƒ­, ê°œí–‰ ë“±) ë° HTML íƒœê·¸ ì œê±°
            clean_answer = re.sub(r'[\b\r\f\v\x00-\x08\x0B\x0C\x0E-\x1F\x7F]|<[^>]+>', '', ans['answer'])
            clean_answer = self.remove_greeting_and_closing(clean_answer, 'ko') # ì¸ì‚¬ë§ê³¼ ëë§ºìŒë§ ì œê±°í•˜ì—¬ ë³¸ë¬¸ë§Œ ì¶”ì¶œ
            
            # ì˜ì–´ ì§ˆë¬¸ì¸ ê²½ìš° ë‹µë³€ì„ ë²ˆì—­
            if target_lang == 'en' and ans.get('lang', 'ko') == 'ko':
                clean_answer = self.translate_text(clean_answer, 'ko', 'en')
            
            if self.is_valid_text(clean_answer, target_lang) and len(clean_answer.strip()) > 20:
                context_parts.append(f"[ì°¸ê³ ë‹µë³€ {used_answers+1} - ì ìˆ˜: {ans['score']:.2f}]\n{clean_answer[:300]}")
                used_answers += 1

        # 3ë‹¨ê³„: ë‹µë³€ì´ ë¶€ì¡±í•œ ê²½ìš° ì¤‘ê°„ í’ˆì§ˆ ë‹µë³€ ì¶”ê°€ (50-60% êµ¬ê°„)
        if used_answers < 3:  # ìµœì†Œ 3ê°œ ì´ìƒ í™•ë³´í•˜ê¸° ìœ„í•¨
            for ans in medium_low_score[:2]:
                if used_answers >= max_answers:
                    break
                clean_answer = re.sub(r'[\b\r\f\v\x00-\x08\x0B\x0C\x0E-\x1F\x7F]|<[^>]+>', '', ans['answer'])
                clean_answer = self.remove_greeting_and_closing(clean_answer, 'ko')
                
                # ì˜ì–´ ì§ˆë¬¸ì¸ ê²½ìš° ë‹µë³€ì„ ë²ˆì—­
                if target_lang == 'en' and ans.get('lang', 'ko') == 'ko':
                    clean_answer = self.translate_text(clean_answer, 'ko', 'en')
                
                if self.is_valid_text(clean_answer, target_lang) and len(clean_answer.strip()) > 20:
                    context_parts.append(f"[ì°¸ê³ ë‹µë³€ {used_answers+1} - ì ìˆ˜: {ans['score']:.2f}]\n{clean_answer[:250]}")
                    used_answers += 1
        
        logging.info(f"ì»¨í…ìŠ¤íŠ¸ ìƒì„±: {used_answers}ê°œì˜ ë‹µë³€ í¬í•¨ (ì–¸ì–´: {target_lang})")
        
        # ìµœì¢… ì»¨í…ìŠ¤íŠ¸ ì¡°í•© (êµ¬ë¶„ì„ ìœ¼ë¡œ ë‹µë³€ë“¤ ë¶„ë¦¬)
        return "\n\n" + "="*50 + "\n\n".join(context_parts)

    # â˜† ì´ì „ ì•± ì´ë¦„ì„ ì œê±°í•˜ëŠ” ë©”ì„œë“œ (êµ¬ ë‹¤ë²ˆì—­ì„±ê²½ì°¬ì†¡ ë“±)
    # Args:
    #     text (str): ì œê±°í•  í…ìŠ¤íŠ¸
            
    # Returns:
    #     str: ì œê±°ëœ í…ìŠ¤íŠ¸
    def remove_old_app_name(self, text: str) -> str:
        patterns_to_remove = [
            r'\s*\(êµ¬\)\s*ë‹¤ë²ˆì—­ì„±ê²½ì°¬ì†¡',
            r'\s*\(êµ¬\)ë‹¤ë²ˆì—­ì„±ê²½ì°¬ì†¡',
            r'ë°”ì´ë¸”\s*ì• í”Œ\s*\(êµ¬\)\s*ë‹¤ë²ˆì—­ì„±ê²½ì°¬ì†¡',
            r'ë°”ì´ë¸”ì• í”Œ\s*\(êµ¬\)ë‹¤ë²ˆì—­ì„±ê²½ì°¬ì†¡',
        ]
        
        for pattern in patterns_to_remove:
            text = re.sub(pattern, '', text, flags=re.IGNORECASE)
        
        text = re.sub(r'(GOODTV\s+ë°”ì´ë¸”\s*ì• í”Œ)\s+', r'\1', text)
        
        return text

    # â˜† ë‹µë³€ í…ìŠ¤íŠ¸ë¥¼ HTML ë‹¨ë½ í˜•ì‹ìœ¼ë¡œ í¬ë§·íŒ…í•˜ëŠ” ë©”ì„œë“œ
    def format_answer_with_html_paragraphs(self, text: str, lang: str = 'ko') -> str:
        if not text:
            return ""
        
        text = self.remove_old_app_name(text)
        
        # ë¬¸ì¥ì„ ë§ˆì¹¨í‘œ, ëŠë‚Œí‘œ, ë¬¼ìŒí‘œë¡œ ë¶„ë¦¬
        sentences = re.split(r'(?<=[.!?])\s+', text)
        
        paragraphs = [] # ë‹¨ë½ë“¤ì„ ì €ì¥í•  ë¦¬ìŠ¤íŠ¸
        current_paragraph = [] # í˜„ì¬ ë‹¨ë½ì„ ì €ì¥í•  ë¦¬ìŠ¤íŠ¸
        
        # ë‹¨ë½ ë¶„ë¦¬ íŠ¸ë¦¬ê±° í‚¤ì›Œë“œë“¤ (ë” í¬ê´„ì ìœ¼ë¡œ í™•ì¥)
        # ì ‘ì†ì‚¬ë‚˜ ì¸ì‚¬ë§ë¡œ ì‹œì‘í•˜ëŠ” ë¬¸ì¥ì€ ë“¤ì—¬ì“°ê¸°ë¥¼ í†µí•´ ìƒˆ ë‹¨ë½ìœ¼ë¡œ ë¶„ë¦¬
        if lang == 'ko':
            paragraph_triggers = [
                # ì¸ì‚¬ ë° ê°ì‚¬
                'ì•ˆë…•í•˜ì„¸ìš”', 'ê°ì‚¬í•©ë‹ˆë‹¤', 'ê°ì‚¬ë“œë¦½ë‹ˆë‹¤', 'ë°”ì´ë¸” ì• í”Œì„',
                # ì ‘ì†ì–´ ë° ì—°ê²°ì–´
                'ë”°ë¼ì„œ', 'ê·¸ëŸ¬ë¯€ë¡œ', 'ë˜í•œ', 'ê·¸ë¦¬ê³ ', 'ë˜ëŠ”', 'í•˜ì§€ë§Œ', 'ê·¸ëŸ°ë°',
                'ì´ì™¸', 'ì´ì—', 'ì´ë¥¼', 'ì´ë¡œ', 'ì´ì™€', 'ì´ì—', 'ì´ìƒ', 'ì´í•˜',
                # ìƒí™© ì„¤ëª…
                'í˜„ì¬', 'ì§€ê¸ˆ', 'í˜„ì¬ë¡œ', 'í˜„ì¬ê¹Œì§€', 'í˜„ì¬ë¡œì„œëŠ”',
                'ë‚´ë¶€ì ìœ¼ë¡œ', 'ì™¸ë¶€ì ìœ¼ë¡œ', 'ê¸°ìˆ ì ìœ¼ë¡œ', 'ìš´ì˜ìƒ',
                # ì¡°ê±´ ë° ê°€ì •
                'ë§Œì•½', 'í˜¹ì‹œ', 'ë§Œì¼', 'ë§Œì•½ì—', 'ë§Œì•½ì˜',
                'í•´ë‹¹', 'ì´', 'ê·¸', 'ì €', 'ì´ëŸ°', 'ê·¸ëŸ°', 'ì €ëŸ°',
                # ìš”ì²­ ë° ì•ˆë‚´
                'ì„±ë„ë‹˜', 'ê³ ê°ë‹˜', 'ì´ìš©ì', 'ì‚¬ìš©ì',
                'ë²ˆê±°ë¡œìš°ì‹œ', 'ë¶ˆí¸í•˜ì‹œ', 'ì£„ì†¡í•˜ì§€ë§Œ', 'ì°¸ê³ ë¡œ',
                'ì–‘í•´ë¶€íƒë“œë¦½ë‹ˆë‹¤', 'ì–‘í•´í•´ì£¼ì‹œê¸°', 'ì´í•´í•´ì£¼ì‹œê¸°',
                # ì‹œê°„ ê´€ë ¨
                'í•­ìƒ', 'ëŠ˜', 'ì•ìœ¼ë¡œë„', 'ì§€ì†ì ìœ¼ë¡œ', 'ê³„ì†ì ìœ¼ë¡œ',
                'ì‹œê°„ì´', 'ì†Œìš”ë ', 'ê±¸ë¦´', 'í•„ìš”í•œ',
                # ê¸°ëŠ¥ ê´€ë ¨
                'ê¸°ëŠ¥', 'ê¸°ëŠ¥ì€', 'ê¸°ëŠ¥ì˜', 'ê¸°ëŠ¥ì´', 'ê¸°ëŠ¥ì„',
                'ìŠ¤í”¼ì»¤', 'ë²„íŠ¼', 'ë©”ë‰´', 'í™”ë©´', 'ì„¤ì •', 'ì˜µì…˜',
                # ì˜ê²¬ ë° ì „ë‹¬
                'ì˜ê²¬ì€', 'ì˜ê²¬ì„', 'ì „ë‹¬í• ', 'ì „ë‹¬í•˜ê² ìŠµë‹ˆë‹¤', 'ì „ë‹¬ë“œë¦¬ê² ìŠµë‹ˆë‹¤',
                'í† ì˜ê°€', 'ê²€í† ê°€', 'ê²€í† ë¥¼', 'ë…¼ì˜ê°€', 'ë…¼ì˜ë¥¼'
            ]
        else:  # ì˜ì–´
            paragraph_triggers = [
                # Greetings and appreciation
                'Hello', 'Hi', 'Dear', 'Thank', 'Thanks', 'Appreciate',
                'Grateful', 'Welcome', 'Greetings',
                
                # Conjunctions and transitions
                'Therefore', 'However', 'Additionally', 'Furthermore', 
                'Moreover', 'Nevertheless', 'Nonetheless', 'Meanwhile',
                'Subsequently', 'Consequently', 'Hence', 'Thus', 'Besides',
                'Although', 'Though', 'While', 'Whereas', 'Instead',
                
                # Situation descriptions
                'Currently', 'Presently', 'At the moment', 'Now',
                'At this time', 'As of now', 'Recently', 'Lately',
                'Technically', 'Internally', 'Externally', 'Generally',
                'Specifically', 'Basically', 'Essentially', 'Fundamentally',
                
                # Conditions and assumptions
                'If', 'When', 'Where', 'Whether', 'Unless', 'Provided',
                'Assuming', 'Suppose', 'In case', 'Should', 'Would',
                'Could', 'Might', 'May',
                
                # Requests and guidance
                'Please', 'Kindly', 'We recommend', 'We suggest',
                'You can', 'You may', 'You should', 'You might',
                'Try', 'Consider', 'Note that', 'Be aware',
                'Remember', 'Keep in mind', 'Important',
                
                # Apologies and understanding
                'Sorry', 'Apologize', 'Apologies', 'Unfortunately',
                'Regret', 'Understand', 'Realize', 'Acknowledge',
                'We know', 'We understand', 'We appreciate',
                
                # Time-related
                'Always', 'Usually', 'Often', 'Sometimes', 'Occasionally',
                'Frequently', 'Regularly', 'Continuously', 'Constantly',
                'Soon', 'Shortly', 'Eventually', 'Later', 'Previously',
                
                # Feature and function related
                'Feature', 'Function', 'Option', 'Setting', 'Button',
                'Menu', 'Screen', 'Tab', 'Page', 'Section', 'Tool',
                'Service', 'System', 'Application', 'Update', 'Version',
                
                # Problem-solving related
                'To fix', 'To solve', 'To resolve', 'To address',
                'Solution', 'Resolution', 'Workaround', 'Alternative',
                'Issue', 'Problem', 'Error', 'Bug', 'Trouble',
                
                # Feedback and communication
                'Your feedback', 'Your suggestion', 'Your opinion',
                'We will', 'We are', 'We have', 'Our team',
                'Will be', 'Has been', 'Have been', 'Working on',
                'Looking into', 'Reviewing', 'Considering', 'Planning',
                
                # Instructions and steps
                'First', 'Second', 'Third', 'Next', 'Then', 'After',
                'Before', 'Finally', 'Lastly', 'To begin', 'To start',
                'Step', 'Follow', 'Navigate', 'Click', 'Tap', 'Select',
                
                # Emphasis and clarification
                'Indeed', 'In fact', 'Actually', 'Certainly', 'Definitely',
                'Clearly', 'Obviously', 'Importantly', 'Notably',
                'Particularly', 'Especially', 'Specifically'
            ]
        
        for i, sentence in enumerate(sentences):
            sentence = sentence.strip()
            if not sentence:
                continue
            
            # ì²« ë²ˆì§¸ ë¬¸ì¥ (ì¸ì‚¬ë§)ì€ í•­ìƒ ë³„ë„ ë‹¨ë½
            if i == 0:
                if current_paragraph:
                    paragraphs.append(' '.join(current_paragraph))
                    current_paragraph = []
                paragraphs.append(sentence)
                continue
            
            should_break = False
            
            # íŠ¸ë¦¬ê±° í‚¤ì›Œë“œë¡œ ì‹œì‘í•˜ëŠ” ë¬¸ì¥ì€ ìƒˆ ë‹¨ë½
            for trigger in paragraph_triggers:
                if sentence.startswith(trigger):
                    should_break = True
                    break
            
            # í˜„ì¬ ë‹¨ë½ì— 2ê°œ ì´ìƒ ë¬¸ì¥ì´ ìˆìœ¼ë©´ ìƒˆ ë‹¨ë½
            if current_paragraph and len(current_paragraph) >= 2:
                should_break = True

            # ëë§ºìŒë§ì´ í¬í•¨ëœ ë¬¸ì¥ì€ ìƒˆ ë‹¨ë½
            if any(closing in sentence for closing in ['ê°ì‚¬í•©ë‹ˆë‹¤', 'ê°ì‚¬ë“œë¦½ë‹ˆë‹¤', 'í‰ì•ˆí•˜ì„¸ìš”', 'ì£¼ë‹˜ ì•ˆì—ì„œ']):
                should_break = True
            
            # ë¬¸ì¥ ê¸¸ì´ê°€ 50ì ì´ìƒì´ê³  í˜„ì¬ ë‹¨ë½ì´ ìˆìœ¼ë©´ ìƒˆ ë‹¨ë½
            if len(sentence) > 50 and current_paragraph:
                should_break = True
            
            # ìƒˆ ë‹¨ë½ ë¶„ë¦¬
            if should_break and current_paragraph:
                paragraphs.append(' '.join(current_paragraph))
                current_paragraph = [sentence]
            else:
                current_paragraph.append(sentence)
        
        # ë§ˆì§€ë§‰ ë‹¨ë½ ì¶”ê°€
        if current_paragraph:
            paragraphs.append(' '.join(current_paragraph))
        
        # Quill ì—ë””í„° í˜¸í™˜ì„ ìœ„í•œ HTML ë‹¨ë½ìœ¼ë¡œ ë³€í™˜
        # ê° ë‹¨ë½ì„ <p> íƒœê·¸ë¡œ ê°ì‹¸ê³ , ë‹¨ë½ ì‚¬ì´ì— <p><br></p> íƒœê·¸ë¡œ ë¹ˆ ì¤„ ì¶”ê°€
        html_paragraphs = []
        for i, paragraph in enumerate(paragraphs):
            html_paragraphs.append(f"<p>{paragraph}</p>")
            
            # ë‹¨ë½ ì‚¬ì´ì— ë¹ˆ ì¤„ ì¶”ê°€ (ë§ˆì§€ë§‰ ë‹¨ë½ ì œì™¸)
            if i < len(paragraphs) - 1:
                html_paragraphs.append("<p><br></p>")
        
        return ''.join(html_paragraphs)

    # â˜† ë‹µë³€ í…ìŠ¤íŠ¸ë¥¼ ì •ë¦¬í•˜ê³  í¬ë§·íŒ…í•˜ëŠ” ë©”ì„œë“œ (Quill ì—ë””í„°ìš©)
    def clean_answer_text(self, text: str) -> str:
        if not text:
            return ""
        
        # ì œì–´ ë¬¸ìë§Œ ì œê±°í•˜ê³  HTML íƒœê·¸ëŠ” ìœ ì§€
        text = re.sub(r'[\b\r\f\v]', '', text)
        text = re.sub(r'[\x00-\x08\x0B\x0C\x0E-\x1F\x7F]', '', text)

        # HTML íƒœê·¸ ì œê±°í•˜ì§€ ì•ŠìŒ (Quill ì—ë””í„°ìš©)
        text = re.sub(r'\*\*([^*]+)\*\*', r'\1', text)
        text = re.sub(r'\*([^*]+)\*', r'\1', text)
        
        # HTML íƒœê·¸ ë‚´ë¶€ì˜ ê³µë°±ë§Œ ì •ë¦¬ (íƒœê·¸ ìì²´ëŠ” ìœ ì§€)
        text = re.sub(r'>\s+<', '><', text)  # íƒœê·¸ ì‚¬ì´ ê³µë°± ì œê±°
        text = re.sub(r'<p>\s+', '<p>', text)  # <p> íƒœê·¸ ë‚´ë¶€ ì• ê³µë°± ì œê±°
        text = re.sub(r'\s+</p>', '</p>', text)  # </p> íƒœê·¸ ì• ê³µë°± ì œê±°
        
        text = self.remove_old_app_name(text)
        text = self.format_answer_with_html_paragraphs(text)
        
        return text

    # â˜† í…ìŠ¤íŠ¸ ìœ íš¨ì„± ê²€ì¦ ë©”ì„œë“œ
    def is_valid_text(self, text: str, lang: str = 'ko') -> bool:
        if not text or len(text.strip()) < 3:
            return False
        
        if lang == 'ko':
            return self.is_valid_korean_text(text)
        else:  # ì˜ì–´
            return self.is_valid_english_text(text)

    # â˜† í•œêµ­ì–´ í…ìŠ¤íŠ¸ì˜ ìœ íš¨ì„±ì„ ê²€ì¦í•˜ëŠ” ë©”ì„œë“œ
    def is_valid_korean_text(self, text: str) -> bool:
        if not text or len(text.strip()) < 3:
            return False
        
        korean_chars = len(re.findall(r'[ê°€-í£]', text))
        total_chars = len(re.sub(r'\s', '', text))
        
        if total_chars == 0:
            return False
            
        korean_ratio = korean_chars / total_chars
        
        if korean_ratio < 0.3:
            return False
        
        # ë¬´ì˜ë¯¸í•œ íŒ¨í„´ ê°ì§€ (GPT í• ë£¨ì‹œë„¤ì´ì…˜ ë°©ì§€)
        meaningless_patterns = [
            r'^[a-z\s\.,;:\(\)\[\]\-_&\/\'"]+$', # ì˜ì–´
            r'^[A-Z\s\.,;:\(\)\[\]\-_&\/\'"]+$', # ì˜ì–´ ëŒ€ë¬¸ì
            r'^[\s\.,;:\(\)\[\]\-_&\/\'"]+$',    # ê³µë°±
            r'^[0-9\s\.,;:\(\)\[\]\-_&\/\'"]+$', # ìˆ«ì
            r'.*[Ğ°-Ñ].*',                        # ëŸ¬ì‹œì•„ì–´
            r'.*[Î±-Ï‰].*',                        # ê·¸ë¦¬ìŠ¤ì–´
        ]
        
        for pattern in meaningless_patterns:
            if re.match(pattern, text, re.IGNORECASE):
                return False
        
        # ë°˜ë³µ ë¬¸ì ê°ì§€: ê°™ì€ ë¬¸ìê°€ 5ë²ˆ ì´ìƒ ì—°ì†ìœ¼ë¡œ ë‚˜íƒ€ë‚˜ë©´ ë¹„ì •ìƒ í…ìŠ¤íŠ¸ë¡œ ê°„ì£¼
        if re.search(r'(.)\1{5,}', text):
            return False
        
        random_pattern = r'[a-zA-Z]{8,}'
        if re.search(random_pattern, text) and korean_ratio < 0.5:
            return False
        
        return True

    # â˜† ì˜ì–´ í…ìŠ¤íŠ¸ì˜ ìœ íš¨ì„±ì„ ê²€ì¦í•˜ëŠ” ë©”ì„œë“œ
    def is_valid_english_text(self, text: str) -> bool:
        if not text or len(text.strip()) < 3:
            return False
        
        english_chars = len(re.findall(r'[a-zA-Z]', text))
        total_chars = len(re.sub(r'\s', '', text))
        
        if total_chars == 0:
            return False
            
        english_ratio = english_chars / total_chars
        
        if english_ratio < 0.7:  # ì˜ì–´ ë¹„ìœ¨ì´ 70% ë¯¸ë§Œì´ë©´ ë¬´íš¨
            return False
        
        # ë°˜ë³µ ë¬¸ì ê°ì§€
        if re.search(r'(.)\1{5,}', text):
            return False
        
        return True

    # â˜† ìƒì„±ëœ í…ìŠ¤íŠ¸ë¥¼ ì •ë¦¬í•˜ê³  ê²€ì¦í•˜ëŠ” ë©”ì„œë“œ
    def clean_generated_text(self, text: str) -> str:
        if not text:
            return ""
        # ì œì–´ ë¬¸ì ì œê±°
        text = re.sub(r'[\x00-\x08\x0B\x0C\x0E-\x1F\x7F]', '', text)
        text = re.sub(r'[\b\r\f\v]', '', text)

        # ì˜ì–´ ì•½ì–´ ì œê±°
        text = re.sub(r'\b[a-z]{1,2}\b(?:\s+[a-z]{1,2}\b)*', '', text, flags=re.IGNORECASE)
        text = re.sub(r'[Ğ°-Ñ]+', '', text)
        text = re.sub(r'[Î±-Ï‰]+', '', text)

        # í•œê¸€ ë¬¸ì ì œê±°
        text = re.sub(r'[^\w\sê°€-í£.,!?()"\'-]{3,}', '', text)
        text = re.sub(r'[.,;:!?]{3,}', '.', text)

        # ê³µë°± ì •ë¦¬
        text = re.sub(r'\s+', ' ', text)
        text = text.strip()
        
        return text

    # â˜† í†µì¼ëœ GPT í”„ë¡¬í”„íŠ¸ ìƒì„± ë©”ì„œë“œ (ëª¨ë“ˆí™”)
    def get_gpt_prompts(self, query: str, context: str, lang: str = 'ko') -> tuple:
        """ì–¸ì–´ë³„ GPT í”„ë¡¬í”„íŠ¸ ìƒì„±"""
        if lang == 'en': # ì˜ì–´
            system_prompt = """You are a GOODTV Bible App customer service representative.

Guidelines:
1. Follow the style and content of the provided reference answers faithfully
2. Find and apply solutions from similar situations in the reference answers
3. Adapt to the customer's specific situation while maintaining the tone and style of the reference answers

âš ï¸ Absolute Prohibitions:
- Do not guide non-existent features or menus
- Do not create specific settings methods or button locations
- If a feature is not in the reference answers, say "Sorry, this feature is currently not available"
- If uncertain, respond with "We will review this internally"

4. For feature requests or improvement suggestions, use:
   - "Thank you for your valuable feedback"
   - "We will discuss/review this internally"
   - "We will forward this as an improvement"

5. Address customers as 'Dear user' or similar polite forms
6. Use 'GOODTV Bible App' or 'Bible App' as the app name

ğŸš« Do NOT generate greetings or closings:
- Do not use "Hello", "Thank you", "Best regards", etc.
- Do not use "God bless", "In Christ", etc.
- Only write the main content

7. Do not use HTML tags, write in natural sentences"""

            user_prompt = f"""Customer inquiry: {query}

Reference answers (main content only, greetings and closings removed):
{context}

Based on the reference answers' solution methods and tone, write a specific answer to the customer's problem.
Important: Do not include greetings or closings. Only write the main content."""

        else:  # í•œêµ­ì–´
            system_prompt = """ë‹¹ì‹ ì€ GOODTV ë°”ì´ë¸” ì• í”Œ ê³ ê°ì„¼í„° ìƒë‹´ì›ì…ë‹ˆë‹¤.

ì§€ì¹¨:
1. ì œê³µëœ ì°¸ê³  ë‹µë³€ë“¤ì˜ ìŠ¤íƒ€ì¼ê³¼ ë‚´ìš©ì„ ì¶©ì‹¤íˆ ë”°ë¼ ì‘ì„±í•˜ì„¸ìš”
2. ì°¸ê³  ë‹µë³€ì—ì„œ ìœ ì‚¬í•œ ìƒí™©ì˜ í•´ê²°ì±…ì„ ì°¾ì•„ ì ìš©í•˜ì„¸ìš”
3. ê³ ê°ì˜ êµ¬ì²´ì  ìƒí™©ì— ë§ê²Œ ë³´ì™„í•˜ë˜, ì°¸ê³  ë‹µë³€ì˜ í†¤ê³¼ ìŠ¤íƒ€ì¼ì„ ìœ ì§€í•˜ì„¸ìš”

âš ï¸ ì ˆëŒ€ ê¸ˆì§€ì‚¬í•­:
- ì¡´ì¬í•˜ì§€ ì•ŠëŠ” ê¸°ëŠ¥ì´ë‚˜ ë©”ë‰´ë¥¼ ì•ˆë‚´í•˜ì§€ ë§ˆì„¸ìš”
- êµ¬ì²´ì ì¸ ì„¤ì • ë°©ë²•ì´ë‚˜ ë²„íŠ¼ ìœ„ì¹˜ë¥¼ ì°½ì‘í•˜ì§€ ë§ˆì„¸ìš”
- ì°¸ê³  ë‹µë³€ì— ì—†ëŠ” ê¸°ëŠ¥ì€ "ì£„ì†¡í•˜ì§€ë§Œ í˜„ì¬ ì œê³µë˜ì§€ ì•ŠëŠ” ê¸°ëŠ¥"ìœ¼ë¡œ ì•ˆë‚´í•˜ì„¸ìš”
- ë¶ˆí™•ì‹¤í•œ ê²½ìš° "ë‚´ë¶€ì ìœ¼ë¡œ ê²€í† í•˜ê² ë‹¤"ê³  ë‹µë³€í•˜ì„¸ìš”

4. ê¸°ëŠ¥ ìš”ì²­ì´ë‚˜ ê°œì„  ì œì•ˆì˜ ê²½ìš°:
   - "ì¢‹ì€ ì˜ê²¬ ê°ì‚¬í•©ë‹ˆë‹¤"
   - "ë‚´ë¶€ì ìœ¼ë¡œ ë…¼ì˜/ê²€í† í•˜ê² ìŠµë‹ˆë‹¤"
   - "ê°œì„  ì‚¬í•­ìœ¼ë¡œ ì „ë‹¬í•˜ê² ìŠµë‹ˆë‹¤"
   ìœ„ í‘œí˜„ë“¤ì„ ì‚¬ìš©í•˜ì„¸ìš”

5. ê³ ê°ì€ ë°˜ë“œì‹œ 'ì„±ë„ë‹˜'ìœ¼ë¡œ í˜¸ì¹­í•˜ì„¸ìš”
6. ì•± ì´ë¦„ì€ 'GOODTV ë°”ì´ë¸” ì• í”Œ' ë˜ëŠ” 'ë°”ì´ë¸” ì• í”Œ'ë¡œ í†µì¼í•˜ì„¸ìš”

ğŸš« ì¸ì‚¬ë§ ë° ëë§ºìŒë§ ìƒì„± ê¸ˆì§€:
- "ì•ˆë…•í•˜ì„¸ìš”", "ê°ì‚¬í•©ë‹ˆë‹¤", "í‰ì•ˆí•˜ì„¸ìš”" ë“±ì˜ ì¸ì‚¬ë§ì„ ì ˆëŒ€ ì‚¬ìš©í•˜ì§€ ë§ˆì„¸ìš”
- "ì£¼ë‹˜ ì•ˆì—ì„œ", "ê¸°ë„ë“œë¦¬ê² ìŠµë‹ˆë‹¤" ë“±ì˜ ëë§ºìŒë§ì„ ì ˆëŒ€ ì‚¬ìš©í•˜ì§€ ë§ˆì„¸ìš”
- ì˜¤ì§ ë³¸ë¬¸ ë‚´ìš©ë§Œ ì‘ì„±í•˜ì„¸ìš”

7. HTML íƒœê·¸ ì‚¬ìš© ê¸ˆì§€, ìì—°ìŠ¤ëŸ¬ìš´ ë¬¸ì¥ìœ¼ë¡œ ì‘ì„±í•˜ì„¸ìš”"""

            user_prompt = f"""ê³ ê° ë¬¸ì˜: {query}

ì°¸ê³  ë‹µë³€ë“¤ (ì¸ì‚¬ë§ê³¼ ëë§ºìŒë§ì€ ì œê±°ëœ ë³¸ë¬¸ë§Œ í¬í•¨):
{context}

ìœ„ ì°¸ê³  ë‹µë³€ë“¤ì˜ í•´ê²° ë°©ì‹ê³¼ í†¤ì„ ê·¸ëŒ€ë¡œ ë”°ë¼ì„œ ê³ ê°ì˜ ë¬¸ì œì— ëŒ€í•œ êµ¬ì²´ì ì¸ ë‹µë³€ì„ ì‘ì„±í•˜ì„¸ìš”.
ì¤‘ìš”: ì¸ì‚¬ë§("ì•ˆë…•í•˜ì„¸ìš”", "ê°ì‚¬í•©ë‹ˆë‹¤" ë“±)ì´ë‚˜ ëë§ºìŒë§("í‰ì•ˆí•˜ì„¸ìš”", "ì£¼ë‹˜ ì•ˆì—ì„œ" ë“±)ì„ ì ˆëŒ€ í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”. ì˜¤ì§ ë³¸ë¬¸ ë‚´ìš©ë§Œ ì‘ì„±í•˜ì„¸ìš”."""

        return system_prompt, user_prompt

    # í–¥ìƒëœ GPT ìƒì„± - í†µì¼ëœ í”„ë¡¬í”„íŠ¸ ì‚¬ìš©
    def generate_with_enhanced_gpt(self, query: str, similar_answers: list, context_analysis: dict, lang: str = 'ko') -> str:
        try:
            with memory_cleanup():
                approach = context_analysis['recommended_approach']
                context = self.create_enhanced_context(similar_answers, target_lang=lang)
                
                if not context:
                    logging.warning("ìœ íš¨í•œ ì»¨í…ìŠ¤íŠ¸ê°€ ì—†ì–´ GPT ìƒì„± ì¤‘ë‹¨")
                    return ""
                
                # í†µì¼ëœ í”„ë¡¬í”„íŠ¸ ìƒì„±
                system_prompt, user_prompt = self.get_gpt_prompts(query, context, lang)
                
                # ì ‘ê·¼ ë°©ì‹ë³„ temperatureì™€ max_tokens ì„¤ì •
                if approach == 'gpt_with_strong_context':
                    temperature = 0.2 # ì°½ì˜ì„±: ë§¤ìš° ë³´ìˆ˜ì ì 
                    max_tokens = 600
                elif approach == 'gpt_with_weak_context':
                    temperature = 0.4 # ì°½ì˜ì„±: ì ë‹¹í•œ ì°½ì˜ì„±
                    max_tokens = 650
                else: # fallbackì´ë‚˜ ê¸°íƒ€
                    return ""
                
                # GPT API í˜¸ì¶œ
                response = self.openai_client.chat.completions.create(
                    model=GPT_MODEL,
                    messages=[
                        {"role": "system", "content": system_prompt},
                        {"role": "user", "content": user_prompt}
                    ],
                    max_tokens=max_tokens,
                    temperature=temperature,
                    top_p=0.8,
                    frequency_penalty=0.1,
                    presence_penalty=0.1
                )
                
                generated = response.choices[0].message.content.strip()
                del response
                
                # ìƒì„±ëœ í…ìŠ¤íŠ¸ ì •ë¦¬
                generated = self.clean_generated_text(generated)
                
                # í…ìŠ¤íŠ¸ ìœ íš¨ì„± ê²€ì¦
                if not self.is_valid_text(generated, lang):
                    logging.warning(f"GPTê°€ ë¬´íš¨í•œ í…ìŠ¤íŠ¸ ìƒì„±: {generated[:50]}...")
                    return ""
                
                logging.info(f"GPT ìƒì„± ì„±ê³µ ({approach}, ì–¸ì–´: {lang}): {len(generated)}ì")
                return generated[:650]
                
        except Exception as e:
            logging.error(f"í–¥ìƒëœ GPT ìƒì„± ì‹¤íŒ¨: {e}")
            return ""

    # â˜† ìµœì ì˜ í´ë°± ë‹µë³€ ì„ íƒ ë©”ì„œë“œ
    def get_best_fallback_answer(self, similar_answers: list, lang: str = 'ko') -> str:
        if not similar_answers:
            return ""
        
        # ì ìˆ˜ì™€ í…ìŠ¤íŠ¸ í’ˆì§ˆì„ ì¢…í•© í‰ê°€
        best_answer = ""
        best_score = 0
        
        for ans in similar_answers[:5]: # ìƒìœ„ 5ê°œë§Œ ê²€í† 
            score = ans['score']
            answer_text = self.clean_generated_text(ans['answer'])
            
            # ì˜ì–´ ì§ˆë¬¸ì¸ ê²½ìš° ë‹µë³€ì„ ë²ˆì—­
            if lang == 'en' and ans.get('lang', 'ko') == 'ko':
                answer_text = self.translate_text(answer_text, 'ko', 'en')
            
            if not self.is_valid_text(answer_text, lang):
                continue
            
            # ì¢…í•© ì ìˆ˜ ê³„ì‚° (ìœ ì‚¬ë„ + í…ìŠ¤íŠ¸ ê¸¸ì´ + ì™„ì„±ë„)
            length_score = min(len(answer_text) / 200, 1.0) # 200ì ê¸°ì¤€ ì •ê·œí™”
            completeness_score = 1.0 if answer_text.endswith(('.', '!', '?')) else 0.8
            
            total_score = score * 0.7 + length_score * 0.2 + completeness_score * 0.1
            
            if total_score > best_score:
                best_score = total_score
                best_answer = answer_text
        
        return best_answer

    # ë” ë³´ìˆ˜ì ì¸ GPT-3.5-turbo ìƒì„± ë©”ì„œë“œ (ê¸°ì¡´ ì½”ë“œì™€ì˜ í˜¸í™˜ì„± ìœ ì§€)
    # ë³´ìˆ˜ì ì´ê³  ì°¸ê³  ë‹µë³€ì— ì¶©ì‹¤í•œ GPT-3.5-turbo í…ìŠ¤íŠ¸ ìƒì„±
    @profile
    def generate_ai_answer(self, query: str, similar_answers: list, lang: str) -> str:
        
        # 1. ì–¸ì–´ ê°ì§€ (lang íŒŒë¼ë¯¸í„°ê°€ ì—†ê±°ë‚˜ 'auto'ì¸ ê²½ìš°)
        if not lang or lang == 'auto':
            detected_lang = self.detect_language(query)
            lang = detected_lang
            logging.info(f"ê°ì§€ëœ ì–¸ì–´: {lang}")
        
        # 2. ìœ ì‚¬ ë‹µë³€ì´ ì—†ëŠ” ê²½ìš°
        if not similar_answers:
            if lang == 'en':
                default_msg = "<p>We need more detailed information to provide an accurate answer to your inquiry.</p><p><br></p><p>Please contact our customer service center for prompt assistance.</p>"
            else:
                default_msg = "<p>ë¬¸ì˜í•´ì£¼ì‹  ë‚´ìš©ì— ëŒ€í•´ ì •í™•í•œ ë‹µë³€ì„ ë“œë¦¬ê¸° ìœ„í•´ ë” ìì„¸í•œ ì •ë³´ê°€ í•„ìš”í•©ë‹ˆë‹¤.</p><p><br></p><p>ê³ ê°ì„¼í„°ë¡œ ë¬¸ì˜í•´ì£¼ì‹œë©´ ì‹ ì†í•˜ê²Œ ë„ì›€ì„ ë“œë¦¬ê² ìŠµë‹ˆë‹¤.</p>"
            return default_msg
        
        # 3. ì»¨í…ìŠ¤íŠ¸ ë¶„ì„
        context_analysis = self.analyze_context_quality(similar_answers, query)
        
        # 4. ìœ ìš©í•œ ì»¨í…ìŠ¤íŠ¸ê°€ ì—†ëŠ” ê²½ìš°
        if not context_analysis['has_good_context']:
            logging.warning("ìœ ìš©í•œ ì»¨í…ìŠ¤íŠ¸ê°€ ì—†ì–´ ê¸°ë³¸ ë©”ì‹œì§€ ë°˜í™˜")
            if lang == 'en':
                return "<p>We need more detailed information to provide an accurate answer to your inquiry.</p><p><br></p><p>Please contact our customer service center for prompt assistance.</p>"
            else:
                return "<p>ë¬¸ì˜í•´ì£¼ì‹  ë‚´ìš©ì— ëŒ€í•´ ì •í™•í•œ ë‹µë³€ì„ ë“œë¦¬ê¸° ìœ„í•´ ë” ìì„¸í•œ ì •ë³´ê°€ í•„ìš”í•©ë‹ˆë‹¤.</p><p><br></p><p>ê³ ê°ì„¼í„°ë¡œ ë¬¸ì˜í•´ì£¼ì‹œë©´ ì‹ ì†í•˜ê²Œ ë„ì›€ì„ ë“œë¦¬ê² ìŠµë‹ˆë‹¤.</p>"
        
        try:
            approach = context_analysis['recommended_approach']
            logging.info(f"ì„ íƒëœ ì ‘ê·¼ ë°©ì‹: {approach}, ì–¸ì–´: {lang}")
            
            if approach == 'direct_use':
                base_answer = self.get_best_fallback_answer(similar_answers[:3], lang)
                logging.info("ë†’ì€ ìœ ì‚¬ë„ë¡œ ì§ì ‘ ì‚¬ìš©")
                
            elif approach in ['gpt_with_strong_context', 'gpt_with_weak_context']:
                base_answer = self.generate_with_enhanced_gpt(query, similar_answers, context_analysis, lang)
                
                if not base_answer or not self.is_valid_text(base_answer, lang):
                    logging.warning("GPT ìƒì„± ì‹¤íŒ¨, í´ë°± ë‹µë³€ ì‚¬ìš©")
                    base_answer = self.get_best_fallback_answer(similar_answers, lang)
                    
            else:
                base_answer = self.get_best_fallback_answer(similar_answers, lang)
            
            if not base_answer or not self.is_valid_text(base_answer, lang):
                logging.error("ëª¨ë“  ë‹µë³€ ìƒì„± ë°©ë²• ì‹¤íŒ¨")
                if lang == 'en':
                    return "<p>We need more detailed information to provide an accurate answer to your inquiry.</p><p><br></p><p>Please contact our customer service center for prompt assistance.</p>"
                else:
                    return "<p>ë¬¸ì˜í•´ì£¼ì‹  ë‚´ìš©ì— ëŒ€í•´ ì •í™•í•œ ë‹µë³€ì„ ë“œë¦¬ê¸° ìœ„í•´ ë” ìì„¸í•œ ì •ë³´ê°€ í•„ìš”í•©ë‹ˆë‹¤.</p><p><br></p><p>ê³ ê°ì„¼í„°ë¡œ ë¬¸ì˜í•´ì£¼ì‹œë©´ ì‹ ì†í•˜ê²Œ ë„ì›€ì„ ë“œë¦¬ê² ìŠµë‹ˆë‹¤.</p>"
            
            # ì–¸ì–´ë³„ í¬ë§·íŒ…
            if lang == 'en':
                # ì˜ì–´ ë‹µë³€ í¬ë§·íŒ…
                base_answer = self.remove_old_app_name(base_answer)
                
                # ê¸°ì¡´ ì¸ì‚¬ë§/ëë§ºìŒë§ ì œê±°
                base_answer = re.sub(r'^Hello[^.]*\.\s*', '', base_answer, flags=re.IGNORECASE)
                base_answer = re.sub(r'^This is GOODTV Bible App[^.]*\.\s*', '', base_answer, flags=re.IGNORECASE)
                base_answer = re.sub(r'\s*Thank you[^.]*\.?\s*$', '', base_answer, flags=re.IGNORECASE)
                base_answer = re.sub(r'\s*Best regards[^.]*\.?\s*$', '', base_answer, flags=re.IGNORECASE)
                base_answer = re.sub(r'\s*God bless[^.]*\.?\s*$', '', base_answer, flags=re.IGNORECASE)
                
                formatted_body = self.format_answer_with_html_paragraphs(base_answer.strip(), 'en')
                
                # ì˜ì–´ ê³ ì • ì¸ì‚¬ë§ê³¼ ëë§ºìŒë§
                final_answer = "<p>Hello, this is GOODTV Bible Apple App customer service team.</p><p><br></p><p>Thank you very much for using our app and for taking the time to contact us.</p><p><br></p>"
                final_answer += formatted_body
                final_answer += "<p><br></p><p>Thank you once again for sharing your thoughts with us!</p><p><br></p><p>May God's peace and grace always be with you.</p>"
                
            else:  # í•œêµ­ì–´
                # í•œêµ­ì–´ ë‹µë³€ ìµœì¢… í¬ë§·íŒ… (Quill ì—ë””í„°ìš© HTML í˜•ì‹ ìœ ì§€)
                # ì•± ì´ë¦„ ì •ë¦¬ ë° ê³ ê°ë‹˜ â†’ ì„±ë„ë‹˜ ë³€ê²½
                base_answer = self.remove_old_app_name(base_answer)
                base_answer = re.sub(r'ê³ ê°ë‹˜', 'ì„±ë„ë‹˜', base_answer)
                
                # ê¸°ì¡´ ì¸ì‚¬ë§/ëë§ºìŒë§ ì œê±° (ì¼ë°˜ í…ìŠ¤íŠ¸ì—ì„œ)
                # ì¸ì‚¬ë§ ì œê±°
                base_answer = re.sub(r'^ì•ˆë…•í•˜ì„¸ìš”[^.]*\.\s*', '', base_answer, flags=re.IGNORECASE)
                base_answer = re.sub(r'^GOODTV\s+ë°”ì´ë¸”\s*ì• í”Œ[^.]*\.\s*', '', base_answer, flags=re.IGNORECASE)
                base_answer = re.sub(r'^ë°”ì´ë¸”\s*ì• í”Œ[^.]*\.\s*', '', base_answer, flags=re.IGNORECASE)
                base_answer = re.sub(r'ê³ ê°ì„¼í„°[^.]*\.\s*', '', base_answer, flags=re.IGNORECASE)
                
                # ëë§ºìŒë§ ì œê±° (ë” ê°•í™”ëœ íŒ¨í„´)
                base_answer = re.sub(r'\s*ê°ì‚¬í•©ë‹ˆë‹¤[^.]*\.?\s*$', '', base_answer, flags=re.IGNORECASE)
                base_answer = re.sub(r'\s*í‰ì•ˆí•˜ì„¸ìš”[^.]*\.?\s*$', '', base_answer, flags=re.IGNORECASE)
                base_answer = re.sub(r'\s*ì£¼ë‹˜\s*ì•ˆì—ì„œ[^.]*\.?\s*$', '', base_answer, flags=re.IGNORECASE)
                base_answer = re.sub(r'\s*í•¨ê»˜\s*ê¸°ë„í•˜ë©°[^.]*\.?\s*$', '', base_answer, flags=re.IGNORECASE)
                base_answer = re.sub(r'\s*í•­ìƒ[^.]*ë°”ì´ë¸”\s*ì• í”Œ[^.]*\.?\s*$', '', base_answer, flags=re.IGNORECASE)
                
                # ì¶”ê°€ ëë§ºìŒë§ íŒ¨í„´ë“¤ (ë” í¬ê´„ì ìœ¼ë¡œ)
                base_answer = re.sub(r'\s*í•­ìƒ\s*ì£¼ë‹˜\s*ì•ˆì—ì„œ[^.]*\.?\s*$', '', base_answer, flags=re.IGNORECASE)
                base_answer = re.sub(r'\s*ì£¼ë‹˜\s*ì•ˆì—ì„œ\s*í‰ì•ˆí•˜ì„¸ìš”[^.]*\.?\s*$', '', base_answer, flags=re.IGNORECASE)
                base_answer = re.sub(r'\s*í‰ì•ˆí•˜ì„¸ìš”[^.]*\.?\s*$', '', base_answer, flags=re.IGNORECASE)
                
                # ë¬¸ì¥ ëì˜ ëë§ºìŒë§ë“¤ë„ ì œê±°
                base_answer = re.sub(r'[,.!?]\s*í•­ìƒ\s*ì£¼ë‹˜\s*ì•ˆì—ì„œ[^.]*\.?\s*$', '', base_answer, flags=re.IGNORECASE)
                base_answer = re.sub(r'[,.!?]\s*ê°ì‚¬í•©ë‹ˆë‹¤[^.]*\.?\s*$', '', base_answer, flags=re.IGNORECASE)
                base_answer = re.sub(r'[,.!?]\s*í‰ì•ˆí•˜ì„¸ìš”[^.]*\.?\s*$', '', base_answer, flags=re.IGNORECASE)
                
                # ë³¸ë¬¸ì„ HTML ë‹¨ë½ í˜•ì‹ìœ¼ë¡œ í¬ë§·íŒ…
                formatted_body = self.format_answer_with_html_paragraphs(base_answer.strip(), 'ko')
                
                # í•œêµ­ì–´ ê³ ì • ì¸ì‚¬ë§ (HTML í˜•ì‹ìœ¼ë¡œ)
                final_answer = "<p>ì•ˆë…•í•˜ì„¸ìš”. GOODTV ë°”ì´ë¸” ì• í”Œì…ë‹ˆë‹¤.</p><p><br></p><p>ë°”ì´ë¸” ì• í”Œì„ ì´ìš©í•´ì£¼ì…”ì„œ ê°ì‚¬ë“œë¦½ë‹ˆë‹¤.</p><p><br></p>"
                
                # í¬ë§·íŒ…ëœ ë³¸ë¬¸ ì¶”ê°€
                final_answer += formatted_body
                
                # ê³ ì •ëœ ëë§ºìŒë§ (HTML í˜•ì‹ìœ¼ë¡œ)
                final_answer += "<p><br></p><p>í•­ìƒ ì„±ë„ë‹˜ê»˜ ì¢‹ì€ ì„±ê²½ì•±ì„ ì œê³µí•˜ê¸° ìœ„í•´ ë…¸ë ¥í•˜ëŠ” ë°”ì´ë¸” ì• í”Œì´ ë˜ê² ìŠµë‹ˆë‹¤.</p><p><br></p><p>ê°ì‚¬í•©ë‹ˆë‹¤. ì£¼ë‹˜ ì•ˆì—ì„œ í‰ì•ˆí•˜ì„¸ìš”.</p>"
            
            logging.info(f"ìµœì¢… ë‹µë³€ ìƒì„± ì™„ë£Œ: {len(final_answer)}ì, ì ‘ê·¼ë°©ì‹: {approach}, ì–¸ì–´: {lang}")
            return final_answer
            
        except Exception as e:
            logging.error(f"ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜: {e}")
            if lang == 'en':
                return "<p>Sorry, we cannot generate an answer at this moment.</p><p><br></p><p>Please contact our customer service center.</p>"
            else:
                return "<p>ì£„ì†¡í•©ë‹ˆë‹¤. í˜„ì¬ ë‹µë³€ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.</p><p><br></p><p>ê³ ê°ì„¼í„°ë¡œ ë¬¸ì˜í•´ì£¼ì„¸ìš”.</p>"

    # â˜† ë©”ëª¨ë¦¬ ìµœì í™”ëœ ë©”ì¸ ì²˜ë¦¬ ë©”ì„œë“œ
    def process(self, seq: int, question: str, lang: str) -> dict:
        try:
            with memory_cleanup():
                processed_question = self.preprocess_text(question)
                if not processed_question:
                    return {"success": False, "error": "ì§ˆë¬¸ì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤."}
                
                # ì–¸ì–´ ìë™ ê°ì§€
                if not lang or lang == 'auto':
                    lang = self.detect_language(processed_question)
                    logging.info(f"ìë™ ê°ì§€ëœ ì–¸ì–´: {lang}")
                
                logging.info(f"ì²˜ë¦¬ ì‹œì‘ - SEQ: {seq}, ì–¸ì–´: {lang}, ì§ˆë¬¸: {processed_question[:50]}...")
                
                # ìœ ì‚¬ ë‹µë³€ ê²€ìƒ‰ (ì–¸ì–´ íŒŒë¼ë¯¸í„° ì „ë‹¬)
                similar_answers = self.search_similar_answers(processed_question, lang=lang)
                
                # AI ë‹µë³€ ìƒì„± (ì–¸ì–´ íŒŒë¼ë¯¸í„° ì „ë‹¬)
                ai_answer = self.generate_ai_answer(processed_question, similar_answers, lang)
                
                # íŠ¹ìˆ˜ë¬¸ì ì •ë¦¬
                ai_answer = ai_answer.replace('"', '"').replace('"', '"')
                ai_answer = ai_answer.replace(''', "'").replace(''', "'")
                
                result = {
                    "success": True,
                    "answer": ai_answer,
                    "similar_count": len(similar_answers),
                    "embedding_model": "text-embedding-3-small",
                    "generation_model": "gpt-3.5-turbo",
                    "detected_language": lang
                }
                
                logging.info(f"ì²˜ë¦¬ ì™„ë£Œ - SEQ: {seq}, ì–¸ì–´: {lang}, HTML ë‹µë³€ ìƒì„±ë¨")
                return result
                
        except Exception as e:
            logging.error(f"ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ - SEQ: {seq}, ì˜¤ë¥˜: {str(e)}")
            return {"success": False, "error": str(e)}

# ==================================================
# 8. Pinecone ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ë™ê¸°í™” í´ë˜ìŠ¤
# ==================================================
# MSSQL ìš´ì˜ ë°ì´í„°ë² ì´ìŠ¤ì™€ Pinecone ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ê°„ì˜ ë™ê¸°í™”ë¥¼ ë‹´ë‹¹í•˜ëŠ” í´ë˜ìŠ¤
# 
# ì£¼ìš” ê¸°ëŠ¥:
# 1. MSSQLì—ì„œ ìƒˆë¡œìš´ Q&A ë°ì´í„° ì¡°íšŒ
# 2. AIë¥¼ ì´ìš©í•œ í•œêµ­ì–´ ì˜¤íƒ€ ìˆ˜ì •
# 3. OpenAIë¡œ ì„ë² ë”© ë²¡í„° ìƒì„±
# 4. Pineconeì— ë²¡í„° ë°ì´í„° ì €ì¥/ìˆ˜ì •/ì‚­ì œ
# 
# ìš´ì˜ ì‹œë‚˜ë¦¬ì˜¤:
# - ìƒˆë¡œìš´ ê³ ê° ë¬¸ì˜ ë‹µë³€ì´ MSSQLì— ì €ì¥ë˜ë©´
# - ì´ í´ë˜ìŠ¤ë¥¼ í†µí•´ Pineconeì— ë™ê¸°í™”í•˜ì—¬
# - í–¥í›„ ìœ ì‚¬ ì§ˆë¬¸ ê²€ìƒ‰ì´ ê°€ëŠ¥í•˜ê²Œ í•¨
class PineconeSyncManager:
    
    # ë™ê¸°í™” ë§¤ë‹ˆì € ì´ˆê¸°í™”
    # ì™¸ë¶€ì—ì„œ ìƒì„±ëœ Pinecone ì¸ë±ìŠ¤ì™€ OpenAI í´ë¼ì´ì–¸íŠ¸ ì°¸ì¡°
    def __init__(self):
        self.index = index                    # Pinecone ë²¡í„° ì¸ë±ìŠ¤
        self.openai_client = openai_client    # OpenAI API í´ë¼ì´ì–¸íŠ¸
    
    # â˜† AIë¥¼ ì´ìš©í•œ í•œêµ­ì–´ ì˜¤íƒ€ ìˆ˜ì • ë©”ì„œë“œ
    def fix_korean_typos_with_ai(self, text: str) -> str:
        if not text or len(text.strip()) < 3:
            return text
        
        # ë„ˆë¬´ ê¸´ í…ìŠ¤íŠ¸ëŠ” ì²˜ë¦¬í•˜ì§€ ì•ŠìŒ (ë¹„ìš© ì ˆì•½)
        if len(text) > 500:
            logging.warning(f"í…ìŠ¤íŠ¸ê°€ ë„ˆë¬´ ê¸¸ì–´ ì˜¤íƒ€ ìˆ˜ì • ê±´ë„ˆëœ€: {len(text)}ì")
            return text
        
        try:
            with memory_cleanup():
                system_prompt = """ë‹¹ì‹ ì€ í•œêµ­ì–´ ë§ì¶¤ë²• ë° ì˜¤íƒ€ êµì • ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

ì§€ì¹¨:
1. ì…ë ¥ëœ í•œêµ­ì–´ í…ìŠ¤íŠ¸ì˜ ë§ì¶¤ë²•ê³¼ ì˜¤íƒ€ë§Œ ìˆ˜ì •í•˜ì„¸ìš”
2. ì›ë¬¸ì˜ ì˜ë¯¸ì™€ ì–´ì¡°ëŠ” ì ˆëŒ€ ë³€ê²½í•˜ì§€ ë§ˆì„¸ìš”
3. ë„ì–´ì“°ê¸°, ë§ì¶¤ë²•, ì¡°ì‚¬ ì‚¬ìš©ë²•ì„ ì •í™•íˆ êµì •í•˜ì„¸ìš”
4. ì•±/ì–´í”Œë¦¬ì¼€ì´ì…˜ ê´€ë ¨ ê¸°ìˆ  ìš©ì–´ëŠ” í‘œì¤€ ìš©ì–´ë¡œ í†µì¼í•˜ì„¸ìš”
5. ìˆ˜ì •ì´ í•„ìš”ì—†ë‹¤ë©´ ì›ë¬¸ ê·¸ëŒ€ë¡œ ë°˜í™˜í•˜ì„¸ìš”
6. ìˆ˜ì •ëœ í…ìŠ¤íŠ¸ë§Œ ë°˜í™˜í•˜ê³  ì¶”ê°€ ì„¤ëª…ì€ í•˜ì§€ ë§ˆì„¸ìš”

ì˜ˆì‹œ:
- "ì–´í”Œì´ ì•ˆë€ë‹¤" â†’ "ì•±ì´ ì•ˆ ë¼ìš”"
- "ë‹¤ìš´ë°›ê¸°ê°€ ì•ˆë˜ìš”" â†’ "ë‹¤ìš´ë¡œë“œê°€ ì•ˆ ë¼ìš”"
- "ì‚­ì¬í•˜ê³ ì‹¶ì–´ìš”" â†’ "ì‚­ì œí•˜ê³  ì‹¶ì–´ìš”"
- "ì—…ë°ì´ë“œí•´ì£¼ì„¸ìš”" â†’ "ì—…ë°ì´íŠ¸í•´ì£¼ì„¸ìš”"
"""

                user_prompt = f"ë‹¤ìŒ í…ìŠ¤íŠ¸ì˜ ë§ì¶¤ë²•ê³¼ ì˜¤íƒ€ë¥¼ ìˆ˜ì •í•´ì£¼ì„¸ìš”:\n\n{text}"

                response = self.openai_client.chat.completions.create(
                    model='gpt-3.5-turbo',
                    messages=[
                        {"role": "system", "content": system_prompt},
                        {"role": "user", "content": user_prompt}
                    ],
                    max_tokens=600,
                    temperature=0.1,  # ë§¤ìš° ë³´ìˆ˜ì ìœ¼ë¡œ ì„¤ì •
                    top_p=0.8,
                    frequency_penalty=0.0,
                    presence_penalty=0.0
                )
                
                corrected_text = response.choices[0].message.content.strip()
                del response # ë©”ëª¨ë¦¬ í•´ì œ
                
                # ê²°ê³¼ ê²€ì¦
                if not corrected_text or len(corrected_text) == 0:
                    logging.warning("AI ì˜¤íƒ€ ìˆ˜ì • ê²°ê³¼ê°€ ë¹„ì–´ìˆìŒ, ì›ë¬¸ ë°˜í™˜")
                    return text
                
                # ë„ˆë¬´ ë§ì´ ë³€ê²½ëœ ê²½ìš° ì˜ì‹¬ìŠ¤ëŸ¬ìš°ë¯€ë¡œ ì›ë¬¸ ë°˜í™˜
                if len(corrected_text) > len(text) * 2:
                    logging.warning("AI ì˜¤íƒ€ ìˆ˜ì • ê²°ê³¼ê°€ ì›ë¬¸ë³´ë‹¤ ë„ˆë¬´ ê¸¸ì–´ì§, ì›ë¬¸ ë°˜í™˜")
                    return text
                
                # ìˆ˜ì • ë‚´ìš©ì´ ìˆìœ¼ë©´ ë¡œê·¸ ê¸°ë¡
                if corrected_text != text:
                    logging.info(f"AI ì˜¤íƒ€ ìˆ˜ì •: '{text[:50]}...' â†’ '{corrected_text[:50]}...'")
                
                return corrected_text
                
        except Exception as e:
            logging.error(f"AI ì˜¤íƒ€ ìˆ˜ì • ì‹¤íŒ¨: {e}")
            # AI ì‹¤íŒ¨ ì‹œ ì›ë¬¸ ê·¸ëŒ€ë¡œ ë°˜í™˜
            return text
        
    # í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ ë©”ì„œë“œ
    def preprocess_text(self, text: str, for_metadata: bool = False) -> str:
        if not text or text == 'None':
            return ""
        
        text = str(text)
        text = html.unescape(text)
        
        # HTML íƒœê·¸ ì œê±°
        text = re.sub(r'<br\s*/?>', '\n', text, flags=re.IGNORECASE)
        text = re.sub(r'</p>', '\n', text, flags=re.IGNORECASE)
        text = re.sub(r'<p[^>]*>', '', text, flags=re.IGNORECASE)
        text = re.sub(r'<[^>]+>', '', text)
        
        # ìœ ë‹ˆì½”ë“œ ì •ê·œí™”
        text = unicodedata.normalize('NFC', text)
        
        # ê³µë°± ì •ë¦¬
        if for_metadata:
            text = re.sub(r'\n{3,}', '\n\n', text)
            text = re.sub(r'[ \t]+', ' ', text)
        else:
            text = re.sub(r'\s+', ' ', text)
        
        text = text.strip()
        
        # ê¸¸ì´ ì œí•œ
        max_length = 1000 if for_metadata else MAX_TEXT_LENGTH
        if len(text) > max_length:
            text = text[:max_length-3] + "..."
        
        return text
    
    # OpenAIë¡œ ì„ë² ë”© ìƒì„±í•˜ëŠ” ë©”ì„œë“œ
    def create_embedding(self, text: str) -> Optional[list]:
        try:
            if not text or not text.strip():
                return None
            
            with memory_cleanup():
                response = openai_client.embeddings.create(
                    model=MODEL_NAME,
                    input=text
                )
                return response.data[0].embedding
            
        except Exception as e:
            logging.error(f"ì„ë² ë”© ìƒì„± ì‹¤íŒ¨: {e}")
            return None
    
    # ì¹´í…Œê³ ë¦¬ ì¸ë±ìŠ¤ë¥¼ ì´ë¦„ìœ¼ë¡œ ë³€í™˜í•˜ëŠ” ë©”ì„œë“œ
    def get_category_name(self, cate_idx: str) -> str:
        return CATEGORY_MAPPING.get(str(cate_idx), 'ì‚¬ìš© ë¬¸ì˜(ê¸°íƒ€)')
    
    # MSSQLì—ì„œ ë°ì´í„° ì¡°íšŒí•˜ëŠ” ë©”ì„œë“œ
    # íŒŒë¼ë¯¸í„°í™”ëœ ì¿¼ë¦¬ë¡œ SQL ì¸ì ì…˜ ë°©ì§€, ? í”Œë ˆì´ìŠ¤í™€ë”ë¥¼ ì‚¬ìš©í•˜ì—¬ ì•ˆì „í•˜ê²Œ ê°’ì„ ë°”ì¸ë”©
    def get_mssql_data(self, seq: int) -> Optional[Dict]:
        try:
            with memory_cleanup():
                conn = pyodbc.connect(connection_string)
                cursor = conn.cursor()
                
                query = """
                SELECT seq, contents, reply_contents, cate_idx, name, 
                       CONVERT(varchar, regdate, 120) as regdate
                FROM mobile.dbo.bible_inquiry
                WHERE seq = ? AND answer_YN = 'Y'
                """
                
                cursor.execute(query, seq)
                row = cursor.fetchone()
                
                if row:
                    data = {
                        'seq': row[0],
                        'contents': row[1],
                        'reply_contents': row[2],
                        'cate_idx': row[3],
                        'name': row[4],
                        'regdate': row[5]
                    }
                    return data
                
                cursor.close()
                conn.close()
                return None
            
        except Exception as e:
            logging.error(f"MSSQL ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return None
    
    # MSSQL ë°ì´í„°ë¥¼ Pineconeì— ë™ê¸°í™”í•˜ëŠ” ë©”ì„œë“œ
    def sync_to_pinecone(self, seq: int, mode: str = 'upsert') -> Dict[str, Any]:
        try:
            with memory_cleanup():
                # ì‚­ì œ ëª¨ë“œ
                if mode == 'delete':
                    vector_id = f"qa_bible_{seq}"
                    self.index.delete(ids=[vector_id])
                    logging.info(f"Pineconeì—ì„œ ì‚­ì œ ì™„ë£Œ: {vector_id}")
                    return {"success": True, "message": "ì‚­ì œ ì™„ë£Œ", "seq": seq}
                
                # MSSQLì—ì„œ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
                data = self.get_mssql_data(seq)
                if not data:
                    return {"success": False, "error": "ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤"}
                
                # í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ (ì§ˆë¬¸ì— AI ì˜¤íƒ€ ìˆ˜ì • ì ìš©)
                raw_question = self.preprocess_text(data['contents'])
                question = self.fix_korean_typos_with_ai(raw_question)
                answer = self.preprocess_text(data['reply_contents'])
                
                # ì„ë² ë”© ìƒì„± (ì§ˆë¬¸ ê¸°ë°˜)
                embedding = self.create_embedding(question)
                if not embedding:
                    return {"success": False, "error": "ì„ë² ë”© ìƒì„± ì‹¤íŒ¨"}
                
                # ì¹´í…Œê³ ë¦¬ ì´ë¦„ ê°€ì ¸ì˜¤ê¸°
                category = self.get_category_name(data['cate_idx'])
                
                # ë©”íƒ€ë°ì´í„° êµ¬ì„± (ì§ˆë¬¸ì€ ì˜¤íƒ€ ìˆ˜ì •ëœ ë²„ì „ ì‚¬ìš©)
                metadata = {
                    "seq": int(data['seq']),
                    "question": question,
                    "answer": self.preprocess_text(data['reply_contents'], for_metadata=True),
                    "category": category,
                    "name": data['name'] if data['name'] else "ìµëª…",
                    "regdate": data['regdate'],
                    "source": "bible_inquiry_mssql",
                    "updated_at": datetime.now().strftime("%Y-%m-%d %H:%M:%S")
                }
                
                # Pineconeì— upsert
                vector_id = f"qa_bible_{seq}"
                
                # ê¸°ì¡´ ë²¡í„° í™•ì¸
                existing = self.index.fetch(ids=[vector_id])
                is_update = vector_id in existing['vectors']
                
                # ë²¡í„° ë°ì´í„° êµ¬ì„±
                vector_data = {
                    "id": vector_id,
                    "values": embedding,
                    "metadata": metadata
                }
                
                # Pineconeì— ì €ì¥
                self.index.upsert(vectors=[vector_data])
                
                action = "ìˆ˜ì •" if is_update else "ìƒì„±"
                logging.info(f"Pinecone {action} ì™„ë£Œ: {vector_id}")
                
                return {
                    "success": True,
                    "message": f"Pinecone {action} ì™„ë£Œ",
                    "seq": seq,
                    "vector_id": vector_id,
                    "is_update": is_update
                }
            
        except Exception as e:
            logging.error(f"Pinecone ë™ê¸°í™” ì‹¤íŒ¨: {str(e)}")
            return {"success": False, "error": str(e)}

# ==================================================
# 9. ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤ ìƒì„± (ì „ì—­ ê°ì²´)
# ==================================================
# ì• í”Œë¦¬ì¼€ì´ì…˜ ì „ì²´ì—ì„œ ì‚¬ìš©í•  ë‹¨ì¼ ì¸ìŠ¤í„´ìŠ¤ë“¤
# ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±ê³¼ ìƒíƒœ ì¼ê´€ì„±ì„ ìœ„í•´ ì‹±ê¸€í†¤ íŒ¨í„´ ì ìš©
generator = AIAnswerGenerator()      # AI ë‹µë³€ ìƒì„±ê¸°
sync_manager = PineconeSyncManager() # Pinecone ë™ê¸°í™” ë§¤ë‹ˆì €

# ==================================================
# 10. Flask RESTful API ì—”ë“œí¬ì¸íŠ¸ ì •ì˜
# ==================================================
# â˜… AI ë‹µë³€ ìƒì„± API ì—”ë“œí¬ì¸íŠ¸ (ë©”ì¸ ê¸°ëŠ¥)
# ASP Classicì—ì„œ í˜¸ì¶œí•˜ëŠ” ì£¼ìš” APIë¡œ, ê³ ê° ì§ˆë¬¸ì— ëŒ€í•œ AI ë‹µë³€ì„ ìƒì„±
# ì²˜ë¦¬ ê³¼ì •:
# 1. ì§ˆë¬¸ ì „ì²˜ë¦¬ ë° ê²€ì¦
# 2. Pineconeì—ì„œ ìœ ì‚¬ ë‹µë³€ ê²€ìƒ‰
# 3. ì»¨í…ìŠ¤íŠ¸ í’ˆì§ˆ ë¶„ì„
# 4. GPTë¥¼ ì´ìš©í•œ ë§ì¶¤ ë‹µë³€ ìƒì„±
# 5. ìµœì¢… í¬ë§·íŒ… ë° ë°˜í™˜
@app.route('/generate_answer', methods=['POST'])
def generate_answer():
    try:
        with memory_cleanup():
            data = request.get_json()
            seq = data.get('seq', 0)
            question = data.get('question', '')
            lang = data.get('lang', 'auto')  # ê¸°ë³¸ê°’ì„ 'auto'ë¡œ ë³€ê²½ (ìë™ ê°ì§€)
            
            if not question:
                return jsonify({"success": False, "error": "ì§ˆë¬¸ì´ í•„ìš”í•©ë‹ˆë‹¤."}), 400
            
            result = generator.process(seq, question, lang)
            
            response = jsonify(result)
            response.headers['Content-Type'] = 'application/json; charset=utf-8'

            # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ëª¨ë‹ˆí„°ë§
            snapshot = tracemalloc.take_snapshot() # ê° ìš”ì²­ í›„ ë©”ëª¨ë¦¬ ìŠ¤ëƒ…ìƒ· ì´¬ì˜
            top_stats = snapshot.statistics('lineno') # ê° ìš”ì²­ í›„ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ í†µê³„
            memory_usage = sum(stat.size for stat in top_stats) / 1024 / 1024  # MBë¡œ ë°˜í™˜
            logging.info(f"í˜„ì¬ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {memory_usage:.2f}MB")
            
            if memory_usage > 500: # 500MB ì´ˆê³¼ì‹œ ê²½ê³  ë° ê°€ë¹„ì§€ ì»¬ë ‰ì…˜ ê°•ì œ ì‹¤í–‰
                logging.warning(f"ë†’ì€ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ê°ì§€: {memory_usage:.2f}MB")
                gc.collect()

            return response
        
    except Exception as e:
        logging.error(f"API í˜¸ì¶œ ì˜¤ë¥˜: {str(e)}")
        return jsonify({"success": False, "error": str(e)}), 500

# â˜… MSSQL ë°ì´í„°ë¥¼ Pineconeì— ë™ê¸°í™”í•˜ëŠ” API ì—”ë“œí¬ì¸íŠ¸
# ìš´ì˜ ì‹œìŠ¤í…œì—ì„œ ìƒˆë¡œìš´ Q&A ë°ì´í„°ê°€ ìƒì„±ë˜ê±°ë‚˜ ìˆ˜ì •ë  ë•Œ í˜¸ì¶œ
# ì²˜ë¦¬ ê³¼ì •:
# 1. MSSQLì—ì„œ í•´ë‹¹ seq ë°ì´í„° ì¡°íšŒ
# 2. AIë¡œ ì§ˆë¬¸ ì˜¤íƒ€ ìˆ˜ì •
# 3. OpenAIë¡œ ì„ë² ë”© ë²¡í„° ìƒì„±
# 4. Pineconeì— ë²¡í„° ì €ì¥/ìˆ˜ì •/ì‚­ì œ
@app.route('/sync_to_pinecone', methods=['POST'])
def sync_to_pinecone():
    try:
        data = request.get_json()
        seq = data.get('seq')
        mode = data.get('mode', 'upsert')

        logging.info(f"ë™ê¸°í™” ìš”ì²­ ìˆ˜ì‹ : seq={seq}, mode={mode}")
        
        if not seq:
            logging.warning("seq ëˆ„ë½")
            return jsonify({"success": False, "error": "seqê°€ í•„ìš”í•©ë‹ˆë‹¤"}), 400
        
        if not isinstance(seq, int):
            seq = int(seq)
        
        result = sync_manager.sync_to_pinecone(seq, mode)

        logging.info(f"ë™ê¸°í™” ê²°ê³¼: {result}")
        
        status_code = 200 if result["success"] else 500
        return jsonify(result), status_code
        
    except ValueError as e:
        logging.error(f"ì˜ëª»ëœ seq ê°’: {str(e)}")
        return jsonify({"success": False, "error": f"ì˜ëª»ëœ seq ê°’: {str(e)}"}), 400
    except Exception as e:
        logging.error(f"Pinecone ë™ê¸°í™” API ì˜¤ë¥˜: {str(e)}")
        return jsonify({"success": False, "error": str(e)}), 500

# â˜… ì‹œìŠ¤í…œ ìƒíƒœ í™•ì¸ì„ ìœ„í•œ í—¬ìŠ¤ì²´í¬ API ì—”ë“œí¬ì¸íŠ¸
# ë¡œë“œë°¸ëŸ°ì„œë‚˜ ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œì—ì„œ í˜¸ì¶œí•˜ì—¬ ì„œë²„ ìƒíƒœ í™•ì¸
@app.route('/health', methods=['GET'])
def health_check():
    try:
        stats = index.describe_index_stats()
        
        return jsonify({
            "status": "healthy",
            "pinecone_vectors": stats.get('total_vector_count', 0),
            "timestamp": datetime.now().isoformat(),
            "services": {
                "ai_answer": "active",
                "pinecone_sync": "active",
                "multilingual_support": "active"  # ë‹¤êµ­ì–´ ì§€ì› ìƒíƒœ ì¶”ê°€
            }
        }), 200
    except Exception as e:
        return jsonify({
            "status": "unhealthy",
            "error": str(e)
        }), 500

# ==================================================
# 11. ë©”ì¸ ì‹¤í–‰ ë¶€ë¶„
# ==================================================
# Flask ì›¹ ì„œë²„ ì‹œì‘ì 
# 
# ì´ ë¶€ë¶„ì€ ì´ íŒŒì¼(ìŠ¤í¬ë¦½íŠ¸)ê°€ ì§ì ‘ ì‹¤í–‰ë  ë•Œë§Œ ì‹¤í–‰ë˜ëŠ” ì ˆì°¨ì  ì½”ë“œ
# ë‹¤ë¥¸ ëª¨ë“ˆì—ì„œ importí•  ë•ŒëŠ” ì‹¤í–‰ë˜ì§€ ì•ŠìŒ
# 
# ì„¤ì •:
# - í¬íŠ¸: í™˜ê²½ë³€ìˆ˜ FLASK_PORT ë˜ëŠ” ê¸°ë³¸ê°’ 8000
# - í˜¸ìŠ¤íŠ¸: 0.0.0.0 (ëª¨ë“  IPì—ì„œ ì ‘ê·¼ ê°€ëŠ¥)
# - ë””ë²„ê·¸: False (ìš´ì˜ ëª¨ë“œ)
# - ìŠ¤ë ˆë“œ: True (ë©€í‹°ìŠ¤ë ˆë”© ì§€ì›)
if __name__ == "__main__":
    
    # í™˜ê²½ë³€ìˆ˜ì—ì„œ í¬íŠ¸ ì„¤ì • ë¡œë“œ (ê¸°ë³¸ê°’: 8000)
    port = int(os.getenv('FLASK_PORT', 8000))
    
    # ì‹œì‘ ë©”ì‹œì§€ ì¶œë ¥
    print("="*60)
    print("ğŸš€ GOODTV ë°”ì´ë¸” ì• í”Œ AI ë‹µë³€ ìƒì„± ì„œë²„ ì‹œì‘")
    print("="*60)
    print(f"ğŸ“¡ ì„œë²„ í¬íŠ¸: {port}")
    print(f"ğŸ¤– AI ëª¨ë¸: {GPT_MODEL} (Enhanced Context Mode)")
    print(f"ğŸ” ì„ë² ë”© ëª¨ë¸: {MODEL_NAME}")
    print(f"ğŸ—ƒï¸  ë²¡í„° DB: Pinecone ({INDEX_NAME})")
    print(f"ğŸŒ ë‹¤êµ­ì–´ ì§€ì›: í•œêµ­ì–´(ko), ì˜ì–´(en)")
    print("ğŸ”§ ì œê³µ ì„œë¹„ìŠ¤:")
    print("   â”œâ”€â”€ AI ë‹µë³€ ìƒì„± (/generate_answer)")
    print("   â”œâ”€â”€ Pinecone ë™ê¸°í™” (/sync_to_pinecone)")
    print("   â””â”€â”€ í—¬ìŠ¤ì²´í¬ (/health)")
    print("="*60)
    
    # Flask ì›¹ ì„œë²„ ì‹œì‘
    # host='0.0.0.0': ëª¨ë“  ë„¤íŠ¸ì›Œí¬ ì¸í„°í˜ì´ìŠ¤ì—ì„œ ì ‘ê·¼ í—ˆìš©
    # debug=False: ìš´ì˜ ëª¨ë“œ (ë³´ì•ˆìƒ ì¤‘ìš”)
    # threaded=True: ë©€í‹°ìŠ¤ë ˆë”© í™œì„±í™”, ë™ì‹œ ìš”ì²­ ì²˜ë¦¬ ê°€ëŠ¥
    app.run(host='0.0.0.0', port=port, debug=False, threaded=True)